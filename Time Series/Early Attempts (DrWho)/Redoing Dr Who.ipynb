{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up modules for Google functionality\n",
    "from google.cloud import bigquery # To run BQ statements\n",
    "from google_auth_oauthlib import flow # To authorise as user\n",
    "from googleapiclient.discovery import build # To pull in from sheets, slides etc. API\n",
    "from google.auth.transport.requests import Request\n",
    "\n",
    "# Display\n",
    "import pprint\n",
    "\n",
    "# Operating system stuff\n",
    "import pickle\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "# Data handling\n",
    "import json\n",
    "import requests\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "\n",
    "# Stats, models, datasheets\n",
    "import pandas as pd\n",
    "import pyreadstat\n",
    "\n",
    "# Visualisation\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_venn # For venn diagrams\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "# Network graphs\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "# Misc\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell # Used to create cell references\n",
    "import itertools\n",
    "\n",
    "# Load custom scripts in reusable_code folder\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "\n",
    "import google_api_functions as gaf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import itertools\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import pacf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/')\n",
    "bq = bigquery.Client(project='itv-bde-analytics-dev',credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''select datetime(timestamp_add('2019-11-07 00:00:00', \n",
    "   INTERVAL cast(floor(timestamp_diff(timestamp_trunc(start_date, HOUR),'2019-11-07 00:00:00',HOUR)/4)*4 as int64) HOUR)) \n",
    "as starting_hour\n",
    ",count(distinct britbox_id) as N from \n",
    "`itv-bde-svod-prd.reporting.Entitlements_oldf_reporting`\n",
    "where date(start_date)>='2019-11-11'\n",
    "and plan_type='trial'\n",
    "group by 1\n",
    "order by 1'''\n",
    "FTS_df = bq.query(query).to_dataframe()\n",
    "FTS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTS_df=FTS_df.set_index('starting_hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datasets(complete_df, training_start, forecast_start, forecast_end, seasonality_period\\\n",
    "                   ,eval_duration=(1,'s'),training_end=None):\n",
    "    \n",
    "    # Aim is to create three datasets:\n",
    "    # Training (the period to build the model off of)\n",
    "    # Eval actuals (a period just before the model is build on which to test the model fit)\n",
    "    # Forecast actuals (a period that you're going to predict. Actuals will only exist if you are using it for incremental analysis)\n",
    "    \n",
    "    # If NO training_end specified, the training dataset and the evaluation dataset will be mutually exclusive, \n",
    "    # i.e. training will end where eval begins. If it is explicitly specified, there will be some overlap\n",
    "    \n",
    "    # Remove the section to be forecasted\n",
    "    partial_df=complete_df[training_start:forecast_start][:-1]\n",
    "    try:\n",
    "        forecast_actuals=complete_df[forecast_start:forecast_end]\n",
    "    except:\n",
    "        forecast_actuals=None\n",
    "    \n",
    "    #\n",
    "    if eval_duration[1]=='s':\n",
    "        if training_end==None:\n",
    "            training_df=partial_df[:-((eval_duration[0]*seasonality_period))]\n",
    "        else:\n",
    "            training_df=partial_df[:training_end]\n",
    "            \n",
    "        eval_actuals=partial_df[-((eval_duration[0]*seasonality_period)):]\n",
    "    \n",
    "    else:\n",
    "        if training_end==None:\n",
    "            training_df=partial_df[:-((eval_duration[0]))]\n",
    "        else:\n",
    "            training_df=partial_df[:training_end]\n",
    "        eval_actuals=partial_df[-((eval_duration[0])):]\n",
    "\n",
    "    return (training_df,eval_actuals,forecast_actuals)\n",
    "\n",
    "def model_fit_results(df,observed_col,forecast_col=None,resid_col=None,showplot=True):\n",
    "    if resid_col and forecast_col:\n",
    "        df['modelled'] = df[forecast_col]\n",
    "        df['Error']=df[resid_col]\n",
    "    elif resid_col:\n",
    "        df['Error']=df[resid_col]\n",
    "        df['modelled'] = df[observed_col]-df['Error']        \n",
    "    elif forecast_col:\n",
    "        df['modelled'] = df[forecast_col]\n",
    "        df['Error']=df[observed_col]-df['modelled']\n",
    "    else:\n",
    "        print(\"Error: you need either the forecasted values or the residuals to show model fit results\")\n",
    "        return\n",
    "    \n",
    "    df['Squared error']=df['Error']**2\n",
    "    df['Absolute error']=abs(df['Error'])\n",
    "    df['Percentage error']=df['Error']/ df[observed_col]\n",
    "    df['Absolute Percentage error']=df['Absolute error']/df[observed_col]\n",
    "    \n",
    "    results=df[['Error','Squared error','Absolute error','Percentage error','Absolute Percentage error']].mean()\n",
    "    if showplot==True:\n",
    "        df[[observed_col,'modelled']].plot()\n",
    "        pyplot.show() \n",
    "    return results.to_dict()\n",
    "    \n",
    "\n",
    "\n",
    "def Fit_various_models(full_df,training_start,forecast_start,forecast_end,seasonality_lag,eval_periods,frequency='d'):\n",
    "    \n",
    "    var_to_model=full_df.columns[0]\n",
    "    \n",
    "    #Call the previously defined \"parse datasets\" function to return the full_df split into components for fitting\n",
    "    # model and forecasting\n",
    "    training, eval_actuals, forecast_actuals=parse_datasets(full_df,training_start,forecast_start\\\n",
    "                                                        ,forecast_end ,seasonality_lag,\\\n",
    "                                                            eval_duration=(eval_periods,'d'))\n",
    "\n",
    "    print('Training on dates between {} and {}'.format(training.index.min(),training.index.max()))\n",
    "    print('Evaluating error on dates between {} and {}'.format(eval_actuals.index.min(),eval_actuals.index.max()))\n",
    "    # Initialise a dictionary holding all model results\n",
    "    all_model_fits=[]\n",
    "        \n",
    "    ####### Part 1: Fit a combination of Holt Winters Models, looping through each version- additive, multiplicative \n",
    "    ####### and those same options for seasonal, or no seasonality at all\n",
    "    \n",
    "    hw_combos = list(itertools.product(['add','mul'],['add','mul','None'])) #Create all 6 combos of additive, multiplicative and True/False\n",
    "    \n",
    "    for n,model_var in enumerate(hw_combos):\n",
    "        print(\"Trying Holt Winter's Model {}, model number {}\".format(model_var,n))\n",
    "        try:\n",
    "            \n",
    "            # Creates the time series model using the ExponentialSmooothing function\n",
    "            if model_var[1]=='None':\n",
    "                model=ExponentialSmoothing(training, trend=model_var[0],freq=frequency)\n",
    " \n",
    "            else:\n",
    "                model=ExponentialSmoothing(training, trend=model_var[0],seasonal=model_var[1]\\\n",
    "                                           ,seasonal_periods=seasonality_lag,freq=frequency)\n",
    "                \n",
    "            print(\"Model fitted\")\n",
    "            # Stores the model fit attribute\n",
    "            model_fit=model.fit()\n",
    "            model_fit.summary() # Provides summary statistics\n",
    "            \n",
    "            ### Evaluation of accuracy in predicting the data in the \"evaluation\" period\n",
    "            \n",
    "            #Produce a 'forecast' for the evaluation time period\n",
    "            eval_actuals['estimate']=model_fit.predict(start=eval_actuals.index.min(), end=eval_actuals.index.max())\n",
    "            my_eval=model_fit_results(eval_actuals,var_to_model,forecast_col='estimate',showplot=False)\n",
    "            all_model_fits.append({'model':\"Holt Winter's {}\".format(model_var),\\\n",
    "                                   'AIC':model_fit.aic,\\\n",
    "                                  'avg error':my_eval['Error'],\\\n",
    "                                   'avg Squared error':my_eval['Squared error'],\\\n",
    "                                   'avg Absolute error':my_eval['Absolute error'],\\\n",
    "                                   'avg Percentage error':my_eval['Percentage error'],\\\n",
    "                                   'avg Absolute Percentage error':my_eval['Absolute Percentage error']\\\n",
    "                                  })\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #eval_estimated=pd.DataFrame(model_fit.predict(start=eval_actuals.index.min(), end=eval_actuals.index.max()))\n",
    "            # Join on the actuals so there is a dataframe with real and estimated values\n",
    "            #full_eval_df=eval_actuals.join(eval_estimated[0]).rename(columns={var_to_model:'Actual',0:'Estimated'})\n",
    "            # Call the function above to return the fit of it\n",
    "            \n",
    "\n",
    "        except: \n",
    "            print(\"Model {} Failed\".format(model_var))\n",
    "    \n",
    "    \n",
    "    ####### Part 2: Fit a range of ARIMA models\n",
    "    \n",
    "    # Define the p, d and q parameters to take any value between 0 and 2\n",
    "    p= P = range(0,3)\n",
    "    d = q = D=Q= range(0, 2)\n",
    "    s=[seasonality_lag,0]\n",
    "    # Generate all different combinations of p, q and q triplets\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    PDQ = list(itertools.product(P,D,Q,s))\n",
    "    arima_combos=list(itertools.product(pdq,PDQ))\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    # Loop through models \n",
    "    for n,model_var in enumerate(arima_combos):\n",
    "        print(\"Trying ARIMA {}, model number {}\".format(model_var,n))\n",
    "        if model_var[0]==(0,0,0):\n",
    "            pass\n",
    "        else:\n",
    "            try:  \n",
    "                model=SARIMAX(training, order=model_var[0], seasonal_order=model_var[1],freq=frequency, simple_differencing=True)\n",
    "                # Stores the model fit attribute\n",
    "                model_fit=model.fit()\n",
    "                model_fit.summary() # Provides summary statistics\n",
    "\n",
    "                ### Evaluation of accuracy in predicting the data in the \"evaluation\" period\n",
    "\n",
    "                #Produce a 'forecast' for the evaluation time period\n",
    "                eval_actuals['estimate']=model_fit.predict(start=eval_actuals.index.min(), end=eval_actuals.index.max())\n",
    "                my_eval=model_fit_results(eval_actuals,var_to_model,forecast_col='estimate',showplot=False)\n",
    "\n",
    "                all_model_fits.append({'model':\"ARIMA {}\".format(model_var),\\\n",
    "                                       'AIC':model_fit.aic,\\\n",
    "                                      'avg error':my_eval['Error'],\\\n",
    "                                       'avg Squared error':my_eval['Squared error'],\\\n",
    "                                       'avg Absolute error':my_eval['Absolute error'],\\\n",
    "                                       'avg Percentage error':my_eval['Percentage error'],\\\n",
    "                                       'avg Absolute Percentage error':my_eval['Absolute Percentage error']\\\n",
    "                                      })\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"Model {} Failed\".format(model_var))\n",
    "            \n",
    "            \n",
    "    best_AIC=[i for i in all_model_fits if i['AIC']==min([i['AIC'] for i in all_model_fits])][0]\n",
    "    print(' The best AIC was on model : {} at a value of {}. The MAPE was {}'\\\n",
    "              .format(best_AIC['model'],best_AIC['AIC'],best_AIC['avg Absolute Percentage error']))\n",
    "        \n",
    "    best_MAPE=[i for i in all_model_fits if i['avg Absolute Percentage error']==min([i['avg Absolute Percentage error'] for i in all_model_fits])][0]\n",
    "    print(' The best MAPE was on model : {} at a value of {}. The MAPE was {}'\\\n",
    "              .format(best_MAPE['model'],best_MAPE['AIC'],best_MAPE['avg Absolute Percentage error']))\n",
    "    \n",
    "    pd.DataFrame(all_model_fits).plot.scatter(x='avg Absolute Percentage error',y='AIC')\n",
    "    pyplot.show()\n",
    "    return all_model_fits\n",
    "  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with itself (yesterday)\n",
    "print(\"Correlation at lag 1 is :\",FTS_df['N'].autocorr())\n",
    "\n",
    "# Correlation with itself (7 days ago)\n",
    "print(\"Correlation at lag 6 is :\",FTS_df['N'].autocorr(6))\n",
    "\n",
    "# Correlation with exactly same record (useless sanity check)\n",
    "print(\"Correlation at lag 42 is :\",FTS_df['N'].autocorr(42))\n",
    "\n",
    "# Plot autoregression to see how it correlates with each lag\n",
    "autocorrelation_plot(FTS_df['N'])\n",
    "pyplot.show()\n",
    "\n",
    "# Get Partial ACF\n",
    "\n",
    "PACF_values=pd.DataFrame(pacf(FTS_df['N'],nlags=200)) #PACF numpy array converted to df\n",
    "PACF_values.plot() #Plot\n",
    "pyplot.show()\n",
    "\n",
    "# In this example it appears a PACF can be >1. We have some erratic values going back beyond 25 instances which mask the pattern\n",
    "sub_df=PACF_values[0:1000]\n",
    "sub_df.plot(style='.-',xlim=(0,26))\n",
    "pyplot.show()\n",
    "\n",
    "PACF_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_training, my_eval_actuals, my_forecast_actuals=parse_datasets(FTS_df,'2019-11-11 00:00:00','2020-02-29 16:00:00',\\\n",
    "                                                                 '2020-03-03 23:00:00',42,eval_duration=(42,'d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6=Fit_various_models(FTS_df,'2019-11-11','2020-02-29 16:00:00','2020-03-03 23:00:00',6,42,frequency='4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6_df=pd.DataFrame(results6)\n",
    "results6_df[(results6_df['avg Absolute Percentage error']<.25)]\\\n",
    ".sort_values(by='avg Absolute Percentage error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results42=Fit_various_models(FTS_df,'2019-11-11','2020-02-29 16:00:00','2020-03-03 23:00:00',42,42,frequency='4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results42_df=pd.DataFrame(results42)\n",
    "results42_df[(results42_df['avg Absolute Percentage error']<.25)]\\\n",
    ".sort_values(by='AIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model42=ExponentialSmoothing(my_training, trend='add',seasonal='mul',seasonal_periods=42,freq='4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit to the latest model\n",
    "print(my_training.index.min(),my_training.index.max())\n",
    "print(my_eval_actuals.index.min(),my_eval_actuals.index.max())\n",
    "print(my_forecast_actuals.index.min(),my_forecast_actuals.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training=my_training.append(my_eval_actuals)\n",
    "print(full_training.index.min(),full_training.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best_model=ExponentialSmoothing(full_training, trend='add',seasonal='mul',seasonal_periods=42,freq='4h')\n",
    "#best_model=SARIMAX(my_training, order=(0,0,1), seasonal_order=(1,0,1,6),freq='4h', simple_differencing=True)\n",
    "best_model=SARIMAX(my_training, order=(1, 0, 0), seasonal_order=(1, 0, 1, 42),freq='4h', simple_differencing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "best_model42.fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "best_model.fit().params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval_actuals['expected']=best_model.fit().predict(start=my_eval_actuals.index.min(),end=my_eval_actuals.index.max())\n",
    "\n",
    "my_forecast_actuals['expected']=best_model.fit().predict(start=my_forecast_actuals.index.min(),end=my_forecast_actuals.index.max())\n",
    "my_forecast_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval_actuals.append(my_forecast_actuals).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_eval_actuals.append(my_forecast_actuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full prediction period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pred_period=pd.DataFrame(my_eval_actuals['N'].append(my_forecast_actuals['N']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pred_period['expected']=best_model.fit().predict(start=full_pred_period.index.min(),end=full_pred_period.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_pred_period.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_pred_period['2020-02-29']['N'].sum()-full_pred_period['2020-03-01']['expected'].sum())\n",
    "print(full_pred_period['2020-03-01']['N'].sum()-full_pred_period['2020-03-01']['expected'].sum())\n",
    "print(full_pred_period['2020-03-02']['N'].sum()-full_pred_period['2020-03-01']['expected'].sum())\n",
    "print(full_pred_period['2020-03-03']['N'].sum()-full_pred_period['2020-03-01']['expected'].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results6A=Fit_various_models(FTS_df,'2019-11-11','2020-02-29 16:00:00','2020-03-03 23:00:00',seasonality_lag=6,\\\n",
    "                            eval_periods=10,frequency='4h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regressing as a function of last obs and last week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''with base as (select datetime(timestamp_add('2019-11-07 00:00:00', \n",
    "   INTERVAL cast(floor(timestamp_diff(timestamp_trunc(start_date, HOUR),'2019-11-07 00:00:00',HOUR)/4)*4 as int64) HOUR)) \n",
    "as starting_hour\n",
    ",count(distinct britbox_id) as N from \n",
    "`itv-bde-svod-prd.reporting.Entitlements_oldf_reporting`\n",
    "where date(start_date)>='2019-11-11'\n",
    "and plan_type='trial'\n",
    "group by 1)\n",
    "\n",
    "select *,\n",
    "timestamp_diff(timestamp(starting_hour),'2019-11-07 00:00:00',HOUR) as hours_since_start,\n",
    "lag(N,6) over (order by starting_hour) as yesterday,\n",
    "lag(N,42) over (order by starting_hour) as last_week\n",
    "from base\n",
    "order by 1'''\n",
    "FTS_df2 = bq.query(query).to_dataframe()\n",
    "FTS_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTS_df2=FTS_df2.set_index('starting_hour')\n",
    "FTS_df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTS_df3=FTS_df2[42:][:'2020-02-29 12:00:00']\n",
    "print(FTS_df3.index.min(),FTS_df3.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "X = FTS_df3[['hours_since_start','yesterday','last_week']]\n",
    "y = FTS_df3['N']\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict=FTS_df2['2020-02-29 12:00:00':][1:][['hours_since_start','yesterday','last_week']]\n",
    "actuals=FTS_df2['2020-02-29 12:00:00':][1:]['N']\n",
    "print(to_predict.index.min(),to_predict.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted=model.predict(to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted=pd.DataFrame(fitted,columns=['fitted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted['actuals']=actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted[:48].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_predicted['2020-03-01 20:00:00':]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is what I'm sending to BBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1=ExponentialSmoothing(full_training, trend='add',seasonal='mul',seasonal_periods=42,freq='4h')\n",
    "model2=SARIMAX(my_training, order=(0,0,1), seasonal_order=(1,0,1,6),freq='4h', simple_differencing=True)\n",
    "model3=SARIMAX(my_training, order=(1, 0, 0), seasonal_order=(1, 0, 1, 42),freq='4h', simple_differencing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval_actuals['expected']=model1.fit().predict(start=my_eval_actuals.index.min(),end=my_eval_actuals.index.max())\n",
    "my_eval_actuals['expected2']=model2.fit().predict(start=my_eval_actuals.index.min(),end=my_eval_actuals.index.max())\n",
    "my_eval_actuals['expected3']=model3.fit().predict(start=my_eval_actuals.index.min(),end=my_eval_actuals.index.max())\n",
    "\n",
    "my_forecast_actuals['expected']=model1.fit().predict(start=my_forecast_actuals.index.min(),end=my_forecast_actuals.index.max())\n",
    "my_forecast_actuals['expected2']=model2.fit().predict(start=my_forecast_actuals.index.min(),end=my_forecast_actuals.index.max())\n",
    "my_forecast_actuals['expected3']=model3.fit().predict(start=my_forecast_actuals.index.min(),end=my_forecast_actuals.index.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_eval_actuals.append(my_forecast_actuals).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "blended=my_eval_actuals.append(my_forecast_actuals)[['N','expected2','expected3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 999\n",
    "blended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
