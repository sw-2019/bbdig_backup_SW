{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import requests\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "sys.path.append(r'/Users/stepwate/Python Codes/Reusable Code')\n",
    "import google_sheets_py as gspy\n",
    "import trello_generic as tg\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import itertools\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import pacf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import FTS data into a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Google Sheets Credentials\n",
    "credentials_path='/Users/stepwate/Python Codes/Reusable Code/Credentials to Use/'\n",
    "gspy.authenticate_sheets(credentials_path, credentials_file_name='credentials.json',scope='readwrite')\n",
    "FTS=gspy.read_google_sheets_as_rows('1e--ETf-JLnQe-VLfwZP0RSiNRrzP9QbdAUHHFc5Bm3c','Data sheet 4 hourly',credentials_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As row headers are first entry in list of lists, transform to a dictionary format which is better read by pandas\n",
    "row_dict={}\n",
    "for n,y in enumerate(FTS[0]):\n",
    "    row_dict[y]=[x[n] for i,x in enumerate(FTS) if i>0]\n",
    "\n",
    "FTS_temp =pd.DataFrame(row_dict)  \n",
    "\n",
    "FTS_temp = FTS_temp[['starting_hour','N_custs']]\n",
    "\n",
    "# Map reporting date as date time in correct format\n",
    "FTS_temp['reporting date']=pd.to_datetime(FTS_temp['starting_hour'],format='%d/%m/%Y %H:%M:%S')\n",
    "\n",
    "# Set reporting date as index too to enable various pandas functions\n",
    "FTS_temp.index=FTS_temp['reporting date']\n",
    "\n",
    "# Map variable types\n",
    "FTS_temp=FTS_temp.astype({'N_custs':'float64'})\n",
    "\n",
    "\n",
    "FTS_df = pd.DataFrame(FTS_temp['N_custs']).rename(columns={'N_custs':'N'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTS_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Create different datasets\n",
    "Once the data is in the format of a single column with the time/date values in the index, you can split it into three components:\n",
    "1) The data used to train the model\n",
    "2) The data used to evaluate the model\n",
    "3) The \"actuals\" for the period you forecast.\n",
    "\n",
    "Note that a set of data is better used for evaluation (2) because the nature of time series is that they use past observations. Therefore after a few forecast periods foreward, they tend to converge on a repeating pattern as there is no \"real\" data to show variance. As such a model can fit extremely well but still veer off when used for forecasting. By using it to model e.g. a week of known values, we can get a sense of how much it diverges.\n",
    "\n",
    "3) is only relevant where you are trying to measure uplift by comparing observed vs expected. If doing a genuine forecast into the future, there will be no \"actuals\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_datasets(complete_df, training_start, forecast_start, forecast_end, seasonality_period\\\n",
    "                   ,eval_duration=(1,'s'),training_end=None):\n",
    "    \n",
    "    # Aim is to create three datasets:\n",
    "    # Training (the period to build the model off of)\n",
    "    # Eval actuals (a period just before the model is build on which to test the model fit)\n",
    "    # Forecast actuals (a period that you're going to predict. Actuals will only exist if you are using it for incremental analysis)\n",
    "    \n",
    "    # If NO training_end specified, the training dataset and the evaluation dataset will be mutually exclusive, \n",
    "    # i.e. training will end where eval begins. If it is explicitly specified, there will be some overlap\n",
    "    \n",
    "    # Remove the section to be forecasted\n",
    "    partial_df=complete_df[training_start:forecast_start][:-1]\n",
    "    try:\n",
    "        forecast_actuals=complete_df[forecast_start:forecast_end]\n",
    "    except:\n",
    "        forecast_actuals=None\n",
    "    \n",
    "    #\n",
    "    if eval_duration[1]=='s':\n",
    "        if training_end==None:\n",
    "            training_df=partial_df[:-((eval_duration[0]*seasonality_period))]\n",
    "        else:\n",
    "            training_df=partial_df[:training_end]\n",
    "            \n",
    "        eval_actuals=partial_df[-((eval_duration[0]*seasonality_period)):]\n",
    "    \n",
    "    else:\n",
    "        if training_end==None:\n",
    "            training_df=partial_df[:-((eval_duration[0]))]\n",
    "        else:\n",
    "            training_df=partial_df[:training_end]\n",
    "        eval_actuals=partial_df[-((eval_duration[0])):]\n",
    "\n",
    "    return (training_df,eval_actuals,forecast_actuals)\n",
    "\n",
    "def model_fit_results(df,observed_col,forecast_col=None,resid_col=None,showplot=True):\n",
    "    if resid_col and forecast_col:\n",
    "        df['modelled'] = df[forecast_col]\n",
    "        df['Error']=df[resid_col]\n",
    "    elif resid_col:\n",
    "        df['Error']=df[resid_col]\n",
    "        df['modelled'] = df[observed_col]-df['Error']        \n",
    "    elif forecast_col:\n",
    "        df['modelled'] = df[forecast_col]\n",
    "        df['Error']=df[observed_col]-df['modelled']\n",
    "    else:\n",
    "        print(\"Error: you need either the forecasted values or the residuals to show model fit results\")\n",
    "        return\n",
    "    \n",
    "df['Squared error']=df['Error']**2\n",
    "df['Absolute error']=abs(df['Error'])\n",
    "df['Percentage error']=df['Error']/ df[observed_col]\n",
    "df['Absolute Percentage error']=df['Absolute error']/df[observed_col]\n",
    "\n",
    "    results=df[['Error','Squared error','Absolute error','Percentage error','Absolute Percentage error']].mean()\n",
    "    if showplot==True:\n",
    "        df[[observed_col,'modelled']].plot()\n",
    "        pyplot.show() \n",
    "    return results.to_dict()\n",
    "    \n",
    "\n",
    "\n",
    "def Fit_various_models(full_df,training_start,forecast_start,forecast_end,seasonality_lag,eval_periods,frequency='d'):\n",
    "    \n",
    "    var_to_model=full_df.columns[0]\n",
    "    \n",
    "    #Call the previously defined \"parse datasets\" function to return the full_df split into components for fitting\n",
    "    # model and forecasting\n",
    "    training, eval_actuals, forecast_actuals=parse_datasets(full_df,training_start,forecast_start\\\n",
    "                                                        ,forecast_end ,seasonality_lag,eval_duration=(eval_periods,'d'))\n",
    "\n",
    "    \n",
    "    # Initialise a dictionary holding all model results\n",
    "    all_model_fits=[]\n",
    "        \n",
    "    ####### Part 1: Fit a combination of Holt Winters Models, looping through each version- additive, multiplicative \n",
    "    ####### and those same options for seasonal, or no seasonality at all\n",
    "    \n",
    "    hw_combos = list(itertools.product(['add','mul'],['add','mul','None'])) #Create all 6 combos of additive, multiplicative and True/False\n",
    "    \n",
    "    for n,model_var in enumerate(hw_combos):\n",
    "        print(\"Trying Holt Winter's Model {}, model number {}\".format(model_var,n))\n",
    "        try:\n",
    "            \n",
    "            # Creates the time series model using the ExponentialSmooothing function\n",
    "            if model_var[1]=='None':\n",
    "                model=ExponentialSmoothing(training, trend=model_var[0],freq=frequency)\n",
    " \n",
    "            else:\n",
    "                model=ExponentialSmoothing(training, trend=model_var[0],seasonal=model_var[1]\\\n",
    "                                           ,seasonal_periods=seasonality_lag,freq=frequency)\n",
    "                \n",
    "            print(\"Model fitted\")\n",
    "            # Stores the model fit attribute\n",
    "            model_fit=model.fit()\n",
    "            model_fit.summary() # Provides summary statistics\n",
    "            \n",
    "            ### Evaluation of accuracy in predicting the data in the \"evaluation\" period\n",
    "            \n",
    "            #Produce a 'forecast' for the evaluation time period\n",
    "            eval_actuals['estimate']=model_fit.predict(start=eval_actuals.index.min(), end=eval_actuals.index.max())\n",
    "            my_eval=model_fit_results(eval_actuals,var_to_model,forecast_col='estimate',showplot=False)\n",
    "            all_model_fits.append({'model':\"Holt Winter's {}\".format(model_var),\\\n",
    "                                   'AIC':model_fit.aic,\\\n",
    "                                  'avg error':my_eval['Error'],\\\n",
    "                                   'avg Squared error':my_eval['Squared error'],\\\n",
    "                                   'avg Absolute error':my_eval['Absolute error'],\\\n",
    "                                   'avg Percentage error':my_eval['Percentage error'],\\\n",
    "                                   'avg Absolute Percentage error':my_eval['Absolute Percentage error']\\\n",
    "                                  })\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            #eval_estimated=pd.DataFrame(model_fit.predict(start=eval_actuals.index.min(), end=eval_actuals.index.max()))\n",
    "            # Join on the actuals so there is a dataframe with real and estimated values\n",
    "            #full_eval_df=eval_actuals.join(eval_estimated[0]).rename(columns={var_to_model:'Actual',0:'Estimated'})\n",
    "            # Call the function above to return the fit of it\n",
    "            \n",
    "\n",
    "        except: \n",
    "            print(\"Model {} Failed\".format(model_var))\n",
    "    \n",
    "    \n",
    "    ####### Part 2: Fit a range of ARIMA models\n",
    "    \n",
    "    # Define the p, d and q parameters to take any value between 0 and 2\n",
    "    p= P = range(0,3)\n",
    "    d = q = D=Q= range(0, 2)\n",
    "    s=[seasonality_lag,0]\n",
    "    # Generate all different combinations of p, q and q triplets\n",
    "    pdq = list(itertools.product(p, d, q))\n",
    "    PDQ = list(itertools.product(P,D,Q,s))\n",
    "    arima_combos=list(itertools.product(pdq,PDQ))\n",
    "    \n",
    "    \n",
    "\n",
    "   \n",
    "    # Loop through models \n",
    "    for n,model_var in enumerate(arima_combos):\n",
    "        print(\"Trying ARIMA {}, model number {}\".format(model_var,n))\n",
    "        if model_var[0]==(0,0,0):\n",
    "            pass\n",
    "        else:\n",
    "            try:  \n",
    "                model=SARIMAX(training, order=model_var[0], seasonal_order=model_var[1],freq=frequency, simple_differencing=True)\n",
    "                # Stores the model fit attribute\n",
    "                model_fit=model.fit()\n",
    "                model_fit.summary() # Provides summary statistics\n",
    "\n",
    "                ### Evaluation of accuracy in predicting the data in the \"evaluation\" period\n",
    "\n",
    "                #Produce a 'forecast' for the evaluation time period\n",
    "                eval_actuals['estimate']=model_fit.predict(start=eval_actuals.index.min(), end=eval_actuals.index.max())\n",
    "                my_eval=model_fit_results(eval_actuals,var_to_model,forecast_col='estimate',showplot=False)\n",
    "\n",
    "                all_model_fits.append({'model':\"ARIMA {}\".format(model_var),\\\n",
    "                                       'AIC':model_fit.aic,\\\n",
    "                                      'avg error':my_eval['Error'],\\\n",
    "                                       'avg Squared error':my_eval['Squared error'],\\\n",
    "                                       'avg Absolute error':my_eval['Absolute error'],\\\n",
    "                                       'avg Percentage error':my_eval['Percentage error'],\\\n",
    "                                       'avg Absolute Percentage error':my_eval['Absolute Percentage error']\\\n",
    "                                      })\n",
    "\n",
    "\n",
    "            except:\n",
    "                print(\"Model {} Failed\".format(model_var))\n",
    "            \n",
    "            \n",
    "    best_AIC=[i for i in all_model_fits if i['AIC']==min([i['AIC'] for i in all_model_fits])][0]\n",
    "    print(' The best AIC was on model : {} at a value of {}. The MAPE was {}'\\\n",
    "              .format(best_AIC['model'],best_AIC['AIC'],best_AIC['avg Absolute Percentage error']))\n",
    "        \n",
    "    best_MAPE=[i for i in all_model_fits if i['avg Absolute Percentage error']==min([i['avg Absolute Percentage error'] for i in all_model_fits])][0]\n",
    "    print(' The best MAPE was on model : {} at a value of {}. The MAPE was {}'\\\n",
    "              .format(best_MAPE['model'],best_MAPE['AIC'],best_MAPE['avg Absolute Percentage error']))\n",
    "    \n",
    "    pd.DataFrame(all_model_fits).plot.scatter(x='avg Absolute Percentage error',y='AIC')\n",
    "    pyplot.show()\n",
    "    return all_model_fits\n",
    "  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with itself (yesterday)\n",
    "print(\"Correlation at lag 1 is :\",FTS_df['N'].autocorr())\n",
    "\n",
    "# Correlation with itself (7 days ago)\n",
    "print(\"Correlation at lag 24 is :\",FTS_df['N'].autocorr(24))\n",
    "\n",
    "# Correlation with exactly same record (useless sanity check)\n",
    "print(\"Correlation at lag 168 is :\",FTS_df['N'].autocorr(168))\n",
    "\n",
    "# Plot autoregression to see how it correlates with each lag\n",
    "autocorrelation_plot(FTS_df['N'])\n",
    "pyplot.show()\n",
    "\n",
    "# Get Partial ACF\n",
    "\n",
    "PACF_values=pd.DataFrame(pacf(FTS_df['N'],nlags=200)) #PACF numpy array converted to df\n",
    "PACF_values.plot() #Plot\n",
    "pyplot.show()\n",
    "\n",
    "# In this example it appears a PACF can be >1. We have some erratic values going back beyond 25 instances which mask the pattern\n",
    "sub_df=PACF_values[0:1000]\n",
    "sub_df.plot(style='.-',xlim=(0,26))\n",
    "pyplot.show()\n",
    "\n",
    "PACF_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training_start_date='2019-11-11'\n",
    "#forecast_start_date='2020-02-29 19:00:00'\n",
    "#forecast_end_date='2020-03-03 23:00:00'\n",
    "\n",
    "my_training, my_eval_actuals, my_forecast_actuals=parse_datasets(FTS_df,'2019-11-11','2020-02-29 16:00:00',\\\n",
    "                                                                 '2020-03-03 23:00:00',42,eval_duration=(42,'d'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6=Fit_various_models(FTS_df,'2019-11-11','2020-02-29 16:00:00','2020-03-03 23:00:00',6,42,frequency='4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6_df=pd.DataFrame(results6)\n",
    "results6_df[(results6_df['AIC']<6200) &(results6_df['avg Absolute Percentage error']<.2)]\\\n",
    ".sort_values(by='avg Absolute Percentage error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I tried 21 because of the PACF but results were garbage. \n",
    "results42=Fit_various_models(FTS_df,'2019-11-11','2020-02-29 16:00:00','2020-03-03 23:00:00',42,168,frequency='4h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "results42_df=pd.DataFrame(results42)\n",
    "results42_df[(results42_df['avg Absolute Percentage error']<.5)]\\\n",
    ".sort_values(by='avg Absolute Percentage error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "results6[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the best models and store them\n",
    "best_model6=ExponentialSmoothing(my_training, trend='add',seasonal='mul',seasonal_periods=6,freq='4h')\n",
    "best_model6v2=ExponentialSmoothing(my_training, trend='add',seasonal='mul',seasonal_periods=6,freq='4h')\n",
    "\n",
    "best_model42=SARIMAX(my_training, order=(0,0,1), seasonal_order=(1,0,1,42),freq='4h', simple_differencing=True)\n",
    "\n",
    "\n",
    "# Add evaluation period data back into the model\n",
    "training_full_df=pd.DataFrame(my_training['N'].append(my_eval_actuals['N']))\n",
    "\n",
    "\n",
    "best_model6_refitted=ExponentialSmoothing(my_training, trend='add',seasonal='mul',seasonal_periods=6,freq='4h').fit(\\\n",
    "smoothing_level=best_model6.fit().params['smoothing_level'],\\\n",
    "smoothing_slope=best_model6.fit().params['smoothing_slope'],\\\n",
    "smoothing_seasonal=best_model6.fit().params['smoothing_seasonal'],\\\n",
    "damping_slope=best_model6.fit().params['damping_slope'],\\\n",
    "use_boxcox=best_model6.fit().params['use_boxcox'],\\\n",
    "#start_params= best_model6.fit().params['initial_seasons'],\\\n",
    "remove_bias=best_model6.fit().params['remove_bias'],\\\n",
    "initial_level=best_model6.fit().params['initial_level'],\\\n",
    "#initial_seasons=best_model6.fit().params['initial_seasons'],\\\n",
    "initial_slope=best_model6.fit().params['initial_slope'])\n",
    "\n",
    "\n",
    "best_model6_refitted_full=ExponentialSmoothing(training_full_df, trend='add',seasonal='mul',seasonal_periods=6,freq='4h').fit(\\\n",
    "smoothing_level=best_model6.fit().params['smoothing_level'],\\\n",
    "smoothing_slope=best_model6.fit().params['smoothing_slope'],\\\n",
    "smoothing_seasonal=best_model6.fit().params['smoothing_seasonal'],\\\n",
    "damping_slope=best_model6.fit().params['damping_slope'],\\\n",
    "use_boxcox=best_model6.fit().params['use_boxcox'],\n",
    "remove_bias=best_model6.fit().params['remove_bias'],\\\n",
    "initial_level=best_model6.fit().params['initial_level'],\\\n",
    "                                                                                                                         \n",
    "initial_slope=best_model6.fit().params['initial_slope'])\n",
    "\n",
    "\n",
    "best_model42_refitted=SARIMAX(my_training, order=(0,0,1), seasonal_order=(1,0,1,42),freq='4h', simple_differencing=True)\n",
    "best_model42_refitted_full=SARIMAX(training_full_df, order=(0,0,1), seasonal_order=(1,0,1,42),freq='4h', simple_differencing=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model42.fit().params)\n",
    "print(best_model42_refitted.fit().params)\n",
    "print(best_model42_refitted_full.fit().params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model6.fit().params)\n",
    "print(best_model6v2.fit().params)\n",
    "print(best_model6_refitted.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model6_refitted.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model6.fit().summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Want all values to be the same because the training data set is always the same (haven't used the new one yet)\n",
    "Forecast_df=my_forecast_actuals.copy()\n",
    "Forecast_df['best6']=best_model6.fit().predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "Forecast_df['best6 refitted to training']=best_model6_refitted.predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "Forecast_df['best6 dupe']=best_model6v2.fit().predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "Forecast_df['best6 refitted to full']=best_model6_refitted_full.predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "Forecast_df['best42 ARIMA']=best_model42_refitted_full.fit().predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "Forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option('display.max_rows', 700)\n",
    "training2=my_training.copy()\n",
    "training2['best6 fitted']=best_model6.fit().fittedvalues\n",
    "training2['best6 resid']=best_model6.fit().resid\n",
    "training2['best6 dupe fitted']=best_model6v2.fit().fittedvalues\n",
    "training2['best6 dupe resid']=best_model6v2.fit().resid\n",
    "training2['best6 refitted fitted']=best_model6_refitted.fittedvalues\n",
    "training2['best6 refitted resid']=best_model6_refitted.resid\n",
    "training2['best42 refitted fitted']=best_model42_refitted_full.fit().fittedvalues\n",
    "training2['best42 refitted resid']=best_model42_refitted_full.fit().resid\n",
    "\n",
    "training2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best 42 model\n",
    "\n",
    "final=training_full_df.copy()\n",
    "final['expected']=best_model42_refitted_full.fit().fittedvalues\n",
    "\n",
    "\n",
    "Forecast=my_forecast_actuals.copy()\n",
    "Forecast['expected']=best_model42_refitted_full.fit().predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "\n",
    "\n",
    "final['2020-02-01':].append(Forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit model to latest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SARIMAX(my_training, order=(0,0,1), seasonal_order=(1,0,1,42),freq='4h', simple_differencing=True)\n",
    "#model=ExponentialSmoothing(my_training, trend='add',seasonal='mul'\\\n",
    "#                                           ,seasonal_periods=42,freq='4h')\n",
    "                \n",
    "# Stores the model fit attribute\n",
    "model_fit=model.fit()\n",
    "print(model_fit.summary()) # Provides summary statistics\n",
    "\n",
    "                ### Evaluation of accuracy in predicting the data in the \"evaluation\" period\n",
    "\n",
    "                #Produce a 'forecast' for the evaluation time period\n",
    "my_eval_actuals['estimate']=model_fit.predict(start=my_eval_actuals.index.min(), end=my_eval_actuals.index.max())\n",
    "my_eval=model_fit_results(my_eval_actuals,'N',forecast_col='estimate',showplot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_do_forecast_df=pd.DataFrame(my_training['N'].append(my_eval_actuals['N']))\n",
    "to_do_forecast_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.aic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "newly_fitted=ExponentialSmoothing(to_do_forecast_df).fit(\\\n",
    "smoothing_level=model_fit.params['smoothing_level'],\\\n",
    "smoothing_slope=model_fit.params['smoothing_slope'],\\\n",
    "smoothing_seasonal=model_fit.params['smoothing_seasonal'],\\\n",
    "damping_slope=model_fit.params['damping_slope'],\\\n",
    "use_boxcox=model_fit.params['use_boxcox'],\\\n",
    "remove_bias=model_fit.params['remove_bias'],\\\n",
    "initial_level=model_fit.params['initial_level'],\\\n",
    "initial_slope=model_fit.params['initial_slope'])\n",
    "\n",
    "newly_fitted.predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecast_actuals['expected']=model_fit.predict(start='2020-02-29 16:00:00',end='2020-03-03 20:00:00')\n",
    "my_forecast_actuals.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecast_actuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "### I have two models- one trained up to 22nd, other trained up to 29th.\n",
    "### I want the model trained up to 22nd in terms of parameters etc., as that was the one evaluated\n",
    "###, but modelling using data up to 29th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(ExponentialSmoothing.from_formula)\n",
    "#(model_fit,my_forecast_actuals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_forecast_actuals['expected']=model_fit.predict(start='2020-03-01 20:00:00',end='2020-03-03 20:00:00')\n",
    "my_forecast_actuals.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing Data\n",
    "#### Autocorrelation \n",
    "is the correlation between a given point in time Tx and a previous point in time Tx-n . Autocovariance is related in the usual way covariance and correlation are.\n",
    "The AutoCorrelation Function is the set of autocorrelations i.e. where n=1 to n=infinity\n",
    "\n",
    "#### Decomposition \n",
    "is the process of breaking a time series into component parts, namely:\n",
    "##### $Y_t$ (observed value at time T)\n",
    "##### $S_t$ (seasonal component at time T)\n",
    "##### $T_t$ (trend cycle component at time T)\n",
    "##### $ε_t$ (irregular or error component at time T)\n",
    "\n",
    "\n",
    "#### Additive vs Multiplicative Decomposition \n",
    "We have additive decomposition if:\n",
    "$Y_t = S_t + T_t + ε_t$\n",
    "\n",
    "and we have additive decomposition if:\n",
    "$Y_t = S_t * T_t * ε_t$\n",
    "\n",
    "\n",
    "#### Smoothing for moving averages\n",
    "The first step in decomposing a time series is to estimate the trend cycle Tt. This is done using weighted moving averages. A traditional, simple, moving average can be described as a weighted-moving average whereby all the weights are the same. Weighted ones would typically weight the current observation and those nearest in time to it ($n+1$ or $n-1$) higher than those further from it (e.g. $n-5$). \n",
    "For analysis purposes it is ok to use the subsequent observations in a moving average, i.e. the window can look forwards as well as backwards to isolate the trend component.\n",
    "\n",
    "#### Calculating seasonal component\n",
    "You can subtract the Trend component, $T_t$ (i.e. the moving average part) from the overall observed $Y_t$ to be left with $ S_t + ε_t$ . The $S_t$ can then be thought of as the average of each seasonal period e.g. the average value seen in December (for multi-year trends) or the average \"Monday\" for weekly patterns. Importantly, you want the average of the Seasonal+Error ($ S_t + ε_t$ ), not the overall $Y_t$.\n",
    "This average becomes the seasonal part\n",
    "The remaining part, error, $ε_t$ should theoretically be randomly distributed. However in real-world stuff I'm not sure how that works as e.g. you would expect Marketing to provide \"Error\" to the model- but structured Error.\n",
    "\n",
    "N.B. If building a multiplicative rather than an additive model, you would divide the Trend component to get a ratio, rather than subtract.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting Data\n",
    "The above helps describe what we've seen but doesn't provide a means of forecast, or even a means of determining uplift e.g. for Campaign Eval purposes (as that uplift will be baked into the Moving Average Trend).\n",
    "\n",
    "\n",
    "The simplest forecast is a moving average, whereby $Y_{t+1}$ is an average of $Y_t$ back to $Y_{t-n}$. \n",
    "\n",
    "#### Stationarity\n",
    "Moving averages smooth over trends and seasonal differences, so are a very crude forecast. You can also only forecast one step ahead. \n",
    "Effectively they are only useful if data is \"Stationary\"- which means that $Y_t$ is not dependent on $t$. A stationary data set does not contain a trend or seasonal component.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error evaluations\n",
    "- ME: Mean Error (average error)\n",
    "- MAE: Mean Absolute Error\n",
    "- MSE: Mean Squared Error (square first then take the mean of it!)\n",
    "- MPE: Mean Percentage error (as a % of the observed)\n",
    "- MAPE: Mean Absolute Percentage Error\n",
    "\n",
    "Also consider...:\n",
    "- Max Error\n",
    "\n",
    "Note: assuming errors are normally distributed, which in theory they should be, then $F_{t+1} +/- Z* \\surd (MSE)$ gives approximate confidence intervals.\n",
    "\n",
    "$Z= 1.645$ for 90%, $Z=1.960$ for 95% and $Z=2.576$  for 99%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fit_various_models(FTS_df,'2019-11-11','2020-02-29 19:00:00','2020-03-03 23:00:00',168,frequency='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Fit_Selected_HW(full_df,training_start,forecast_start,forecast_end,var_to_forecast,periods_seasonality):\n",
    "    \n",
    "    model = ExponentialSmoothing(training_df, trend='mul',seasonal='mul',seasonal_periods=periods_seasonality)\n",
    "    model_fit=model.fit()\n",
    "    \n",
    "    training_df['residuals'] = pd.DataFrame(model_fit.resid)\n",
    "    my_eval=model_fit_results(training_df,var_to_forecast,resid_col='residuals')\n",
    "    print(my_eval)\n",
    "    return training_df, model_fit\n",
    "\n",
    "def Fit_Selected_ARIMAs(full_df,training_start,forecast_start,forecast_end,var_to_forecast,pdq,PDQS):\n",
    "    \n",
    "    training_df=full_df[training_start:forecast_start][[var_to_forecast]][:-1] # Create a df without the forecasted values\n",
    "    model=SARIMAX(training_df, order=pdq, seasonal_order=PDQS)\n",
    "    model_fit=model.fit()\n",
    "    training_df['residuals'] = pd.DataFrame(model_fit.resid)\n",
    "    my_eval=model_fit_results(training_df,var_to_forecast,resid_col='residuals')\n",
    "    return training_df, model_fit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Expontential Smoothing\n",
    "Where F is the forecast value:\n",
    "\n",
    "$F_{t+1} = αY_t + (1-α) F_t$\n",
    "\n",
    "It can only be used to predict one step ahead. Effectively it combined a weighted blen of the current observation and the forecasted value of the current observation (which in turn, was dependent on the previous value, and so on.\n",
    "A high α value means that the current actual is a powerful preditor, and the \"history\" less so. A low α basically upweights prior forecasts, which are products of previous observations.\n",
    "\n",
    "The model must be initialised, as obviously there is no F1. There are lots of things you can do but a good place to start is $F_2=Y_1$ (i.e. forecast for period 2= observed for period 1).\n",
    "\n",
    "You would iterate over lots of values of α such that you minimised the mean squared error between observed and forecast values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holt's Linear Exponential Smoothing\n",
    "An extension of exponential smoothing to account for a local, linear trend. The trend makes it possible to forecast m time period ahead. There are two smoothing constants, α and β.\n",
    "\n",
    "## Holt- Winter's Method: Additive & Multiplicative\n",
    "These extend the models further to also include Seasonality, where k= the number of periods in one cycle of seasons e.g. 7 days in a week or 12 months in a year.\n",
    "To initialise, we need one complete cycle of data, $s$. E.g. for weekly seasonality we need at least 7 values, obviously. \n",
    "Further to this, to initialise trend, we need $s+k$ observations. This is because for trend there needs to be some sense of change. At absolute minimum you can use $k=1$ so e.g. you would compare two mondays and assume their difference to be the underlying trend. Ideally $s=k$ so that for each seasonal variant, there is a \"trend\".\n",
    "\n",
    "There are three parameters, α,β and γ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ops=['add','mul'] # List the options (additive and multiplicative)\n",
    "hw_combos = itertools.product([True,False],ops,ops) #Create all 8 combos of additive, multiplicative and True/False\n",
    "list(hw_combos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_final_df,hw_selected_model_fit=Fit_Selected_HW(full_df=FTS_df,\\\n",
    "training_start='2019-11-10',\\\n",
    "forecast_start='2020-02-29 19:00:00',\\\n",
    "forecast_end='2020-03-05 12:00:00',\\\n",
    "var_to_forecast='N',periods_seasonality=168)\n",
    "\n",
    "hw_selected_model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_forecast = hw_selected_model_fit.predict(start='2020-02-29 19:00:00', end='2020-03-05 12:00:00')\n",
    "hw_actuals= FTS_df['2020-02-29 19:00:00':'2020-03-05 12:00:00']\n",
    "\n",
    "\n",
    "actual_vs_forecast=pd.DataFrame(hw_final_df['modelled'].append(hw_forecast)).join(pd.DataFrame(FTS_df['N']))\n",
    "actual_vs_forecast['2020-02-21':].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_forecast['2020-02-28':]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_uplift=pd.DataFrame(hw_forecast).join(hw_actuals).rename(columns={0:'expected'})\n",
    "hw_uplift['uplift']=hw_uplift['N']-hw_uplift['expected']\n",
    "hw_uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_uplift.uplift.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_uplift['2020-03-05':'2020-03-06'].uplift.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_vs_forecast[['actual','expected']]['2020-02-28':'2020-03-05'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA\n",
    "Based on the Autocorrelation Function (ACF) and is more refined than a moving averages method.\n",
    "To build an ARIMA, there are a number of steps to take, in addition to those used in e.g. a Moving Averages method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Step 1: Autocorrelation\n",
    "Auto correlation is the correlation between an observation and another observation in the same series at a different point in time, e.g. between $Y_t$ and $Y_{t-7}$.\n",
    "\n",
    "The Autocorrelation Function (ACF) is the full distribution of correlation coefficients up to $t-N$ moments in time ago.\n",
    "However the ACF doesn't tell the full story. Many $Y_t$ values will be highly correlated with $Y_{t-1}$, for example. And so that means that because $Y_{t-1}$ is highly correlated with $Y_{t-2}$, there will also be a decent agreement between $Y_t$ and $Y_{t-2}$. This can mask true repeating correlations. \n",
    "\n",
    "Therefore we have the:\n",
    "Partial Autocorrelation Function (PACF).\n",
    "\n",
    "This produces an ACF that removes the effect of other time lags and produces something independent. This is called \"partialling out\". The PACF is calculated using an AutoRegression (AR), which is literally a regression on the previous Yt-n values. PACF is derived from the coefficients of that regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new clean dataset to test Arima\n",
    "ARIMA_training=pd.DataFrame(training['N']) #Must specify the \"pd.DataFrame\" else it'll return a Series which has different properties\n",
    "\n",
    "# Correlation with itself (yesterday)\n",
    "print(\"Correlation at lag 1 is :\",ARIMA_training['N'].autocorr())\n",
    "\n",
    "# Correlation with itself (7 days ago)\n",
    "print(\"Correlation at lag 24 is :\",ARIMA_training['N'].autocorr(24))\n",
    "\n",
    "# Correlation with exactly same record (useless sanity check)\n",
    "print(\"Correlation at lag 168 is :\",ARIMA_training['N'].autocorr(168))\n",
    "\n",
    "# Plot autoregression to see how it correlates with each lag\n",
    "autocorrelation_plot(ARIMA_training['N'])\n",
    "pyplot.show()\n",
    "\n",
    "# Get Partial ACF\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "\n",
    "PACF_values=pd.DataFrame(pacf(ARIMA_training['N'],nlags=200)) #PACF numpy array converted to df\n",
    "PACF_values.plot() #Plot\n",
    "pyplot.show()\n",
    "\n",
    "# In this example it appears a PACF can be >1. We have some erratic values going back beyond 25 instances which mask the pattern\n",
    "sub_df=PACF_values[0:1000]\n",
    "sub_df.plot(style='.-',xlim=(0,26))\n",
    "pyplot.show()\n",
    "\n",
    "PACF_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Step 2: White Noise\n",
    "A forecasting model is deemed 'well suited' if the forecast errors are purely random. If this is the case, the residuals are described as \"white noise\".\n",
    "If a time series is white noise, both AC and PAC coefficients are approximately independent and normally distributed with mean 0 and standard deviation of $1/\\surd n$.\n",
    "Hence it is useful to plot ACF and PACF with a range of $+/- 1.96/\\surd(n)$. Coefficients outside these boundaries are probably not white noise i.e. significant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "1.96*math.sqrt(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Step 3: Recognising non-stationarity\n",
    "First you have to examine the time plot. If the mean or variance change with time it isn't stationary.\n",
    "ACF and PACF also give evidence, as autocorrelations of stationary data drop to zero quite quickly, whilst those for non-stationary data can take a number of lags to do so. The PACF of a non-stationary time series will typically have a large coefficient close to 1 at lag 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Step 4: Removing non-stationarity- by Differencing\n",
    "It is important to remove trends from time series data prior to model building since such autocorrelations dominate the ACF. This can be done by differencing. \n",
    "A differenced series is the change in each observation in a time series e.g. $Y_t'= Y_t - Y_{t-1}$\n",
    "\n",
    "\n",
    "\n",
    "Occasionally, removing such differences still doesn't result in non-stationarity, i.e. there is a trend in the changes. Ordinarily, a second round of differencing does the trick i.e. $Y_t''= Y_t'- Y_{t-1}'$\n",
    "Basically you difference the already-differenced data.\n",
    "\n",
    "When the differenced data is effectively white noise you know you've done it enough.\n",
    "\n",
    "\n",
    "### Seasonal Differencing\n",
    "\n",
    "Seasonal Differencing is the same process but rather than taking Yt- Yt-1, you do Yt- Yt-n where n= the number of periods after which seasonality plays a part, e.g. 7 days in a week, 12 months in a year. It can be repeated to obtain second order seasonal differencing but this is rarely needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA Step 5: Unpicking an ARIMA Model\n",
    "\n",
    "ARIMA stands for AR - I - MA\n",
    "or \n",
    "Autoregression - I (differences) - Moving Average\n",
    "\n",
    "#### AR\n",
    "An AR model basically predicts $Y_t$ using a regression of  \n",
    "$ β_1 Y_{t-1} + β_2 Y_{t-2} + ... β_p Y_{t-p} + c + ε_{t} $\n",
    "\n",
    "It takes one parameter ($p$)- which is the number of time lags you use to make a the prediction.\n",
    "\n",
    "There are constraints on the coefficients:\n",
    "- If $p=1$ then $-1 < β_1 < 1$\n",
    "- If $p=2$ then $-1 < β_2 < 1$  AND $β_1+β2<1$ AND $β_2-β_1<1$\n",
    "- If $p=3$ even more conditions hold\n",
    "\n",
    "#### I\n",
    "The \"I\" in ARIMA details the differencing applied. So if first order differencing is applied, then the parameter($d$)=1. If second order differencing is necessary then the parameter,$d$=2.\n",
    "\n",
    "#### MA\n",
    "An MA model, confusingly has nothing to do with Moving Averages in the traditional sense. Instead it predicts $Y_t$ but regressing on the historical errors. The equation looks identical to above in AR except that $Y_{t-k}$ is replaced with $ε_{t-k}$\n",
    "\n",
    "The parameter dictacting how many time lags to use is called q.\n",
    "The coefficients are again constrained.\n",
    "An MA(1) and an AR(1) model are mirror images.\n",
    "\n",
    "In truth, I'm not sure that an MA model in isolation would do much. However I think the value comes in conjunction with the AR-I parts, as it would help to ensure no systematic error or deviance over time.\n",
    "\n",
    "#### Combining for an ARIMA model $(p,d,q)$\n",
    "Combining the above creates a model whereby we have three parameters $(p,d,q)$:\n",
    "- $p$ is how many times to autoregress, i.e. how many lags on observed values $Y_{t-k}$ to use to build the model. Rarely above 2. A slow decline in ACF but fast decline in PACF characterises an AR(1) model\n",
    "- $d$ is how many time to difference the data to achieve stationarity, again rarely above 2\n",
    "- $q$ is how many lags to regress on the error term. An ACF with a single peak at lag 1, and negative PACF values dying down to 0 characterise and MA(1) model.\n",
    "\n",
    "#### ARIMA $(p,d,q) (P,D,Q)s$ models\n",
    "Standard $(p,d,q)$ models can also incorporate a seasonal component. In effect I think that the standard $(p,d,q)$ model is regressing on recent observations to predict the next, i.e. yesterday and maybe the day before.\n",
    "Adding $(P,D,Q)$ adds the seasonal component e.g. last Saturday. \n",
    "\n",
    "So for example I can predict tomorrow (Weds) by looking at today (Tuesday) and last Wednesday because of seasonality. \n",
    "I think $(P,D,Q)$ perform the same function as $(p,d,q)$ but $s$ periods ago. So $p=1$ would autoregress on yesterday, $P=2$ and $s=7$ would autoregress on 7 and 14 periods ago. I think.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARIMA: Step 6: Explaining it and writing it\n",
    "\n",
    "Convention is that ARIMA models are written using the \"backshift\" operator, $B$ which shifts data back one period, i.e.\n",
    "$BY_t =Y_{t-1}$\n",
    "\n",
    "\n",
    "Two applications of $B$ shift the data back two period. i.e. $B(BY_t) = B^2Y_t = Y_{t-2}$\n",
    "\n",
    "For monthly data, we can use the notation $B^12$ to shift back to the same month last year:\n",
    "$B^12Y_t = Y_{t-12}$\n",
    "\n",
    "Therefore in general, $B^s$ represents \"shift back $s$ time periods\".\n",
    "\n",
    "A first order *difference* is represented by $(1-B)$\n",
    "\n",
    "$Y_t' = Y_t - Y_{t-1} $ and so $= Y_t - BY_t = (1-B)Y_t$\n",
    "\n",
    "Similarly, a second order difference is represented as $(1-B)^2$\n",
    "\n",
    "A backshift notation is convenient because terms can be multiplied together to see the combined effect. For example,  a first difference followed by a first seasonal difference can be written as:\n",
    "$$(1-B)(1-B^S)Y_t = (1-B-B^S +B^{S+1})Y_t =Y_t -Y_{t-1} -Y_{t-S} +Y_{t-S+1}$$\n",
    "\n",
    "#### An AR model can be written as \n",
    "\n",
    "$$(1-\\varphi_1B - ... - \\varphi_pB^p)Y_t = c + \\epsilon_t$$\n",
    "\n",
    "(where $\\varphi$ is the regression coefficient on the previous time lags $B$)\n",
    "\n",
    "\n",
    "#### An MA model can be written as \n",
    "\n",
    "$$ Y_t = c + (1-\\theta_1B - ... - \\theta_qB^q)\\epsilon_t$$\n",
    "\n",
    "(where $\\theta$ is the regression coefficient on the previous error terms $\\epsilon$)\n",
    "\n",
    "#### An ARIMA $(p,d,q)$ model can be written as:\n",
    "\n",
    "$$(1-\\varphi_1B - ... - \\varphi_pB^p)(1-B)^dY_t=(1-\\theta_1B - ... - \\theta_qB^q)\\epsilon_t$$\n",
    "\n",
    "#### An ARIMA $(p,d,q)(P,D,Q)s$ model can be written as:\n",
    "\n",
    "$$(1-\\varphi_1B - ... - \\varphi_pB^p)(1-\\Phi_1B - ... - \\Phi_PB^SP)(1-B)^d(1-B^S)^DY_t$$\n",
    "$$=(1-\\theta_1B - ... - \\theta_qB^q)(1-\\Theta_1B^S - ... - \\Theta_QB^sQ)\\epsilon_t$$\n",
    "\n",
    "As an example, a $(0,1,1)(0,1,1)12$ model can be written as:\n",
    "$$(1-B)(1-B^{12})Y_t=(1-\\theta_1B)(1-\\Theta_1B^{12})\\epsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA Step 7: Measuring it\n",
    "In addition to the backshift notation, $\\sigma$ denotes the standard deviation of the errors, $\\epsilon_t$\n",
    "\n",
    "All the parameters $(\\varphi, \\theta, \\Theta, \\Phi) $ etc. have to be estimated. This is done using *ordinary least squares (OLS)* or *maximum likelihood estimation (MLE)*\n",
    "\n",
    "#### Portmanteau tests for white noise\n",
    "Insstead of examining each individual autocorrelation coefficient, 'portmanteau' tests can be used which consider the coefficients taken together. An example is the Ljung-Box text which employs the $Q$ statistic:\n",
    "$$ Q^*=n(n+2)\\sum_{k=1}^h(n-k)^{-1}r_k^2$$\n",
    "\n",
    "where $n$ is the number of observations,\n",
    "$h$ is the maximum time lag considered, and $r_k$ is the autocorrelation at lag $k$. \n",
    "\n",
    "If the residuals of the model are white noise (which you want) $Q^*$ has chi-square $\\chi^2$ distribution with $(h-m)$ degrees of freedom, where $m$ is the number of parameters in the model.\n",
    "\n",
    "If the $Q^*$ statistic lies in the right hand 5% tail of the $\\chi^2$ distribution (i.e. P<0.05) it is normally concluded that the data are not white noise.\n",
    "So you want it to be >0.05 but equally as this is a low bar, it isn't sufficient as a test of model quality.\n",
    "\n",
    "#### AIC for model comparison\n",
    "Several models may be a good fit, so we need to distinguish between them. Indeed the more parameters you use, the better your fit will inevitably be, but this could just be overfitting of randomness. AIC is useful because it penalises overfitting in measuring model fit.\n",
    "\n",
    "$$ AIC = -2LogL + 2m $$\n",
    "where $L$ is the likelihood and $m= p +q +P +Q$\n",
    "\n",
    "The **lower** the AIC score the better.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get modelling ARIMA!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_modelling_results=Fit_ARIMAs(full_df=FTS_df,\\\n",
    "training_start='2019-11-10',\\\n",
    "forecast_start='2020-02-29 19:00:00',\\\n",
    "forecast_end='2020-03-03 00:00:00',\\\n",
    "var_to_forecast='N',seasonality_lag=168)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_modelling_results_df=pd.DataFrame(ARIMA_modelling_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_AIC=[i for i in ARIMA_modelling_results if i['AIC']==min([i['AIC'] for i in ARIMA_modelling_results])]\n",
    "best_AIC\n",
    "best_MAPE=[i for i in ARIMA_modelling_results if i['avg Absolute Percentage error']==min([i['avg Absolute Percentage error'] for i in ARIMA_modelling_results])]\n",
    "best_MAPE\n",
    "ARIMA_modelling_results_df.plot.scatter(x='avg Absolute Percentage error',y='AIC')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_MAPE=[i for i in ARIMA_modelling_results if i['avg Absolute Percentage error']==min([i['avg Absolute Percentage error'] for i in ARIMA_modelling_results])]\n",
    "best_MAPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARIMA_modelling_results_df.plot.scatter(x='avg Absolute Percentage error',y='AIC')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\tmodel\tAIC\tavg error\tavg Squared error\tavg Absolute error\tavg Percentage error\tavg Absolute % error\n",
    "56\t((1, 0, 0), (1, 0, 1, 24))\t26070.939874\t2.620083\t970.455234\t16.953621\t-0.033887\t0.195712"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "ARIMA_modelling_results_df[(ARIMA_modelling_results_df['AIC']<26200)\\\n",
    "                           &(pd.DataFrame(ARIMA_modelling_results)['avg Absolute Percentage error']<.2)]\\\n",
    ".sort_values(by='avg Absolute Percentage error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df,selected_model_fit=Fit_Selected_ARIMAs(full_df=FTS_df,\\\n",
    "training_start='2019-11-10',\\\n",
    "forecast_start='2020-02-28 00:00:00',\\\n",
    "forecast_end='2020-03-03 23:00:00',\\\n",
    "var_to_forecast='N',pdq=(1,0,0),PDQS=(1, 0, 1, 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = selected_model_fit.predict(start='2020-02-28 00:00:00', end='2020-03-03 23:00:00')\n",
    "actuals= FTS_df['2020-02-28 00:00:00':'2020-03-03 23:00:00']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pd.DataFrame(final_df['modelled']).append(pd.DataFrame(forecast))\n",
    "actual_vs_forecast=pd.DataFrame(final_df['modelled'].append(forecast)).join(pd.DataFrame(FTS_df['N']))\n",
    "actual_vs_forecast['2020-02-27 00:00:00':].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift=pd.DataFrame(forecast).join(actuals).rename(columns={0:'expected'})\n",
    "uplift['uplift']=uplift['N']-uplift['expected']\n",
    "uplift.uplift.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uplift['2020-03-05':'2020-03-06'].uplift.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gspy.Write_whole_df_to_gsheet(actual_vs_forecast, '', 'vs forecast',\\\n",
    "                              valueInputOption='RAW', append_overwrite='overwrite', picklepath=credentials_path, headers='Y', topleftcell='A1')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
