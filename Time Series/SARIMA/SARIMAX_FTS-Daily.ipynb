{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import requests\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "import google_api_functions as gaf\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "import itertools\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Set up modules for Google functionality\n",
    "from google.cloud import bigquery # To run BQ statements\n",
    "from google_auth_oauthlib import flow # To authorise as user\n",
    "from googleapiclient.discovery import build # To pull in from sheets, slides etc. API\n",
    "from google.auth.transport.requests import Request\n",
    "from google.cloud.bigquery import magics\n",
    "import google\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(actual, estimated): \n",
    "    actual, estimated = np.array(actual), np.array(estimated)\n",
    "    return np.mean(np.abs((actual - estimated) / actual)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r\"/home/jupyter/reusable_code/\") #GAF is a package steve created with a list of useful functions\n",
    "bq = bigquery.Client(project='itv-bde-analytics-dev',credentials=creds) #Apply credentials to BQ client \"bq\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the FTS table created in BQ\n",
    "\n",
    "query=\"\"\"\n",
    "select \n",
    "  timestamp_trunc(start,DAY) as reportingDate\n",
    "  , count(*) as vol \n",
    "from `itv-bde-analytics-dev.britbox_sandbox.ss_entitlements` \n",
    "where eventSubType.reportingEvent = 'A' and billingProvider in ('Stripe', 'iTunes') \n",
    "group by \n",
    "  1\n",
    "\"\"\"\n",
    "FTS_df = bq.query(query).to_dataframe()\n",
    "FTS_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names (was relevant when using steves code)\n",
    "FTS_df = FTS_df.rename(columns={ 'reportingDate':'reporting date','vol':'N'})\n",
    "\n",
    "# Indexes the date column \n",
    "FTS_df = FTS_df.set_index(\"reporting date\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the data\n",
    "FTS_df = FTS_df.sort_index()\n",
    "FTS_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Apple Mobility Data\n",
    "mobility_data=pd.read_csv('https://covid19-static.cdn-apple.com/covid19-mobility-data/2021HotfixDev18/v3/en-us/applemobilitytrends-2020-11-19.csv')\n",
    "walkingdataUK=mobility_data[(mobility_data['region']=='United Kingdom')&(mobility_data['transportation_type']=='walking')]\n",
    "datecols=[x for x in walkingdataUK.columns if re.match('\\d{4}-\\d{2}-\\d{2}',x)]\n",
    "walkingdataUK_TS=walkingdataUK[datecols].stack().droplevel(level=0)\n",
    "walkingdataUK_TS.index=pd.to_datetime(walkingdataUK_TS.index)\n",
    "# Fill in known blanks in May 11/12 2020\n",
    "walkingdataUK_TS=walkingdataUK_TS.reindex(pd.date_range(start=walkingdataUK_TS.index.min(), end=walkingdataUK_TS.index.max()))\n",
    "walkingdataUK_TS['2020-05-11']=(walkingdataUK_TS['2020-05-10']+(1/3)*(walkingdataUK_TS['2020-05-13']-walkingdataUK_TS['2020-05-10']))\n",
    "walkingdataUK_TS['2020-05-12']=(walkingdataUK_TS['2020-05-10']+(2/3)*(walkingdataUK_TS['2020-05-13']-walkingdataUK_TS['2020-05-10']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "FTS_df=FTS_df.tz_localize(None) # Remove timezone from FTS_df index\n",
    "min_idx,max_idx=max(walkingdataUK_TS.index.min(),FTS_df.index.min()),min(walkingdataUK_TS.index.max(),FTS_df.index.max()) # Work out overlapping indexed\n",
    "# Align to get both indexes\n",
    "FTS_df=FTS_df[min_idx:max_idx]\n",
    "walkingdataUK_TS=walkingdataUK_TS[min_idx:max_idx]\n",
    "# Convert mobility\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(FTS_df)\n",
    "plt.plot((1/walkingdataUK_TS)*100000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "exogdata=pd.DataFrame((1/walkingdataUK_TS)*100000)\n",
    "exogdata['SI']=0\n",
    "exogdata.loc['2020-10-03':'2020-10-10','SI'] = [7,6,5,4,3,2,1,0]\n",
    "exogdata['2020-10-03':'2020-10-10']\n",
    "\n",
    "exogtrain,exogtest = exogdata['2019-11-11 00:00:00':'2020-10-31 20:00:00'], exogdata['2020-11-01 00:00:00':'2020-11-16 20:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative way of generating loop\n",
    "import itertools\n",
    "p_values = range(0,3)\n",
    "d_values = range(0,2)\n",
    "q_values = range(0,2)\n",
    "s_values=[7,0] #Weekly, daily, none\n",
    "pdq = list(itertools.product(p_values, d_values, q_values))\n",
    "PDQ = list(itertools.product(p_values, d_values, q_values,s_values))\n",
    "arima_combos=list(itertools.product(pdq,PDQ))\n",
    "arima_combos=[i for i in arima_combos if i[0]!=(0,0,0)] # Remove all the 0,0,0 combos\n",
    "print(len(pdq))\n",
    "print(len(arima_combos))\n",
    "\n",
    "train,test = FTS_df['2019-11-11 00:00:00':'2020-10-31 20:00:00'], FTS_df['2020-11-01 00:00:00':'2020-11-16 20:00:00']           \n",
    "Upper_Bound=train.mean()+(3*train.std())\n",
    "train['N2'] = np.where(train['N']>Upper_Bound[0], Upper_Bound[0],train['N'])\n",
    "train=train['N2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "errors=[]\n",
    "errors2=[]\n",
    "predictions = list()\n",
    "\n",
    "# Initialise best model dictionary\n",
    "best_model={'MAPE':999999999.0}\n",
    "\n",
    "# Generate dictionary of the parameters that will be needed for each iteration of the model\n",
    "ARIMA_params={i:(['ar.L{}'.format(x+1) for x in range(0,i[0])],['ma.L{}'.format(x+1) for x in range(0,i[2])]) for i in pdq}\n",
    "SARIMA_params={i:(['ar.S.L{}'.format((x+1)*i[3]) for x in range(0,i[0])],['ma.S.L{}'.format((x+1)*i[3]) for x in range(0,i[2])]) for i in PDQ}\n",
    "\n",
    "all_params=[]\n",
    "for i in arima_combos:\n",
    "    [all_params.append(item) for sublist in [x for x in ARIMA_params[i[0]]]+[y for y in SARIMA_params[i[1]]]+[['sigma2']] for item in sublist]\n",
    "all_params=all_params+['SI','0']\n",
    "start_params=pd.Series({x:0.01 for x in set(all_params)})\n",
    "\n",
    "\n",
    "for n,i in enumerate(reversed(arima_combos)):\n",
    "    print(n)\n",
    "    keep_paramids = pd.Series(['0','SI']+[item for sublist in [x for x in ARIMA_params[i[0]]]+[y for y in SARIMA_params[i[1]]]+[['sigma2']] for item in sublist])\n",
    "    print(keep_paramids)\n",
    "    if i[0]!=(0,0,0):\n",
    "        try:\n",
    "            if i[1][3]==0: \n",
    "                model=ARIMA(train,order=i[0],freq='d') # Initialise ARIMA class\n",
    "\n",
    "                print('Starting ARIMA for {}  at {}'.format(i,datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "                model_fit = model.fit(disp=0)  # Actually fit the ARIMA\n",
    "\n",
    "                print('Finished ARIMA for {}  at {}'.format(i,datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "                pred_y = model_fit.forecast(steps=len(test))[0]# create a forecast for the \"expected\" values on the Test Dataset\n",
    "\n",
    "            else:\n",
    "         #   try:\n",
    "\n",
    "                model=SARIMAX(train, exog=exogtrain,order=i[0], seasonal_order=i[1],freq='d', simple_differencing=True) # Initialise SARIMAX class\n",
    "\n",
    "                print('Starting SARIMAX for {} at {}'.format(i,datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "                #print(start_params.filter(items=keep_paramids))\n",
    "                # Actually fit the SARIMAX\n",
    "                init_params=start_params.filter(items=keep_paramids)\n",
    "                print(init_params)\n",
    "                model_fit = model.fit(disp=0,low_memory=True,start_params=init_params) #start params are filtered and sorted according to keep_paramids\n",
    "                #model_fit = model.fit(disp=0,low_memory=True) # Actually fit the SARIMAX\n",
    "                print('Finished SARIMAX for {}  at {}'.format(i,datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "                print(model_fit.params)\n",
    "\n",
    "                pred_y = model_fit.forecast(steps=len(test),exog=exogtest) # create a forecast for the \"expected\" values on the Test Dataset\n",
    "\n",
    "            predictions.append(pred_y)\n",
    "\n",
    "            error = mean_squared_error(test,pred_y)\n",
    "            error2 = mean_absolute_percentage_error(test,pred_y)\n",
    "\n",
    "            if error2*100<best_model['MAPE']:\n",
    "\n",
    "                print('{} was this model''s error, and {} is the previous best so model will update'.format(error2*100,best_model['MAPE']))\n",
    "                # Update view of what best model is\n",
    "                best_model={'model':model_fit\\\n",
    "                           ,'modelParams':model_fit.params\\\n",
    "                           ,'modelParamsConfInt':model_fit.conf_int()\\\n",
    "                            ,'ARIMA':i[0]\\\n",
    "                            ,'SARIMA':i[1]\\\n",
    "                            ,'RMSE':np.sqrt(error)\n",
    "                            ,'MAPE':error2*100\n",
    "                           }\n",
    "                # Update Start Params to hopefully optimise future loops\n",
    "                start_params=model_fit.params.combine_first(start_params)\n",
    "            errors.append(error)\n",
    "            errors2.append(error2)\n",
    "            print(i[0],i[1], np.sqrt(error), error2*100)\n",
    "        except:\n",
    "            print('Guess what? It didn''t work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(test)\n",
    "pred_y =best_model['model'].predict(start=df.index.min(), end=df.index.max(),exog=exogtest)\n",
    "df['est']=pred_y\n",
    "df.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SARIMAX(train, exog=exogtrain,order=(0,0,1), seasonal_order=(2,1,1,7),freq='d', simple_differencing=True) # Initialise SARIMAX class\n",
    "model_fit = model.fit(disp=0,low_memory=True,start_params=init_params) #start params are filtered and sorted according to keep_paramids\n",
    "print('Finished SARIMAX for {}  at {}'.format(i,datetime.now().strftime(\"%d/%m/%Y %H:%M:%S\")))\n",
    "print(model_fit.params)\n",
    "\n",
    "pred_y = model_fit.forecast(steps=len(test),exog=exogtest) # create a forecast for the \"expected\" values on the Test Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(test)\n",
    "pred_y =best_model['model'].predict(start=df.index.min(), end=df.index.max())\n",
    "df['est']=pred_y\n",
    "df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hw_combos = list(itertools.product(['add','mul'],['add','mul','None'])) #Create all 6 combos of additive, multiplicative and True/False\n",
    "    \n",
    "for n,model_var in enumerate(hw_combos):\n",
    "    print(\"Trying Holt Winter's Model {}, model number {}\".format(model_var,n))\n",
    "    #try:\n",
    "\n",
    "    # Creates the time series model using the ExponentialSmooothing function\n",
    "    if model_var[1]=='None':\n",
    "        model=ExponentialSmoothing(train, trend=model_var[0],freq='d')\n",
    "\n",
    "\n",
    "    else:\n",
    "        model=ExponentialSmoothing(train, trend=model_var[0],seasonal=model_var[1]\\\n",
    "                                   ,seasonal_periods=7,freq='d')\n",
    "\n",
    "\n",
    "        model_fit=model.fit()\n",
    "    pred_y = model_fit.predict(start=test.index.min(), end=test.index.max()) # create a forecast for the \"expected\" values on the Test Dataset\n",
    "    error = mean_squared_error(test,pred_y)\n",
    "    error2 = mean_absolute_percentage_error(test,pred_y)\n",
    "    print(model_var[0],model_var[1], np.sqrt(error), error2*100)\n",
    "    #except: \n",
    "     #   print(\"Model {} Failed\".format(model_var))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=ExponentialSmoothing(train, trend='mul',seasonal='mul'\\\n",
    "                                   ,seasonal_periods=7,freq='d')\n",
    "model_fit=model.fit()\n",
    "print(model_fit.params)\n",
    "df=pd.DataFrame(test)\n",
    "pred_y =model_fit.predict(start=df.index.min(), end=df.index.max())\n",
    "df['est']=pred_y\n",
    "\n",
    "df.plot(ylim=(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fit.predict(start=df.index.min(), end=df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_params=[]\n",
    "for i in arima_combos:\n",
    "    [all_params.append(item) for sublist in [x for x in ARIMA_params[i[0]]]+[y for y in SARIMA_params[i[1]]]+[['sigma2']] for item in sublist]\n",
    "all_params=['SI','0']+all_params\n",
    "start_params=pd.Series({x:0.01 for x in set(all_params)})\n",
    "start_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m54",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m54"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
