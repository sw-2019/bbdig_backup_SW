{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from   scipy import optimize\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "import pmdarima as pm\n",
    "\n",
    "# Install/import plotly packages- this package has lots of graphical properties\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "# Load custom scripts in reusable_code folder\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "\n",
    "import google_api_functions as gaf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from datetime import timedelta, datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pmdarima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Interpolate_Missing (df,freq='d'):\n",
    "    df=df.reindex(pd.date_range(start=df.index.min(), end=df.index.max(),freq=freq))\n",
    "    df=df.interpolate()\n",
    "    return df\n",
    "\n",
    "def Cap_data (ser,show_info=True):\n",
    "    \n",
    "    newSer=ser.copy() # Make a deep copy of the series to manipulate, without affecting original series\n",
    "    \n",
    "    if type(ser) is pd.core.series.Series : # Check if series\n",
    "        skewvalue=ser.skew()\n",
    "        print('The skew is {:.2f}. A value of <-1 indicates lots of small values, and a value of >1 indicates the presence of some large values.'.format(skewvalue))\n",
    "        \n",
    "        if show_info==True:\n",
    "            print(ser.describe())\n",
    "            ser.hist()\n",
    "            plt.show()\n",
    "        if skewvalue>1:\n",
    "            threshold=newSer.quantile(0.99)# For lots of large values, consider the 99th percentile as the threshold\n",
    "            mask=newSer>threshold # Create the boolean mask for all the records exceeding the threshold\n",
    "            \n",
    "            affected_records=mask[mask==True].size # Identify how many records affected\n",
    "            series_size=newSer.size # How many records in dataset\n",
    "            pc_affected=float(affected_records)/float(series_size)*100\n",
    "            \n",
    "            newSer=newSer.mask(newSer>threshold,threshold)  # Apply the mask\n",
    "            print('The 99th percentile is {}. Values greater than this will be capped, amounting to {:.0f} records and {:.1f}% of the dataset.'.format(threshold,affected_records,pc_affected))\n",
    "        elif skewvalue<-1:\n",
    "            threshold=newSer.quantile(0.01)[columnname] # For lots of large values, consider the 99th percentile as the threshold\n",
    "            mask=newSer<threshold # Create the boolean mask for all the records exceeding the threshold\n",
    "            \n",
    "            affected_records=mask[mask==True].size # Identify how many records affected\n",
    "            series_size=newSer.size # How many records in dataset\n",
    "            pc_affected=float(affected_records)/float(series_size)*100\n",
    "            #newSer= np.where(newSer<threshold, threshold,newSer)  # Apply the mask\n",
    "            newSer=newSer.mask(newSer<threshold,threshold)\n",
    "            print('The 1st percentile is {}. Values less than this will be capped, amounting to {:.0f} records and {:.1f}% of the dataset.'.format(threshold,affected_records,pc_affected))\n",
    "            \n",
    "        else:\n",
    "            print('Skew is ok, no capping applied')\n",
    "            \n",
    "        newskew=newSer.skew()\n",
    "        print('The skew is now {:.2f}.'.format(newskew))\n",
    "        \n",
    "        if show_info==True:\n",
    "            print(newSer.describe())\n",
    "            newSer.hist()  \n",
    "            plt.show()\n",
    "        \n",
    "        \n",
    "    \n",
    "    elif type(ser) is pd.core.frame.DataFrame and ser.shape[1]==1: # Check if one-column dataframe, if so take the first column and reference throughout for the processing\n",
    "        columnname=ser.columns[0] #Get the name of the series to use later\n",
    "        skewvalue=ser[columnname].skew()\n",
    "        print('The skew is {:.2f}. A value of <-1 indicates lots of small values, and a value of >1 indicates the presence of some large values.'.format(skewvalue))\n",
    "        \n",
    "        if show_info==True:\n",
    "            print(ser.describe())\n",
    "            ser.hist()\n",
    "            plt.show()\n",
    "        if skewvalue>1:\n",
    "            threshold=newSer.quantile(0.99)[columnname] # For lots of large values, consider the 99th percentile as the threshold\n",
    "            mask=newSer[columnname]>threshold # Create the boolean mask for all the records exceeding the threshold\n",
    "            print(mask[columnname==True])\n",
    "            affected_records=mask[mask==True].size # Identify how many records affected\n",
    "            series_size=newSer.size # How many records in dataset\n",
    "            pc_affected=float(affected_records)/float(series_size)*100\n",
    "            newSer[columnname]= np.where(mask, threshold,newSer[columnname])  # Apply the mask\n",
    "            print('The 99th percentile is {}. Values greater than this will be capped, amounting to {:.0f}/{:.0f} records ({:.1f}% of the dataset).'.format(threshold,affected_records,series_size,pc_affected))\n",
    "        \n",
    "        elif skewvalue<-1:\n",
    "            threshold=newSer.quantile(0.01)[columnname] # For lots of large values, consider the 99th percentile as the threshold\n",
    "            mask=newSer[columnname]<threshold # Create the boolean mask for all the records exceeding the threshold\n",
    "            print(mask[columnname==True])\n",
    "            affected_records=mask[mask==True].size # Identify how many records affected\n",
    "            series_size=newSer.size # How many records in dataset\n",
    "            pc_affected=float(affected_records)/float(series_size)*100\n",
    "            newSer[columnname]= np.where(mask, threshold,newSer[columnname])  # Apply the mask\n",
    "            print('The 1st percentile is {}. Values less than this will be capped, amounting to {:.0f}/{:.0f} records ({:.1f}% of the dataset).'.format(threshold,affected_records,series_size,pc_affected))\n",
    "        \n",
    "        else:\n",
    "            print('Skew is ok, no capping applied')\n",
    "            \n",
    "        newskew=newSer[columnname].skew()\n",
    "        print('The skew is now {:.2f}.'.format(newskew))\n",
    "        \n",
    "        if show_info==True:\n",
    "            print(newSer[columnname].describe())\n",
    "            newSer.hist()  \n",
    "            plt.show()\n",
    "\n",
    "    else:\n",
    "        print('Can''t run data, needs to be a 1 column dataframe or a series')\n",
    "    return newSer\n",
    "\n",
    "def generate_test_trains(df,HoldoutPC=0.05,recenttrain=90,lowerCutoff=None,upperCutoff=None,holdoutsize=None):\n",
    "    \n",
    "    if lowerCutoff:\n",
    "        df=df[lowerCutoff:]\n",
    "    if upperCutoff:\n",
    "        df=df[:upperCutoff]\n",
    "    \n",
    "    # This function splits the samples into three groups. A holdout is taken from the most recent data, used to evaluate model fit. The proportion to hold out is determined by HoldoutPC    \n",
    "    # Two different training sets are generated. One uses all available data, the other uses the most recent N periods, on the proviso that older day may no longer be relevant so models \n",
    "    # trained on this data may be better\n",
    "    \n",
    "    # If for whatever reason we wish to trim the edges (i.e. we want to build a model for \"last week\", this can be done using the lower- and upperCutoff)\n",
    "    if holdoutsize==None:\n",
    "        holdoutsize=int(np.floor(len(df)*HoldoutPC))\n",
    "    \n",
    "    holdout=df[-holdoutsize:]\n",
    "    print('{} records used as a test dataset ranging from {} and {}.'.format(holdoutsize,holdout.index.min() ,holdout.index.max()))\n",
    "    fullTrain=df[:-holdoutsize]\n",
    "    print('{} records used in the \"Full Training\" dataset ranging from {} and {}.'.format(len(fullTrain),fullTrain.index.min() ,fullTrain.index.max()))\n",
    "    recentTrain=fullTrain[-recenttrain:]\n",
    "    print('{} records used in the \"Recent Data\" training dataset ranging from {} and {}.'.format(len(recentTrain),recentTrain.index.min() ,recentTrain.index.max()))\n",
    "    return(holdout,fullTrain,recentTrain)\n",
    "\n",
    "def timeSeries_diagnostics(ser,hypothesised_seasonality=7):\n",
    "    if type(ser) is pd.core.series.Series :  \n",
    "        print(\"Correlation at lag 1 is :\",ser.autocorr())\n",
    "\n",
    "        # Correlation with itself (7 days ago)\n",
    "        print(\"Correlation at lag {} is :\".format(hypothesised_seasonality),ser.autocorr(hypothesised_seasonality))\n",
    "\n",
    "        xcutoff=hypothesised_seasonality*3+1 # Show three seasonal periods to see if that is validated\n",
    "        plot_acf(ser, lags=xcutoff)\n",
    "\n",
    "        \n",
    "        # Get Partial ACF\n",
    "\n",
    "        PACF_values=pd.DataFrame(pacf(ser,nlags=xcutoff)) #PACF numpy array converted to df\n",
    "        #PACF_values.plot() #Plot\n",
    "        #pyplot.show()\n",
    "        plot_pacf(ser)\n",
    "        print(\"Reminder- the point at which PACF drops to 0 is the point at which no further value is added by this lag over and above the points before it\")\n",
    "\n",
    "    else:\n",
    "        print('Please pass a Pandas Series for evaluation')\n",
    "\n",
    "def HW_grid(seasonal=[7]):\n",
    "    Initial_grid=list(itertools.product(['add','mul'],['add','mul',None],seasonal))\n",
    "    Deduped_grid=list(set([(i[0],i[1],i[2]) if i[1]!=None else (i[0],i[1],0) for i in Initial_grid ])) # If seasonality is none, we don't need the individual variantsv `    \n",
    "     #Create all 6 combos of additive, multiplicative multiplied by the seasonal\n",
    "    return Deduped_grid\n",
    "\n",
    "\n",
    "def Remove_duplicate_training_sets(training_sets):\n",
    "    if type(training_sets)!=dict:\n",
    "        print ('ERROR: training_sets parameter needs to take the form of a dictionary {label: pandas series}')\n",
    "    else:\n",
    "        del_dict={x:False for x in training_sets} #Create a dictionary saying which sets to delete, default to None\n",
    "        for n1,x in enumerate(training_sets): # Loop training sets\n",
    "            for n2,y in enumerate(training_sets): # For each set\n",
    "                if x==y or n2<n1: # Don't check references to self and only look at outstanding combinations\n",
    "                    pass\n",
    "                else:\n",
    "                    print('Comparing {} with {}'.format(x,y))\n",
    "                    if training_sets[x].equals(training_sets[y]): # If any other series are duplicate\n",
    "                        del_dict[y]=True # Mark them for deletion\n",
    "        # Now actually delete if appropriate\n",
    "        for i in del_dict:\n",
    "            if del_dict[i]==True:\n",
    "                del training_sets[i]\n",
    "    return training_sets\n",
    "\n",
    "def error_calcs(actual, estimated): \n",
    "    actual, estimated = np.array(actual), np.array(estimated)\n",
    "    nobs=len(actual)\n",
    "    \n",
    "    fit_dict={'MAPE':np.mean(np.abs(actual - estimated) / actual) ,\n",
    "             'MSE':np.mean((actual - estimated)**2),\n",
    "             'RMSE':np.sqrt(np.mean((actual - estimated)**2)),\n",
    "             'MAE':np.mean(np.abs(actual - estimated)),\n",
    "             'Correlation': np.corrcoef(actual , estimated)[0][1]}\n",
    "    \n",
    "    return fit_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data to determine LTV\n",
    "bq = bigquery.Client(project='itv-bde-analytics-prd',credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "query=\"\"\"\n",
    "\n",
    "with breakdown as \n",
    "    (\n",
    "    select \n",
    "    date(churnDate) as churnDate\n",
    "      , count(distinct britbox_ID) as churns\n",
    "\n",
    "    from\n",
    "    \n",
    "    (\n",
    "    \n",
    "    (\n",
    "    select itvid,\n",
    "  timestamp_trunc(eventDate,DAY) as churnDate\n",
    "    from `itv-bde-analytics-prd.britbox_mart.E_entitlements`\n",
    "    where (eventSubType.reportingEvent in ('H3','H4','HX') -- from paid to churned\n",
    "          or eventSubType.reportingEvent in ('Q1','Q3') -- from paid - AR off to churn\n",
    "          or eventSubType.reportingEvent in ('P2', 'P3','PX') -- from grace period to churn\n",
    "          ) and billingProvider in ('Stripe', 'iTunes') \n",
    "    and eventDate <= '2021-03-17'\n",
    "    ) as b\n",
    "    \n",
    "\n",
    "\n",
    "        left join \n",
    "\n",
    "        (select distinct\n",
    "        britbox_ID\n",
    "        ,min(Event_partition) as first_watched_LI\n",
    "            from `itv-bde-analytics-prd.britbox_analytics.Viewing_clean` where Event_partition >= '2020-01-01'\n",
    "            and lower(title.programme) like '%love island%' group by 1\n",
    "            ) as c\n",
    "\n",
    "            on b.itvid = c.britbox_ID and c.first_watched_LI <b.churnDate\n",
    "\n",
    "    \n",
    ") group by 1 )\n",
    "\n",
    "select * from breakdown where churnDate >= '2020-01-01' order by 1\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "df = bq.query(query ).to_dataframe()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'churnDate':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index(df['date'], inplace=True)\n",
    "df=df.drop(columns=['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TS_capped=Cap_data(df,show_info=False)       \n",
    "\n",
    "\n",
    "#df_excl_analysis=df[:'2021-01-31']\n",
    "#df_for_analysis=df['2021-02-01':]\n",
    "#df_excl_analysis.index.max()\n",
    "\n",
    "df_excl_analysis=TS_capped[:'2021-01-31']\n",
    "df_for_analysis=TS_capped['2021-02-01':]\n",
    "print(df_excl_analysis.index.max())\n",
    "print(df_for_analysis.index.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "holdout_sample_size=42\n",
    "plt.plot(df_excl_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=pm.auto_arima(df_excl_analysis,\\\n",
    "                    start_p=0,max_p=5,\\\n",
    "                    start_d=0,max_d=2,\\\n",
    "                    start_q=0,max_q=5,\\\n",
    "                    start_P=0,max_P=5,\\\n",
    "                    start_D=0,max_D=2,\\\n",
    "                    start_Q=0,max_Q=5, m=7\\\n",
    "                    , seasonal=True,error_action='warn', trace=True, supress_warnings=True,stepwise=True,random_state=20,n_fits=50\n",
    "                   ,out_of_sample_size=holdout_sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated=pd.DataFrame(model.predict_in_sample(),index=df_excl_analysis.index)\n",
    "estimated.columns = ['predicted_GrossChurn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Overall fit\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_excl_analysis,label=\"Training\")\n",
    "plt.plot(estimated,label=\"Training (Predicted)\")\n",
    "#plt.plot(test,label=\"Test (Actuals)\")\n",
    "#plt.plot(test_prediction,label=\"Test (Predicted)\")\n",
    "#plt.plot(df_for_analysis,label=\"Non-exclusive period (Actuals)\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise Fit on non-training sample\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_excl_analysis[-holdout_sample_size:],label=\"Test\")\n",
    "plt.plot(estimated[-holdout_sample_size:],label=\"Test (Predicted)\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate actual prediction\n",
    "prediction = pd.DataFrame(model.predict(n_periods = len(df_for_analysis.index)),index=df_for_analysis.index)\n",
    "prediction.columns = ['predicted_GrossChurn']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise all items\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_excl_analysis[:-holdout_sample_size],label=\"Training\")\n",
    "plt.plot(estimated[:-holdout_sample_size],label=\"Training (Predicted)\")\n",
    "plt.plot(df_excl_analysis[-holdout_sample_size:],label=\"Test\")\n",
    "plt.plot(estimated[-holdout_sample_size:],label=\"Test (Predicted)\")\n",
    "plt.plot(df_for_analysis,label=\"Non-exclusive period (Actuals)\")\n",
    "plt.plot(prediction,label=\"Non-exclusive period (Predicted)\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise all items except the training portion\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_excl_analysis[-holdout_sample_size:],label=\"Test\")\n",
    "plt.plot(estimated[-holdout_sample_size:],label=\"Test (Predicted)\")\n",
    "plt.plot(df_for_analysis,label=\"Non-exclusive period (Actuals)\")\n",
    "plt.plot(prediction,label=\"Non-exclusive period (Predicted)\")\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_actuals=pd.concat([df_excl_analysis,df_for_analysis])\n",
    "all_estimates=pd.concat([estimated,prediction])\n",
    "outputdf=pd.merge(all_actuals,all_estimates,how=\"inner\",left_index=True,right_index=True)\n",
    "\n",
    "outputdf['Date']=outputdf.index.strftime(\"%Y-%m-%d\")\n",
    "outputdf['Date_as_DT']=outputdf.index\n",
    "outputdf[\"Day_of_Week\"] = outputdf.Date_as_DT.dt.weekday\n",
    "outputdf[\"Day_of_Week\"] = outputdf.Date_as_DT.dt.weekday*1\n",
    "outputdf['Month']=outputdf.Date_as_DT.dt.to_period('M').dt.strftime(\"%Y-%m-%d\")\n",
    "outputdf[\"Week\"] = outputdf.apply(lambda x: x.Date_as_DT- timedelta(days=x.Day_of_Week), axis=1).dt.strftime(\"%Y-%m-%d\")\n",
    "outputdf=outputdf.drop(columns=['Date_as_DT'])\n",
    "outputdf\n",
    "gaf.Write_whole_df_to_gsheet(creds, outputdf, '1vFoQTXSD6Kw_fLj5JwWBv1VYTuPDnKkgkD7Z8UJrY-c', 'Churn_TimeSeries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fit on Training is: \",error_calcs(df_excl_analysis[:-holdout_sample_size],estimated[:-holdout_sample_size]))\n",
    "print(\"Fit on Test is: \",error_calcs(df_excl_analysis[-holdout_sample_size:],estimated[-holdout_sample_size:]))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
