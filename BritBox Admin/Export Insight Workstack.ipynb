{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up:\n",
    "1) Modules\n",
    "2) Credentials & keys for Trello and Google Sheets\n",
    "3) Load Trello board\n",
    "4) Configure pandas output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up modules\n",
    "import pandas as pd\n",
    "import sys\n",
    "import requests\n",
    "from xlsxwriter.utility import xl_rowcol_to_cell\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "import google_api_functions as gaf\n",
    "import trello_generic as tg\n",
    "import sqlite3\n",
    "from google.cloud import bigquery # To run BQ statements\n",
    "import re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "from plotly.validators.scatter.marker import SymbolValidator\n",
    "import random\n",
    "\n",
    "import requests\n",
    "import json\n",
    "# Set up SQL DB\n",
    "conn = sqlite3.connect('SQL_connection1.db') #Create a connection object\n",
    "\n",
    "# Set up credentials for Trello and Google \n",
    "\n",
    "# Google Sheets Credentials\n",
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/') # Return logged-in credentials\n",
    "\n",
    "# General setup and credentials: Trello\n",
    "from trello import TrelloClient\n",
    "trelloUserCreds=tg.readTrelloCredsFromFile(r'/home/jupyter/reusable_code/trellocreds.pickle')\n",
    "mykey,mysecret,mytoken=trelloUserCreds\n",
    "\n",
    "client = TrelloClient(api_key=mykey,api_secret=mysecret,token=mytoken)\n",
    "\n",
    "# Return Trello board, client and other credentials objects. \"myboard_creds\" is a tuple of items which can be unpacked\n",
    "# to cover off all of the various levels you might need access at\n",
    "dataBoard,dataBoard_id,dataBoard_creds=tg.Return_board_by_name(mykey,mysecret,mytoken,\"Data 2021\")\n",
    "researchBoard,researchBoard_id,researchBoard_creds=tg.Return_board_by_name(mykey,mysecret,mytoken,\"Research 2021\")\n",
    "#oldBoard,oldBoard_id,oldBoard_creds=tg.Return_board_by_name(mykey,mysecret,mytoken,\"Insights & Data\")\n",
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Trello board into a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCard_df,dataCard_list=tg.cards_to_dataframe(dataBoard_creds,checklist_options=None\\\n",
    "                           ,labels_as_binary_flags=True, label_colours=True,comment_names=False,get_attachments=True #)\n",
    "                                               ,card_number_cutoff=10000, \\\n",
    "                                                lists_to_exclude=['Template(s)','Ideation','No longer required','Completed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dataCard_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Turn \"New Insight Brief(s)\" into proper cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monthToNum(shortMonth):\n",
    "    return {\n",
    "            'Jan' : '01',\n",
    "            'Feb' : '02',\n",
    "            'Mar' : '03',\n",
    "            'Apr' : '04',\n",
    "            'May' : '05',\n",
    "            'Jun' : '06',\n",
    "            'Jul' : '07',\n",
    "            'Aug' : '08',\n",
    "            'Sep' : '09', \n",
    "            'Oct' : '10',\n",
    "            'Nov' : '11',\n",
    "            'Dec' : '12'\n",
    "    }[shortMonth]\n",
    "\n",
    "\n",
    "def Process_New_Insight_Briefs():\n",
    "    MonthLookup={}\n",
    "    \n",
    "    New_briefs=[i for i in dataCard_list if i['Name']=='New Insight Brief' and i['List'] in ['New Projects (need prioritisation)','Backlog', 'Prioritised','In Progress']]\n",
    "    labellist=dataBoard.get_labels()\n",
    "    for i in New_briefs:\n",
    "            CardInfo=re.split('Sent via Google Form Notifications',i['Description'])[0] # Take the bit before the generic email signature\n",
    "            print(CardInfo)\n",
    "            \n",
    "            #############################\n",
    "            # Reset card name\n",
    "            #############################\n",
    "            i['Card Object'].set_name(re.search('\\*Project Name\\*: \\*(.*)\\*',CardInfo)[1])\n",
    "\n",
    "            #############################\n",
    "            # Set Due Date            \n",
    "            #############################\n",
    "            DeadlineInfo=re.search('\\*Needed by: \\*(.*)',CardInfo)[1]\n",
    "            #print(DeadlineInfo)\n",
    "            try:\n",
    "                DueDate=datetime.strptime(DeadlineInfo[:12], '%b %d, %Y')\n",
    "                #print(DueDate)\n",
    "                i['Card Object'].set_due(DueDate)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            try: \n",
    "                DueText=str(DueDate)[:10]\n",
    "            #print(DueText)\n",
    "                tg.Update_custom_field(dataBoard_creds,i['Trello ID'],'Hard Deadline',DueText)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            #############################\n",
    "            # Set labels for team        \n",
    "            #############################\n",
    "            try:\n",
    "                Team=re.search('\\*Submitted by\\*: .* in the (.+) team',CardInfo)[1]\n",
    "                try:\n",
    "                    LabelObject=[i for i in labellist if i.name==Team][0]\n",
    "                    i['Card Object'].add_label(LabelObject)\n",
    "                except:\n",
    "                    print('Could not add label for team: \"{}\"'.format(Team))\n",
    "            except:\n",
    "                pass\n",
    "          \n",
    "                \n",
    "            #############################\n",
    "            # Set labels for priorities/ projects            \n",
    "            #############################\n",
    "            projects=re.findall('\\*Supports projects\\*: (.*)',CardInfo)\n",
    "            if len(projects)>0:\n",
    "                projectList=re.split('\\n',projects[0])\n",
    "                for project in projectList:\n",
    "                    try:\n",
    "                        LabelObject=[i for i in labellist if i.name==project][0]\n",
    "                        i['Card Object'].add_label(LabelObject)\n",
    "                    except:\n",
    "                        print('Could not add label for project: \"{}\"'.format(project))\n",
    "    \n",
    "            #############################\n",
    "            # Set custom field (type)\n",
    "            #############################\n",
    "            WorkType=re.search('\\*(.*) brief\\*',CardInfo)[1]\n",
    "            if WorkType=='No idea- you tell me':            # If unknown leave blank\n",
    "                pass\n",
    "            else:\n",
    "                try:\n",
    "                    tg.Update_custom_field(dataBoard_creds,i['Trello ID'],'Type',WorkType)\n",
    "                except:\n",
    "                    print('Could not update Type field with value: {}'.format(WorkType))\n",
    "            \n",
    "            #############################\n",
    "            # Set custom field (Blocker) \n",
    "            try:\n",
    "                Blockerinfo= re.search('\\*Blocker: \\*(.*)? which should be resolved by',CardInfo)[1]\n",
    "                tg.Update_custom_field(dataBoard_creds,i['Trello ID'],'Blockers/ Dependencies 1',Blockerinfo)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Set custom field (Blocker date) \n",
    "            try:\n",
    "                Blockerdatetext= re.search('\\*Blocker: \\*.* which should be resolved by (.+)',CardInfo)[1]\n",
    "                BlockerDate= '-'.join([Blockerdatetext[8:12],monthToNum(Blockerdatetext[0:3]),Blockerdatetext[4:6]])   \n",
    "                tg.Update_custom_field(dataBoard_creds,i['Trello ID'],'Blocker 1 Due Date',BlockerDate)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            # Redo description            \n",
    "            NewDesc=re.split('Project Detail',CardInfo)[1]+'\\n Submitted By: '\\\n",
    "            +re.search('\\*Submitted by\\*: (.*)@...',CardInfo)[1].replace('.',' ')\\\n",
    "            +'\\nRequired by: '+DeadlineInfo\n",
    "            i['Card Object'].set_description(NewDesc)\n",
    "            \n",
    "            # If research move to research board\n",
    "            if WorkType=='Research':\n",
    "                tg.MoveCard(i['Card Object'],'5fe35ef42dd5616a3e37bc12',mykey,mytoken,boardid=researchBoard_id)\n",
    "    \n",
    "    return New_briefs\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "briefs=Process_New_Insight_Briefs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assign card numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardlist=dataBoard.get_cards() # Return all cards on board\n",
    "cardnums=[int(re.search('\\#(\\d{1,4})',i.name)[1]) for i in cardlist if re.search('\\#\\d{1,4}',i.name)!=None] # Identify where there are already card numbers in the format #number\n",
    "max_cardnum=np.max(cardnums) # Get the maximum card number on the board\n",
    "cards_without_num=[i for i in cardlist if re.search('\\#\\d{1,4}',i.name)==None] # List all cards without a number \n",
    "\n",
    "# Loop cards without a number and add one to their name\n",
    "for n,i in enumerate(cards_without_num):\n",
    "    i.set_name('#'+str(n+1+max_cardnum)+' '+i.name)\n",
    "    print('#'+str(n+1+max_cardnum)+' '+i.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reimport\n",
    "dataCard_df,dataCard_list=tg.cards_to_dataframe(dataBoard_creds,checklist_options=None\\\n",
    "                           ,labels_as_binary_flags=True, label_colours=False,comment_names=False,get_attachments=True #)\n",
    "                                               ,card_number_cutoff=10000,lists_to_exclude=['Template(s)','Ideation','No longer required','Completed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy to BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/')\n",
    "bq = bigquery.Client(project='itv-bde-analytics-dev',credentials=creds)\n",
    "dataset=bq.dataset('britbox_sandbox')\n",
    "table_ref = dataset.table(\"SW_Data_Workstack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for amending then loading into BQ\n",
    "df_for_bq=dataCard_df.copy()\n",
    "\n",
    "# Most of the columns are \"object\" type, which holds mixed types. Explicitly make dates as such else it'll break load as it expects a \"bytes\" type then finds a datetime\n",
    "df_for_bq['Due Date'] = pd.to_datetime(df_for_bq['Due Date'].astype(str))\n",
    "df_for_bq['Card Created Date'] = pd.to_datetime(df_for_bq['Card Created Date'].astype(str))\n",
    "df_for_bq['Hard Deadline'] = pd.to_datetime(df_for_bq['Hard Deadline'].astype(str))\n",
    "df_for_bq['Blocker 1 Due Date'] = pd.to_datetime(df_for_bq['Blocker 1 Due Date'].astype(str))\n",
    "\n",
    "# Struggles to identify and load array of STRUCTs with different datatypes, so just don't for now\n",
    "\n",
    "df_for_bq=df_for_bq.drop(columns=['listMovementHistory', 'listMovementSummary','coordinates'])\n",
    "\n",
    "\n",
    "# Remove characters that you can't have in a BQ variable name\n",
    "newcol_names={x:x.replace(\" \", \"_\").replace(\"/\",\"\").replace(\"?\",\"\").replace(\"-\",\"\").replace(\"&\",\"\").replace(\":\",\"\").replace(\"(\",\"\").replace(\")\",\"\") for x in df_for_bq.columns}\n",
    "df_for_bq=df_for_bq.rename(columns=newcol_names)\n",
    "\n",
    "#Remove blank column names which might arise from blank labels on the board\n",
    "df_col=[i for i in df_for_bq.columns if len(i)>0] \n",
    "df_for_bq=df_for_bq[df_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_bq.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    bq.delete_table(table_ref)\n",
    "except:\n",
    "    pass\n",
    "job = bq.load_table_from_dataframe(df_for_bq, table_ref)\n",
    "\n",
    "job.result()  # Waits for table load to complete.\n",
    "print(\"Loaded dataframe to {}\".format(table_ref.path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_bq.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tidy up DataFrame to include only the records and columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"\"\"\n",
    "create or replace table `itv-bde-analytics-dev.britbox_sandbox.SW_Data_Workstack1` as\n",
    "with x  as (select \n",
    "Name\n",
    ",Description\n",
    ",List\n",
    ",Card_Number\n",
    ",Due_Date\n",
    ",Trello_ID\n",
    ",Trello_URL\n",
    ",Card_Created_Date\n",
    ",Comments\n",
    ",Trello_attachments\n",
    ",Other_attachments\n",
    ",Blocker_1_Due_Date\n",
    ",Supported_by\n",
    ",Ad_hoc\n",
    ",Assigned_to\n",
    ",Blockers_Dependencies_1\n",
    ",Type\n",
    ",Subtype\n",
    ",Blockers_Dependencies_2\n",
    "--,Project_Brief_Location\n",
    ",Paused_or_Blocked\n",
    ",Mark_for_Deletion\n",
    ",Hard_Deadline\n",
    ",EPIC\n",
    "--,isEPIC\n",
    "\n",
    ",currentListTimeSpent\n",
    ",currentListTimesEntered\n",
    ",currentListFirstEntered\n",
    ",currentListLastEntered\n",
    ",listpos\n",
    ",cardpos\n",
    ",boardpos\n",
    "\n",
    ",split(Labels,'|') as labels1\n",
    "from `itv-bde-analytics-dev.britbox_sandbox.SW_Data_Workstack`\n",
    "where list not in ('Template(s)','No longer required')\n",
    ")\n",
    "\n",
    "select x.* except (labels1)\n",
    ", array_agg(case when trim(split(labels2,':')[safe_offset(1)])='green' then \n",
    "trim(split(labels2,':')[safe_offset(0)]) end ignore nulls) as Teams\n",
    ", array_agg(case when trim(split(labels2,':')[safe_offset(1)])='yellow' then \n",
    "trim(split(labels2,':')[safe_offset(0)]) end ignore nulls) as Priorities\n",
    ", array_agg(case when trim(split(labels2,':')[safe_offset(1)])='blue' then \n",
    "trim(split(labels2,':')[safe_offset(0)]) end ignore nulls) as TeamObjectives\n",
    "from x\n",
    "cross join unnest (labels1) as labels2\n",
    "group by 1,2,3,4,5,6,7,8,9,10\n",
    ",11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30\n",
    ";\"\"\"\n",
    "df = bq.query(query).to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-prioritisation session admin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move all cards from New to Backog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from_list=[i for i in dataBoard.list_lists() if i.name=='New Projects (need prioritisation)'][0]\n",
    "to_list=[i for i in dataBoard.list_lists() if i.name=='Backlog'][0]\n",
    "from_list.move_all_cards(to_list)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move all cards from Recently Completed to Completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from_list=[i for i in dataBoard.list_lists() if i.name=='Recently Completed'][0]\n",
    "to_list=[i for i in dataBoard.list_lists() if i.name=='Completed'][0]\n",
    "from_list.move_all_cards(to_list)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One off Updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### Set Hard Deadlines for historically processed cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i in dataCard_list:\n",
    "    DeadlineInfo=None\n",
    "    DeadlineInfo=re.search('Required by: (.*)',i['Description'])\n",
    "\n",
    "    if DeadlineInfo!=None:\n",
    "        print(DeadlineInfo[1])\n",
    "        try:\n",
    "            DueDate=datetime.strptime(DeadlineInfo[1][:12], '%b %d, %Y')\n",
    "            print(DueDate)\n",
    "            DueText=str(DueDate)[:10]\n",
    "            print(DueText)\n",
    "            tg.Update_custom_field(dataBoard_creds,i['Trello ID'],'Hard Deadline',DueText)\n",
    "            print('Updated to {}'.format(DueText))\n",
    "        except:\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataCard_list[:23]:\n",
    "    tg.Update_custom_field(dataBoard_creds,i['Trello ID'],'Hard Deadline','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One  off- assign members using assigned to field, so timelines can be grouped by member"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Members=dataBoard.get_members()\n",
    "MemberDict={i.full_name:i for i in Members}\n",
    "MemberDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "assignees=list(dataCard_df['Assigned to'][dataCard_df['Assigned to'].notna()].unique())\n",
    "assignee_dict={i:None for i in assignees}\n",
    "\n",
    "for i in assignee_dict:\n",
    "    for j in MemberDict:\n",
    "        if i[:3].lower() ==j[:3].lower():\n",
    "            print(i,j)\n",
    "            assignee_dict[i]=j\n",
    "assignee_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "query = {'key': mykey,'token': mytoken}\n",
    "\n",
    "for i in dataCard_list:\n",
    "\n",
    "    if 'Assigned to' in i.keys():\n",
    "        print(i['Name'])\n",
    "        print(i['Assigned to'])\n",
    "\n",
    "        Member_needed=assignee_dict[i['Assigned to']]\n",
    "        print(Member_needed)\n",
    "        \n",
    "        url = \"https://api.trello.com/1/cards/{}/members\".format(i['Trello ID'])\n",
    "        response = requests.request(\"GET\",url,params=query)\n",
    "        if response.ok:\n",
    "            responses = json.loads(response.text)\n",
    "            if Member_needed in [i['fullName'] for i in responses]:\n",
    "                print('Ok')\n",
    "            else:\n",
    "                print('Missing')\n",
    "                i['Card Object'].add_member(MemberDict[Member_needed])\n",
    "                \n",
    "                \n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate start date as entered \"in progress\" list and end date as entered  \"complete\" accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in dataCard_list:\n",
    "    if i['List']=='In Progress':\n",
    "        print(i['Name'])\n",
    "        df=pd.DataFrame(i['listMovementSummary'])\n",
    "        print(df[['enteredList','FirstEntered','LastExited']][df['enteredList']=='In Progress'])\n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCard_df[['Name','Blockers/ Dependencies 1','Blockers/ Dependencies 2']][dataCard_df['Blockers/ Dependencies 1'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Map EPICs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import all cards including \"completed\"\n",
    "(NB the checklist import isn't working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCard_df,dataCard_list=tg.cards_to_dataframe(dataBoard_creds,checklist_options=None\\\n",
    "                           ,labels_as_binary_flags=True, label_colours=True,comment_names=False,get_attachments=True #)\n",
    "                                               ,card_number_cutoff=10000,lists_to_exclude=['Template(s)','Ideation','No longer required'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify EPICS (as cards with the isEPIC label applied... rather than using the \"EPIC\" field or being in the \"EPIC\" list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPICS=[]\n",
    "for i in dataCard_list:\n",
    "    if 'isEPIC' in i.keys():\n",
    "        if i['isEPIC']==True:\n",
    "            EPICS.append(i)\n",
    "#EPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create EPIC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labellist=dataBoard.get_labels()\n",
    "\n",
    "for i in EPICS:\n",
    "    EPIC_Name=re.sub('\\#(\\d{1,4}) ','',i['Name'])\n",
    "    if 'EPIC: {}'.format(EPIC_Name) not in [i.name for i in labellist]:\n",
    "        dataBoard.add_label('EPIC: {}'.format(EPIC_Name), color='lime')\n",
    "        print('Created new label: {}'.format(EPIC_Name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove labels that shouldn't be there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPIC_labels=[i for i in labellist if i.name[:4]=='EPIC']\n",
    "accepted_labels=['EPIC: {}'.format(re.sub('\\#(\\d{1,4}) ','',i['Name'])) for i in EPICS]\n",
    "\n",
    "for x in EPIC_labels:\n",
    "    if x.name not in accepted_labels:\n",
    "        print(x.name)\n",
    "        dataBoard.delete_label(x.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a mapping by looping through the checklists on each EPIC card of items assigned to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Epic_mapping={}\n",
    "for card in EPICS: #Loop epics\n",
    "    checklists_to_export=[i for i in card['Card Object'].checklists] # Get the checklists on the cards\n",
    "\n",
    "    for m,i in enumerate(checklists_to_export): # Loop the checklists\n",
    "        url = \"https://api.trello.com/1/checklists/{}/checkItems\".format(i.id)\n",
    "        querystring = {\"filter\":'all',\"fields\":'all',\"key\":mykey,\"token\":mytoken}\n",
    "        if i.name=='Sub Projects': # Return the items on the checklist if it's called \"Sub Projects\"\n",
    "            response = requests.request(\"GET\", url, params=querystring)\n",
    "            checklist_items=json.loads(response.text)\n",
    "            if checklist_items!=None:\n",
    "                for j in checklist_items:\n",
    "                    matched=re.search('https://trello.com/c/.{8}',j['name']) # Extract the common part of the URL\n",
    "                    try:\n",
    "                        Epic_mapping[matched[0]]={'URL':card['Trello URL'],'Label':'EPIC: {}'.format(re.sub('\\#(\\d{1,4} )','',card['Name'])) } #Store in a dictionary\n",
    "                    except:\n",
    "                        print('No match on {}'.format(j))\n",
    "Epic_mapping                        \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for key in Epic_mapping:\n",
    "    for card in dataCard_list:\n",
    "        if key==card['Trello URL']:\n",
    "            print(key,Epic_mapping[key]['URL'])\n",
    "            tg.Update_custom_field(myboard_creds=dataBoard_creds, cardId=card['Trello ID'], customFieldname='EPIC', value=Epic_mapping[key]['URL']) # Update the custom field\n",
    "            \n",
    "            #Labels=[print(i.name,Epic_mapping[key]['Label']) for i in labellist]# if i.name==Epic_mapping[key]['Label']]\n",
    "            LabelObject=[i for i in labellist if i.name==Epic_mapping[key]['Label']][0]\n",
    "            if card[LabelObject.name]==False:\n",
    "                try:\n",
    "                    card['Card Object'].add_label(LabelObject)\n",
    "                except:\n",
    "                    print('Couldn''t do it')\n",
    "            else:\n",
    "                print('Already on')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project dependency graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reload all cards (inc. Completes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataCard_df,dataCard_list=tg.cards_to_dataframe(dataBoard_creds,checklist_options=None\\\n",
    "                           ,labels_as_binary_flags=True, label_colours=True,comment_names=False,get_attachments=True #)\n",
    "                                               ,card_number_cutoff=10000,lists_to_exclude=['Template(s)','Ideation','No longer required','EPICs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Colour palette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "values,colourDf=gaf.read_google_sheets_as_rows('18t_E2ZBbFxxm32ApsajUMP1KGo--m2aqEO6xy4v4XvY','Sheet1',creds)\n",
    "colourDictHex={i['Colour']:i['HEX'] for i in colourDf[['Colour','HEX','Colour Palette']].to_dict('records') if i['Colour Palette']=='Overall'}\n",
    "colourDictHexInternal={i['Colour']:i['HEX'] for i in colourDf[['Colour','HEX','Colour Palette']].to_dict('records') if i['Colour Palette']=='Internal'}# and i['Colour'][-9:]!='Highlight']\n",
    "#colourDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Marker Lookups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_symbols = SymbolValidator().values\n",
    "marker_lookup={raw_symbols[i+2]:raw_symbols[i] for i in range(0,len(raw_symbols),3)} # Create a full lookup\n",
    "marker_lookup_basic={i:marker_lookup[i] for i in marker_lookup if 'open' not in i.split('-') and 'dot' not in i.split('-')} # Subset the lookup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subset the graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_EPIC_from_labels(label_string):\n",
    "    label_string=str(label_string)\n",
    "    list_of_labels=label_string.split('|')\n",
    "    list_of_labelDicts=[]\n",
    "    for i in list_of_labels:\n",
    "        split=i.split(':')\n",
    "        if len(split)>1:\n",
    "            list_of_labelDicts.append({split[0]:split[1]})\n",
    "    \n",
    "    label=[]\n",
    "    for dictobj in list_of_labelDicts:\n",
    "        for key in dictobj.keys():\n",
    "            if key.strip()=='EPIC':\n",
    "                label.append(dictobj[key].strip())\n",
    "                \n",
    "    if len(label)>0:\n",
    "        return label[0]\n",
    "    else:\n",
    "        return 'Standalone'\n",
    "    \n",
    "\n",
    "# Create sub df for use in graph build    \n",
    "graphdf=dataCard_df[['Name','Description','Trello URL','List','Card Number','Labels','Hard Deadline','Due Date','Type','Assigned to','EPIC','Depends on','Blockers/ Dependencies 1','Blockers/ Dependencies 2','Blocker 1 Due Date','Blocker 2 Due Date']].copy()   \n",
    "\n",
    "# Remove the number from the project name for display purposes\n",
    "graphdf['Name']=graphdf['Name'].apply(lambda x: re.sub('\\#(\\d{1,4} )','',x))\n",
    "\n",
    "# Apply the above function to extract the EPIC from the labels (or class as 'standalone' if not)\n",
    "graphdf['EPIC_Class']=graphdf['Labels'].apply(Get_EPIC_from_labels) \n",
    "\n",
    "# Turn the \"Depends on\" field into a list\n",
    "graphdf['Dependent']=graphdf['Depends on'].apply(lambda x:  str(x).split(',') if pd.notnull(x) else None) # Extract Dependent by Number\n",
    "\n",
    "####################################################################################################################################\n",
    "# Dataframe for Graph Edges\n",
    "####################################################################################################################################\n",
    "# Create an edge df by looking at the dependencies field and the blockers field\n",
    "\n",
    "edge_df=graphdf[graphdf['Dependent'].notna()][['Name','Dependent']].explode('Dependent')# Explode out the dependencies\n",
    "edge_df['Dependent']=edge_df['Dependent'].astype(int) # Convert from string to integer\n",
    "edge_df=edge_df.merge(graphdf,left_on='Dependent',right_on='Card Number')[['Name_x','Name_y']].rename(columns={'Name_x':'To','Name_y':'From'}) # Match edges using dependents field\n",
    "\n",
    "# Create a df holding on the stuff in the dependencies fields, which holds off-board dependencies\n",
    "external_edges=pd.concat([\n",
    "graphdf[graphdf['Blockers/ Dependencies 2'].notna()][['Name','Blockers/ Dependencies 2','EPIC_Class']].rename(columns={'Name':'To','Blockers/ Dependencies 2':'From'}),\n",
    "graphdf[graphdf['Blockers/ Dependencies 1'].notna()][['Name','Blockers/ Dependencies 1','EPIC_Class']].rename(columns={'Name':'To','Blockers/ Dependencies 1':'From'})])\n",
    "\n",
    "# Union it in to the dataframe\n",
    "edge_df=pd.concat([edge_df,external_edges[['To','From']]])\n",
    "\n",
    "# Package up the from/to into a Tuple for plugging into NetworkX\n",
    "edge_df['Tuple'] = list(zip(edge_df['From'],edge_df['To']))\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# Dataframe for Graph Nodes\n",
    "####################################################################################################################################\n",
    "# Create a df to hold the nodes not in cards on my board that are referenced in the edges (BDE projects, etc.)\n",
    "extranodes=external_edges.groupby(['From','EPIC_Class']).count().drop(columns=['To']).reset_index()\n",
    "extranodes_df = pd.DataFrame(index=range(0,len(extranodes)),columns = list(graphdf.columns))\n",
    "extranodes_df['Name']=extranodes['From']\n",
    "extranodes_df['EPIC_Class']=extranodes['EPIC_Class'] \n",
    "extranodes_df['Type']='External' # hard code the type field\n",
    "\n",
    "\n",
    "# Concatenate the dfs containing cards on this board and cards on other boards (or other references) \n",
    "nodedf=graphdf.append(extranodes_df,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# Create fields holding the various display properties of the graph (text, colour, shape, etc.)\n",
    "####################################################################################################################################\n",
    "\n",
    "############ Hover text\n",
    "nodedf['Node_HoverText']='Name: '+nodedf['Name']+'<br>'+'URL: '+nodedf['Trello URL']+'<br>'+'Type: '+nodedf['Type']+'<br>'+'EPIC: '+nodedf['EPIC_Class']\n",
    "\n",
    "\n",
    "\n",
    "############ Colour (EPICS)\n",
    "\n",
    "# Create a colour map for use in epics\n",
    "classes=nodedf['EPIC_Class'].unique() # List unique values for EPICs\n",
    "loops_round=int(np.ceil(len(classes)/len(colourDictHexInternal))) # If there are more values than there are colours, some colours may need to be re-used, so the loop needs to repeat\n",
    "\n",
    "# Loop through EPICs, and create a lookup that assigns a key\n",
    "colourMapEPIC={}\n",
    "for n,i in enumerate(classes):\n",
    "    for loop in range(0,loops_round):\n",
    "        for m,j in enumerate(colourDictHexInternal):\n",
    "            if n==(loop*len(colourDictHexInternal)+m):\n",
    "                colourMapEPIC[i]=colourDictHexInternal[j]\n",
    "                \n",
    "nodedf['Node_Colour_EPIC']=nodedf['EPIC_Class'].apply(lambda x: colourMapEPIC[x])\n",
    "\n",
    "############ Colour (Status)\n",
    "\n",
    "colourMapList={'Backlog': colourDictHexInternal['Sunset Red'],\n",
    " 'Hygiene Factors': colourDictHexInternal['Sunset Red'],\n",
    " 'Prioritised': colourDictHexInternal['Sunset Red Highlight'],\n",
    " 'In Progress': colourDictHexInternal['Orange'],\n",
    " 'EPICs': '#000000',\n",
    " 'Paused': colourDictHexInternal['Raspberry'],\n",
    " 'Completed': colourDictHexInternal['Apple']}\n",
    "                \n",
    "nodedf['Node_Colour_Status']=nodedf['List'].apply(lambda x: colourMapList[x] if x in colourMapList.keys() else '#000000' )\n",
    "\n",
    "\n",
    "############ Markers (EPICS)\n",
    "markers_to_use={i:marker_lookup_basic[i] for i in marker_lookup_basic if i[:8]!='triangle'}\n",
    "classes=nodedf['EPIC_Class'].unique() # List unique values for EPICs\n",
    "loops_round=int(np.ceil(len(classes)/len(markers_to_use))) # If there are more values than there are colours, some colours may need to be re-used, so the loop needs to repeat\n",
    "\n",
    "# Loop through EPICs, and create a lookup that assigns a key\n",
    "markerMapEPIC={}\n",
    "for n,i in enumerate(classes):\n",
    "    for loop in range(0,loops_round):\n",
    "        for m,j in enumerate(markers_to_use):\n",
    "            if n==(loop*len(markers_to_use)+m):\n",
    "                markerMapEPIC[i]=markers_to_use[j]\n",
    "                \n",
    "nodedf['Node_Marker_EPIC']=nodedf['EPIC_Class'].apply(lambda x: markerMapEPIC[x])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "####################################################################################################################################\n",
    "# Initialise the network X graph itself\n",
    "####################################################################################################################################\n",
    "\n",
    "G=nx.DiGraph()\n",
    "G.add_nodes_from(nodedf['Name'])\n",
    "G.add_edges_from(edge_df['Tuple'])\n",
    "\n",
    "\n",
    "#### Classify node type\n",
    "def classify_node(node,G):\n",
    "    if G.in_degree()[node]==0 and G.out_degree()[node]==0:\n",
    "        nodeType='unconnected'\n",
    "    elif G.in_degree()[node]==0 and G.out_degree()[node]>0:\n",
    "        nodeType='start'\n",
    "    elif G.in_degree()[node]>0 and G.out_degree()[node]==0:\n",
    "        nodeType='terminal'\n",
    "    else:\n",
    "        nodeType='interim'\n",
    "    return(nodeType)\n",
    "\n",
    "nodedf['nodeType']=nodedf['Name'].apply(classify_node,G=G)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently:\n",
    "- Marker size is not varied\n",
    "- Edge weight is not varied\n",
    "- Edge colour is not varied\n",
    "- Edges are not labelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing something that positions things such that:\n",
    "- X axis is distributed according to the longest observed path\n",
    "- Starts are on the left, Terminals on the right, interims in between\n",
    "OR\n",
    "- Starts are on the left, everything is positioned according to the max number of nodes before\n",
    "- isolated nodes are just distributed evenly along the x-axis in a block\n",
    "\n",
    "\n",
    "- Y axis can be grouped by a column, positionally. In this example it will be EPIC but you could also group by a dummy column\n",
    "- Each 'group by' is given the same space\n",
    "-- For each of those, the within-height group is then the max width of each X position (starts, ends or interims)\n",
    "-- A spacer is added between "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or do I just iterate, 1 by 1 from left to right?\n",
    "Or do I look for the most interconnected nodes according to betweeness centrality and position them centrally?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If I have the range of how left or right something could be (left is earliest in path, right is latest), there could be a condition to check its successors\n",
    "and make sure they're more to the right. If not, shift up? Do while not true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodedf['Groupby']='All' # Use if you don't want to group\n",
    "nodedf['Groupby']=nodedf['EPIC_Class'].astype(str) # Or specify the field to group by\n",
    "# Identify the groups\n",
    "Grouplist=list(nodedf['Groupby'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function to shift x positions right if there is a feeding node. This assumes the presence of the global var \"xposDict\"\n",
    "def shift_successors_right(node,G):\n",
    "    # List a nodes successors\n",
    "    succ=list(G.successors(node))\n",
    "    if len(succ)==0:\n",
    "        return\n",
    "    else:\n",
    "        for s in succ:\n",
    "            if xposDict[s]<=xposDict[node]: # If successor is positionally equal to or left of the dependant node\n",
    "                xposDict[s]=xposDict[node]+1 # Update that node\n",
    "                #print('Updating {} and searching for successors'.format(s))\n",
    "                shift_successors_right(s,G)\n",
    "    return None\n",
    "\n",
    "\n",
    "# Initialise all items\n",
    "xposDict={node:0 for node in G.nodes()}\n",
    "\n",
    "# Loop all nodes and shift them and their successors\n",
    "for node in G.nodes():\n",
    "    shift_successors_right(node,G)\n",
    "\n",
    "# Store the positions in a dataframe\n",
    "xposdf=pd.DataFrame.from_dict(xposDict,orient='index').reset_index().rename(columns={0:'xpos_unadjusted','index':'Name'}) # Store x positions in a dataframe\n",
    "nodedf=nodedf.merge(xposdf,how='inner',on='Name') # Join them in\n",
    "max_x=nodedf['xpos_unadjusted'].max() # Identify the maximum positional width\n",
    "x_scale=max_x+1\n",
    "\n",
    "#### Update the xposition of unconnected nodes to just span the range\n",
    "#print(nodedf[nodedf['nodeType']=='unconnected'].xpos_unadjusted.unique())\n",
    "for Group in Grouplist:\n",
    "    unconnected_nodes=nodedf[(nodedf['nodeType']=='unconnected')&(nodedf['Groupby']==Group)]['Name']\n",
    "    unconnected_pos={node:int(n%(max_x+1)) for n,node in enumerate(unconnected_nodes)}\n",
    "    unconnected_nodes=pd.DataFrame(unconnected_nodes,columns=['Name'])\n",
    "\n",
    "    unconnected_nodes['xpos_unadjusted']=unconnected_nodes['Name'].apply(lambda x: unconnected_pos[x])\n",
    "    # Join it back in\n",
    "    nodedf=nodedf.merge(unconnected_nodes,how='left',on='Name',suffixes=[None,\"_new\"])\n",
    "    # Update the column\n",
    "    nodedf['xpos_unadjusted'] = np.where(nodedf[\"xpos_unadjusted_new\"].isnull(), nodedf[\"xpos_unadjusted\"], nodedf[\"xpos_unadjusted_new\"] )\n",
    "    # Drop the added column\n",
    "    nodedf=nodedf.drop(columns=['xpos_unadjusted_new'])\n",
    "#print(nodedf[nodedf['nodeType']=='unconnected'].xpos_unadjusted.unique())\n",
    "\n",
    "\n",
    "# Re-scale to be between 0 and 1, with margins  \n",
    "nodedf['xpos']=(nodedf['xpos_unadjusted']+0.5)/x_scale \n",
    "nodedf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each group, for each x position, work out how many nodes there will be in the same vertical column\n",
    "heights_by_group_and_xpos=nodedf[['xpos_unadjusted','Groupby','Name']].groupby(['xpos_unadjusted','Groupby']).count().reset_index()\n",
    "# For each group, work out what the maximum column height needs to be\n",
    "heights_by_group=heights_by_group_and_xpos[['Groupby','Name']].groupby('Groupby').max()\n",
    "# For each group, work out what the maximum column height needs to be, and add '1' position between graphs\n",
    "heights_by_group['withSpacing']=heights_by_group['Name']+1\n",
    "# Aggregate the sum of column heights to fit each of the Groupby values one on top of the other\n",
    "heights_by_group['cutoff']=heights_by_group['withSpacing'].cumsum()\n",
    "heights_by_group['ypos_min']=heights_by_group['cutoff']-heights_by_group['Name']\n",
    "heights_by_group['ypos_max']=heights_by_group['ypos_min']-1+heights_by_group['Name']\n",
    "\n",
    " # Merge the min and max y range for the group back in to the main dataframe\n",
    "nodedf=nodedf.merge(heights_by_group[['ypos_min','ypos_max']].reset_index(),how='inner',on='Groupby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodedf['ypos_range']=nodedf['ypos_max']-nodedf['ypos_min']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 1: RANDOMLY DISTRIBUTE WITHIN VERTICALS\n",
    "nodedf['ypos_unadjusted']=nodedf['ypos_min']\n",
    "ypos=nodedf[nodedf['ypos_range']>0][['Name','ypos_min','ypos_max','Groupby','xpos_unadjusted']]\n",
    "#ypos['ypos_unadjusted']=np.random.randint(ypos['ypos_min'],ypos['ypos_max'])\n",
    "ypos['posingroup']=ypos.groupby(['Groupby','xpos_unadjusted']).cumcount()\n",
    "ypos['ypos_unadjusted']=ypos['ypos_min']+ypos['posingroup']\n",
    "\n",
    "\n",
    "\n",
    "# Join it back in\n",
    "nodedf=nodedf.merge(ypos[['Name','ypos_unadjusted']],how='left',on='Name',suffixes=[None,\"_new\"])\n",
    "# Update the column\n",
    "nodedf['ypos_unadjusted'] = np.where(nodedf[\"ypos_unadjusted_new\"].isnull(), nodedf[\"ypos_unadjusted\"], nodedf[\"ypos_unadjusted_new\"] )\n",
    "# Drop the added column\n",
    "nodedf=nodedf.drop(columns=['ypos_unadjusted_new'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_y=nodedf['ypos_unadjusted'].max() # Identify the maximum positional width\n",
    "y_scale=max_y+1\n",
    "print(max_y)\n",
    "nodedf['ypos']=(nodedf['ypos_unadjusted']+0.5)/y_scale \n",
    "# Create normal pos dict used for network x, although this isn't actually needed as the info \n",
    "# should be read from the df in order to preserve the ordering\n",
    "pos={nodedf[nodedf['Name']==node]['Name'].values[0]:nodedf[nodedf['Name']==node][['xpos', 'ypos']].to_numpy()[0] for node in G.nodes()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################################################\n",
    "# Define Nodes\n",
    "##############################################################################################################\n",
    "#node_x,node_y = [],[]\n",
    "#for node in G.nodes():\n",
    "#    x, y = pos[node] # Extract X & Y from networkX \"pos\" attribute\n",
    "#    node_x.append(x)\n",
    "#    node_y.append(y)\n",
    "\n",
    "   \n",
    "# Create data points trace\n",
    "#node_trace = go.Scatter(x=node_x, y=node_y,\n",
    "node_trace = go.Scatter(x=nodedf['xpos'], y=nodedf['ypos'],\n",
    "                        mode='markers',hoverinfo='text'\n",
    "                        ,hovertext=nodedf['Node_HoverText'],# Read from df\n",
    "                        text=nodedf['Name'],\n",
    "                        marker_symbol=nodedf['Node_Marker_EPIC'], # Read marker info from df\n",
    "                        marker=dict(\n",
    "        #showscale=True,\n",
    "        \n",
    "        color=nodedf['Node_Colour_Status'], \n",
    "#color=nodedf['Node_Colour_EPIC'], \n",
    "        size=10,\n",
    "        line_width=0))\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Define Edges\n",
    "##############################################################################################################\n",
    "\n",
    "# Create a list of arrows to draw as edges\n",
    "arrows=[]   \n",
    "for frm,to in G.edges(): \n",
    "\n",
    "    arrow = go.layout.Annotation(dict(x=pos[to][0],y= pos[to][1]\n",
    "                                      ,xref=\"x\", yref=\"y\",text=\"\",showarrow=True,axref = \"x\", ayref='y'\n",
    "                                      ,ax= pos[frm][0],ay= pos[frm][1],\n",
    "                                      arrowhead = 4,arrowwidth=2,arrowcolor=colourDictHex['Dark Cloud'],))\n",
    "\n",
    "    arrows.append(arrow)\n",
    "    \n",
    "\n",
    "\n",
    "arrows.append(arrows[0]) #For some bizarre reason it leaves off the first one, so I re-add it to the list\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Define Plot layout properties\n",
    "##############################################################################################################\n",
    "\n",
    "graph_layout = go.Layout(\n",
    "\n",
    "                title='<br>BritBox insights team project interdependencies',titlefont_size=16,\n",
    "\n",
    "                showlegend=False,\n",
    "width=1000,\n",
    "    height=1200,\n",
    "                hovermode='closest',\n",
    "\n",
    "                #\n",
    "    margin=dict(b=20,l=5,r=5,t=40),\n",
    "annotations=arrows,\n",
    "                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False)\n",
    "    , yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "paper_bgcolor='rgba(0,0,0,0)',\n",
    "    plot_bgcolor='rgba(0,0,0,0)')\n",
    "\n",
    "\n",
    "\n",
    "##############################################################################################################\n",
    "# Display graph\n",
    "##############################################################################################################\n",
    "fig = go.Figure(data=[node_trace],# Draw Nodes\n",
    "\n",
    "             layout=graph_layout)\n",
    "\n",
    "\n",
    "#fig.update_layout(annotations= arrows,)\n",
    "\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_html(\"project_graphs.html\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "f=codecs.open(\"project_graphs.html\", 'r')\n",
    "\n",
    "text_file = open(\"Output.txt\", \"w\")\n",
    "text_file.write(f.read())\n",
    "\n",
    "text_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "html = urlopen(\"project_graphs.html\").read()\n",
    "soup = BeautifulSoup(html, features=\"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1B\n",
    "# Swap pairs of nodes in a vertical and see if the overall avg difference in positions is minimised\n",
    "\n",
    "# Identify for all nodes, the min, max and average distance to their predecessors\n",
    "for loopnum in range(0,1):\n",
    "    nList=[]\n",
    "    for node in G.nodes():\n",
    "        # Get the y pos of the node in question\n",
    "        node_pos=nodedf.loc[nodedf['Name']==node,'ypos_unadjusted'].values[0]\n",
    "        \n",
    "        # Get the neighbours of the node\n",
    "        succ=list(G.successors(node))\n",
    "        pred=list(G.predecessors(node))\n",
    "        neighbours=succ+pred\n",
    "        \n",
    "        # If it has neighbours\n",
    "        if len(neighbours)>0:\n",
    "            # Calculate the best possible differences it could have for the number of inbound and outbound nodes\n",
    "            best_diff_succ=[np.floor((n+1)/2) for n,i in enumerate(succ)] # Calculate the best possible differences in ypos [0,1,1,2,2]\n",
    "            best_diff_pred=[np.floor((n+1)/2) for n,i in enumerate(pred)] # Calculate the best possible differences in ypos [0,1,1,2,2]\n",
    "            best_diff=np.mean(best_diff_succ+best_diff_pred)\n",
    "\n",
    "            neighbours_positions=nodedf.loc[nodedf['Name'].isin(neighbours),['ypos_unadjusted']]\n",
    "             if neighbours_positions.empty:\n",
    "                avg_distance=None\n",
    "            else:\n",
    "                neighbours_positions['difference']=abs(node_pos-neighbours_positions['ypos_unadjusted'])\n",
    "                avg_difference=neighbours_positions['difference'].mean()\n",
    "                nList.append({'node':node,\n",
    "                              'difference':avg_difference,\n",
    "                              'best difference':best_diff,\n",
    "                              'error':avg_difference-best_diff,\n",
    "                              'successors':succ,\n",
    "                              'predecessors':pred,\n",
    "                              'neighbours_positions':list(neighbours_positions.loc[:,'ypos_unadjusted']),\n",
    "                              'current pos':node_pos,\n",
    "                              'range begin':nodedf.loc[nodedf['Name']==node,'ypos_min_y'].values[0],\n",
    "                              'range end':nodedf.loc[nodedf['Name']==node,'ypos_max_y'].values[0],\n",
    "                              'vertical':nodedf.loc[nodedf['Name']==node,'xpos_unadjusted'].values[0],\n",
    "                              'Groupby':nodedf.loc[nodedf['Name']==node,'Groupby'].values[0]\n",
    "                             })\n",
    "            #print('Avg pos is',avg_distance)\n",
    "\n",
    "\n",
    "        \n",
    "        #print('''        ''')\n",
    "        \n",
    "        \n",
    "        \n",
    "    newdf=pd.DataFrame(nList).sort_values(by='error',ascending=False).reset_index(drop=True)\n",
    "    print(newdf['error'].mean())\n",
    "newdf\n",
    "    # Go through each node\n",
    "        # then through range from and to\n",
    "    \n",
    "    \n",
    "    # Calculate the overall graph's average distance\n",
    "\n",
    "# Identify, for each node, the minimum obtainable (e.g. if 5 successors, it would have a minimum avg of (0+1+1+2+2/5))\n",
    "# For nodes with largest variance vs. minimum possible, identify ideal positions (within EPIC range)\n",
    "\n",
    "# See if putting it in one of the other positions would improve things AND reduce the overall graph \n",
    "### (if moving to higher number, shift things up, if moving to lower number, shift things down)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other y pos algorithm attempts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ=['a','b','c','d','e','f','g','h']\n",
    "best_pos_succ=[np.floor((n+1)/2) for n,i in enumerate(succ)]\n",
    "best_pos_succ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METHOD 2: Prioritise/Centralise betweenness centrality & order standalones at the bottom\n",
    "nodedf['ypos_unadjusted']=None\n",
    "centrality=dict(G.degree())\n",
    "nodedf['Centrality']=nodedf['Name'].apply(lambda x: centrality[x] if x in centrality.keys() else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within a groupby, pick the node that has the most connected nodes, and put it in the middle of the group\n",
    "\n",
    "# For that node, loop through it's neighbours and base their position on this one (avg of predecessors)\n",
    "\n",
    "# Repeat for the next most connected node that doesn't have a position already\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work out, for each group, which of the x verticals has the most nodes. This is the densest one and so is best placed to dictate other positions\n",
    "df1=nodedf[['xpos_unadjusted','Groupby','Name']].groupby(['xpos_unadjusted','Groupby']).count().reset_index()\n",
    "df1['Rank']=df1.groupby('Groupby')['Name'].rank(method=\"first\",ascending=False)\n",
    "starting_verticals=df1[df1['Rank']==1][['xpos_unadjusted','Groupby']]\n",
    "starting_verticals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR, for left most nodes, order by successors in common\n",
    "verticals=list(nodedf['xpos'].unique())\n",
    "\n",
    "for group in Grouplist:\n",
    "    starting_vertical=starting_verticals.loc[starting_verticals['Groupby']==group,'xpos_unadjusted'].values[0] # Retrieve the initialisation vertical\n",
    "    \n",
    "    mini_nodedf=nodedf.loc[(nodedf['xpos_unadjusted']==starting_vertical)&(nodedf['Groupby']==group),['Name']]\n",
    "    print(mini_nodedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once initial layer has been done, iterate through layers after it, ordering by average position of predecessors to minimise crossover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ypos=nodedf[nodedf['ypos_range']>0][['Name','ypos_min','ypos_max','Groupby','xpos_unadjusted']]\n",
    "#ypos['ypos_unadjusted']=np.random.randint(ypos['ypos_min'],ypos['ypos_max'])\n",
    "ypos['posingroup']=ypos.groupby(['Groupby','xpos_unadjusted']).cumcount()\n",
    "ypos['ypos_unadjusted']=ypos['ypos_min']+ypos['posingroup']\n",
    "ypos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Join it back in\n",
    "nodedf=nodedf.merge(ypos[['Name','ypos_unadjusted']],how='left',on='Name',suffixes=[None,\"_new\"])\n",
    "# Update the column\n",
    "nodedf['ypos_unadjusted'] = np.where(nodedf[\"ypos_unadjusted_new\"].isnull(), nodedf[\"ypos_unadjusted\"], nodedf[\"ypos_unadjusted_new\"] )\n",
    "# Drop the added column\n",
    "nodedf=nodedf.drop(columns=['ypos_unadjusted_new'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nodedf[['Name','EPIC_Class','ypos_unadjusted']].sort_values(by='ypos_unadjusted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
