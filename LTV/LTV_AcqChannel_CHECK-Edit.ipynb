{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer life time value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install this package for use of graphical properties\n",
    "!pip install plotly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS\n",
    "##### 1. Extract the data required\n",
    "##### 2. Plot actual data - what does this look like? \n",
    "##### 3. Fit a curve based on actual data\n",
    "##### 4. Select best fitted curve\n",
    "##### 5. Intergrate ^, what is the expected tenure per segment?\n",
    "##### 6. Multiple this by rev (for now use 5.99 as base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from   scipy import optimize\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "\n",
    "# Install/import plotly packages- this package has lots of graphical properties\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from pandas.io import gbq\n",
    "from scipy import integrate \n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Load custom scripts in reusable_code folder\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "\n",
    "import google_api_functions as gaf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Define Functions for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Revenues(Cohort_table,End_date):\n",
    "    if not End_date:\n",
    "        End_date=str(date.today())\n",
    "    # Check end date is in the correct format\n",
    "    if len(re.findall(r\"(([12]\\d{3})-(0[1-9]|1[012])-(0\\d|1\\d|2\\d|3[01]))\", End_date))>0:\n",
    "        pass\n",
    "    else:\n",
    "        print(\"Error- End Date in wrong format. Please use YYYY-MM-DD\")\n",
    "    \n",
    "    # Check that the BQ table has the correct columns\n",
    "    # User should input a table of Segment_Name, BritBox_ID, Subscription_ID and AcqnDate\n",
    "    \n",
    "    # The code should then take this table and join it to the revenues data and generate the resultant output in the format needed for the rest of the LTV code\n",
    "    \n",
    "    \n",
    "\n",
    "def plot_df(df, x, y, title=\"Daily Revenue\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Determine the fitted curves per segment and plot this \n",
    "\n",
    "# This is used for the curve-fitting procedure later, which requires the function as an input\n",
    "def func_expdecay(xdata, a, b ,k):\n",
    "    return a * np.exp(-b * xdata) + k\n",
    "\n",
    "def func_log(xdata,a,b):\n",
    "    return a * -np.log(b*xdata) \n",
    "\n",
    "def func_linear(xdata,a,b):\n",
    "    return a * xdata +b  #mx +c\n",
    "\n",
    "def func_normal(xdata,a,mu,std):\n",
    "    return a*(np.exp(-((xdata-mu)**2)/(2*std**2)))\n",
    "\n",
    "def func_weib(xdata,n,a):\n",
    "    return (a / n) * (xdata / n)**(a-1) * np.exp(-(xdata/n)**a)\n",
    "\n",
    "def func_lognormal(xdata, a, mu, std):\n",
    "    return a*((1.0/(xdata*std*np.sqrt(2.0*np.pi)))*np.exp(-1.0*(((np.log(xdata)-mu)**2.0)/(2.0*(std**2.0)))))\n",
    "\n",
    "def func_power(x, a, b):\n",
    "    return a*(x**b)\n",
    "\n",
    "def fit_test(ydata,y2,test='CHI2'):\n",
    "    \"\"\"returns fit scores for chi2 and rmse \n",
    "    chisquare requires large freq ideally greater than 5 \n",
    "    (ref:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html)\n",
    "    \"\"\"\n",
    "    if (test=='CHI2') and (min(ydata)>5): \n",
    "        return stats.chisquare(f_obs=ydata, f_exp=y2)\n",
    "    elif test == 'RMSE':\n",
    "        return np.sqrt(np.mean((ydata - y2)**2))\n",
    "    else:\n",
    "        print('check conditions')\n",
    "\n",
    "        \n",
    "        \n",
    "def mean_absolute_percentage_error(actual, estimated): \n",
    "    actual, estimated = np.array(actual), np.array(estimated)\n",
    "    return np.mean(np.abs((actual - estimated) / actual)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data to determine LTV\n",
    "bq = bigquery.Client(project='itv-bde-analytics-prd',credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter of 500+ customers needed per segment \n",
    "\n",
    "query=\"\"\"\n",
    "\n",
    "with table1 as \n",
    "\n",
    "(select a.*,concat(a.itvid, a.subscriptionid) as ID, b.Segment, b.signuptime from \n",
    "\n",
    "(select distinct itvid, subscriptionid,  proratedDailyRevenue,date_value  from `itv-bde-analytics-prd.britbox_model.LTV_Entlmt_Daily_Revenue`) as a \n",
    "\n",
    "inner join \n",
    "\n",
    "(select a.*, case when a.utm_content = 'CRM' then b.level2 else a.utm_content end as Segment\n",
    "\n",
    "from (select distinct britbox_ID, subscriptionid , subscription.firstStart as signuptime, acquisition.utm_content as utm_content from  `itv-bde-analytics-prd.britbox_analytics.entitlements`\n",
    "where acquisition.utm_source != 'Non_web' and date(subscription.firstStart) <= '2020-11-10' ) as a \n",
    "left join \n",
    "\n",
    "(select  * from `itv-bde-analytics-prd.britbox_mart.last_click_referrer`  ) as b\n",
    "\n",
    "on a.britbox_ID = b.user_ID and  a.subscriptionid = b.subscription_ID) as b\n",
    "\n",
    "on a.itvid = b.britbox_ID \n",
    "    and a.subscriptionid = b.subscriptionid\n",
    "\n",
    "and date(b.signuptime) <= date(a.date_value) and date(b.signuptime) <= (current_date() -1))\n",
    "\n",
    ",table1a as \n",
    "\n",
    "(select distinct id,proratedDailyRevenue, segment, row_number() over (partition by id , Segment , signuptime order by date_value) as Day from table1)\n",
    "\n",
    ", table2 as\n",
    "(\n",
    "select distinct  * , proratedDailyRevenue/Customers as avg_dailyrev,\n",
    "max(Customers) over (partition by Segment) as Total_Customers,\n",
    "max(ifnull(proratedDailyRevenue/Customers, 0.00000000001)) over (partition by Segment) as max_rev \n",
    "from (select distinct \n",
    "Day,\n",
    "Segment,\n",
    "count(distinct id) as Customers,\n",
    "sum(ifnull(proratedDailyRevenue,0.00000000001))/100 as proratedDailyRevenue\n",
    "from  table1a\n",
    "\n",
    "group by 1,2\n",
    "order by 1,2)\n",
    "\n",
    "order by 1,2,3)\n",
    "\n",
    "select * from table2 where Total_Customers > 500\n",
    "\n",
    "    \"\"\"\n",
    "df = bq.query(query ).to_dataframe()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table so data is in correct structure for making graph\n",
    "df_2_pivot = pd.pivot_table(df,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "df_2_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel('Actual Acq Channel graph data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actual data\n",
    "# This enables the data to be plotted in seperate lines split by 'segment' by allocating the ydata per row \n",
    "# hence why data needed to be pivoted so there's one row per segment\n",
    "\n",
    "traces = [go.Scatter (\n",
    "        x = df_2_pivot.columns,\n",
    "        y = df_2_pivot.loc[rowname],\n",
    "        mode = 'lines',\n",
    "        name = rowname\n",
    ")for rowname in df_2_pivot.index]\n",
    "\n",
    "# Plot the data\n",
    "graph = go.Figure(data = traces)\n",
    "#graph.update_xaxes(type=\"category\",)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "x = sorted(df.Day)\n",
    "y = df.avg_dailyrev\n",
    "plot_df(df, x, y, title='Daily Revenue')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "x = sorted(df.Day)\n",
    "y = df.Customers\n",
    "plot_df(df, x, y, title='Daily Customers')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Curve Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SW edit>> functions are held in a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### INITIALISATION  ####################\n",
    "# Specify the test used to determine Goodness of Fit of the fitted curves\n",
    "test_type = \"RMSE\"\n",
    "integrate_to_days=1095 # How many days to integrate to e.g. 365 is one year\n",
    "function = [func_expdecay, func_normal,func_weib, func_lognormal,func_power] # Choose which functions to fit\n",
    "\n",
    "# Initialise empty lists where the parameters can be stored for later evaluation if needed\n",
    "list_parameters = []\n",
    "list_parameter_covariances = []\n",
    "\n",
    "# Initialise an empty DataFrame to hold the final output \n",
    "LTV_dataset3 = pd.DataFrame()\n",
    "\n",
    "# Initialise empty dictionaries to hold the best functions (curves) and parameters\n",
    "selected_functions = {}\n",
    "LTV_selections=[]\n",
    "\n",
    "#### ALL SEGMENTS AND DAYS\n",
    "# df is the original, full table holding all segments and days both as records\n",
    "# df_2_pivot still holds all data, but now days are in columns, and there is just one row per segment\n",
    "\n",
    "#### ONE SEGMENT AND ALL DAYS >> USED FOR PLOTTING AND EMPIRICAL LTV CALCS\n",
    "## sub_df is the original table but for just one segment (recreated in each loop)\n",
    "#### df_pivot is a pivot of sub_df that has one row and all days held in columns\n",
    "\n",
    "#### ONE SEGMENT AND ONLY THE DAYS AFTER THE MAX >> USED FOR CURVE FITTING\n",
    "### sub_df2 is the subset of sub_df that only looks at the data from the Max point onwards\n",
    "#### sub_pivot is a pivot of sub_df2 that again transposes so that days are held in columns. This is used for curve fitting\n",
    "\n",
    "\n",
    "#################### LOOP THROUGH SEGMENTS AND FIT CURVES  ####################\n",
    "\n",
    "# Loop through each segment name (table is structured as one row per segment, one column per day)\n",
    "for n,rowname in enumerate(df_2_pivot.index) : \n",
    "    \n",
    "    print('''\\n ---------------------------------------------------------------------- \\n \\n Segment: ''' + str(rowname))\n",
    "    \n",
    "    #Initialise empty lists for later use\n",
    "    errors = []\n",
    "    param_list= []\n",
    "    \n",
    "    #################### Create a Dataframe that finds the maximum daily revenue and only holds data from there onwards\n",
    "    # Assumption is that there will pretty much always be a decay curve, barring any major changes in price\n",
    "    # This dataframe created will be the one used to fit a function, as it is much easier to fit a decay curve than something which also account for the trial period\n",
    "    \n",
    "    # Subset columns needed and only this segment\n",
    "    sub_df=df[['Day','Segment','avg_dailyrev','max_rev','Total_Customers']][df['Segment']==rowname]\n",
    "    sub_df = sub_df.reset_index(drop = True)\n",
    "    \n",
    "    # Find Index in which avg_dailyrev = max_rev\n",
    "    Starting_Curve_Value = sub_df[sub_df.avg_dailyrev==sub_df.max_rev].index.values\n",
    "    Starting_Curve_Value2 = Starting_Curve_Value[0] # There may be multiple days with the same (max) revenue, so start from the first\n",
    "    \n",
    "    # Subset this data so you start the curve fitting from the max value\n",
    "    sub_df2 = sub_df.iloc[Starting_Curve_Value2:]\n",
    "\n",
    "    \n",
    "    #################### Transpose df and sub_df2 to get days into columns for the selected segment    \n",
    "    sub_pivot=pd.pivot_table(sub_df2,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "    df_pivot=pd.pivot_table(sub_df,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "    \n",
    "    \n",
    "    #################### Loop through the range of functions to be fitted to the LTV data     \n",
    "    func_dict={}   \n",
    "    for func in function : \n",
    "\n",
    "        #print('Function: ' + func.__name__)\n",
    "        \n",
    "        # Turn the above Data Frames into series/arrays: all= _all data for a segment _max = only data since the max\n",
    "        xdata_all = np.array(df_pivot.columns)\n",
    "        ydata_all = np.array(df_pivot.loc[rowname])\n",
    "        \n",
    "        xdata_max = np.array(sub_pivot.columns)\n",
    "        ydata_max = np.array(sub_pivot.loc[rowname])\n",
    "        \n",
    "        #################### Define p0 for each function (effectively parameter seeds) >> could be moved into the functions themselves as defaults\n",
    "     \n",
    "        if func.__name__ in [  'func_expdecay']:\n",
    "            p0 = (0.2,0.1, 1.5) # inital guess\n",
    "    \n",
    "        if func.__name__ in ['func_normal']:\n",
    "            mu, std = norm.fit(xdata_max)\n",
    "            p0 = (85,mu,std)\n",
    "            \n",
    "        if func.__name__ in ['func_lognormal']:\n",
    "            mu, std = norm.fit(xdata_max)\n",
    "            p0 = (10,mu,std)\n",
    "    \n",
    "        if func.__name__ in [ 'func_linear', 'func_log']:\n",
    "            p0 = (2,3)\n",
    "        \n",
    "        if func.__name__ in ['func_weib', 'func_power']:\n",
    "            p0 = (1,1)\n",
    "        \n",
    "        if func.__name__ in ['func_exp2']:\n",
    "            p0 = (0.2,0.1,1.5,2)\n",
    "    \n",
    "        #################### Attempt to fit the function, return the parameters & covariance of the best fit (per function)\n",
    "        try:\n",
    "            params, params_covariance = optimize.curve_fit(func, xdata_max, ydata_max,p0, maxfev=100000,method='dogbox') \n",
    "            \n",
    "            # Generate an array of x values (days) from the peak to the end of the observed dataset.\n",
    "            # We do this because the number of days for any given segment may not = the total number of days across all segments \n",
    "            # E.g. some segments we may only have 100 days for, some we have 200 for, or the peak is at a different place.\n",
    "            x2 = np.linspace(min(xdata_max),max(xdata_max),len(xdata_max))\n",
    "            y2 = func(x2,*params) # Apply the fitted curve to the dummy X values\n",
    "\n",
    "            # Account for when y values trend into negatives as we wouldn't want a negative revenue-- SHOULD WE DO THIS HERE OR LATER, SURELY ONLY NEED FOR INTEGRATION?\n",
    "            y2[y2 < 0] = 0\n",
    "\n",
    "            fit_error = fit_test(ydata_max,y2,test=test_type) # Fits the generated curve against the observed using intended test specified before loop\n",
    "            abserr=mean_absolute_percentage_error(ydata_max,y2) # Calculates mean absolute error\n",
    "            \n",
    "            print('Function {} on segment {} has an average error of {:.2f}% and a value of {:.2f} for {}'.\\\n",
    "                  format(func,rowname,abserr*100,fit_error,test_type))\n",
    "\n",
    "            func_dict[func]={'error':fit_error,'params':params,'mape':abserr}\n",
    "             # Plot each function\n",
    "            plt.plot(x2, y2, color=np.random.rand(3,), label=func.__name__) # generates a random different colour per segment - check if there's a way to fix this? \n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "            print(\"Failed to fit function {} to segment {}\".format(func,rowname))\n",
    "            \n",
    "   \n",
    "       \n",
    "    #################### Select best fitting function according to error, store the function name e.g. \"normal\" and the parameters\n",
    "    selected_func,selected_params = [(key,func_dict[key]['params']) for key in func_dict if func_dict[key]['error']==min([d['error'] for d in func_dict.values()])][0]\n",
    "    selected_functions[rowname]= {'function':selected_func,'params':selected_params} # Add to dictionary storing choices for all segments\n",
    "\n",
    "    \n",
    "    #################### Plot actuals on same graph as above\n",
    "    plt.plot(xdata_all, ydata_all, 'bo', label='Real Data', markersize=0.5) # This is the real data but omitted for now as it makes the graph look messy\n",
    "    \n",
    "    # Format the plot\n",
    "    plt.xlabel(\"Months Subscribed\")\n",
    "    plt.ylabel(\"Daily Revenue\")\n",
    "    plt.title(\"Segment Decay Curve\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() # Display the plot\n",
    "    \n",
    "    \n",
    "    #################### Determine the best fitted curve to the real data and integrate to determine LTV\n",
    "    \n",
    "    observed_days=max(xdata_all) # Calculate the number of days of actual data we have\n",
    "    predicted_days=integrate_to_days-observed_days # Calculate the number of days we're predicting\n",
    "\n",
    "    \n",
    "    ###### Select the necessary data ranges\n",
    "    empirical_x, empirical_y=xdata_all,ydata_all # The actual data observed. This will be both used for visual and for integration\n",
    "    \n",
    "    forecast_x=range(observed_days+1,integrate_to_days + 1)\n",
    "    forecast_y=selected_func(forecast_x,*selected_params) # The forecast into the future. This should be sequentially after the observed data ends and is used for the integration\n",
    "    \n",
    "    curve_x=range(min(xdata_max),integrate_to_days + 1)\n",
    "    curve_y=selected_func(curve_x,*selected_params) # The curve fitted to both the empirical data after the max, and the forecast moving forwards. Used for the visual\n",
    "    \n",
    "\n",
    "    ###### Set -ve forecasts to 0\n",
    "    forecast_y[forecast_y<0]=0\n",
    "    curve_y[curve_y<0]=0\n",
    "    \n",
    "    ###### Plot and Integrate\n",
    "    # Plot & integrate empirical data\n",
    "    plt.plot(empirical_x,empirical_y,c='b') # Plot line in blue\n",
    "    plt.fill_between(empirical_x,empirical_y, where = [(x >= 0)  and (x <= max(empirical_x)) for x in empirical_x], color = 'blue', alpha = 0.3) # Fill in under the graph\n",
    "    val_actual_curve = scipy.integrate.trapz(empirical_y,empirical_x) # Integrate under the curve\n",
    "    \n",
    "    # Plot & integrate the fitted curve\n",
    "    plt.plot(curve_x,curve_y,c='r') # Plot the line in red\n",
    "    plt.fill_between(forecast_x,forecast_y, color = 'red', alpha = 0.3) # Fill in under the forecasted area only\n",
    "    val_fitted_curve = scipy.integrate.trapz(forecast_y, forecast_x)\n",
    "   \n",
    "    # Generate clean name, add as title and save figure\n",
    "    title=rowname + \": \" +str(selected_func.__name__).replace('func_','')\n",
    "    plt.title(title)\n",
    "    save_name=title+ '.png'\n",
    "    #plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    LTV = (val_actual_curve + val_fitted_curve)\n",
    "    print('LTV Value of Cohort: {} is £{:.2f}'.format(str(rowname), LTV))\n",
    "    print('The observed component (blue) totals £{:.2f} across the first {} days'.format(val_actual_curve,observed_days))\n",
    "    print('The forecasted component (red) totals £{:.2f} across the next {} days'.format(val_fitted_curve,predicted_days))\n",
    "    \n",
    "    LTV_selections.append({'Segment':rowname,\\\n",
    "                           'LTV':round(LTV,2),'LTV_days':integrate_to_days,\\\n",
    "                           'LTV_Observed':round(val_actual_curve,2),'LTV_observed_days':observed_days,\\\n",
    "                           'LTV_Predicted':round(val_fitted_curve,2),'LTV_predicted_days':predicted_days,\\\n",
    "                           'Fitted_Function':str(selected_func.__name__).replace('func_',''),'Fitted_Params':str(selected_params),\\\n",
    "                          })\n",
    "\n",
    "    # Export forecasted data sets to csv\n",
    "    # All Data\n",
    "    #export_dataset = pd.DataFrame({'Avg_Daily_Rev':empirical_y} , index=empirical_x) \n",
    "    # Curve Data \n",
    "    #export_dataset2 = pd.DataFrame({'Avg_Daily_Rev':curve_y} , index=curve_x)\n",
    "    # Integrate Data\n",
    "    #export_dataset3 = pd.DataFrame({'Avg_Daily_Rev':forecast_y} , index=forecast_x)\n",
    "\n",
    "    #export_dataset4 = pd.concat([export_dataset, export_dataset2], axis = 1)\n",
    "\n",
    "    #export_dataset5 = pd.merge(export_dataset4, export_dataset3, left_index=True, right_index = True, how = 'left')\n",
    "\n",
    "    #export_dataset5.to_excel(rowname + ' Graph Data.xlsx')\n",
    "          \n",
    "LTV_final_df=pd.DataFrame(LTV_selections)\n",
    "LTV_final_df"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
