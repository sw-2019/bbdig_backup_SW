{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer life time value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install this package for use of graphical properties\n",
    "!pip install plotly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STEPS\n",
    "##### 1. Extract the data required\n",
    "##### 2. Plot actual data - what does this look like? \n",
    "##### 3. Fit a curve based on actual data\n",
    "##### 4. Select best fitted curve\n",
    "##### 5. Intergrate ^, what is the expected tenure per segment?\n",
    "##### 6. Multiple this by rev (for now use 5.99 as base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from   scipy import optimize\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "import sys\n",
    "\n",
    "# Install/import plotly packages- this package has lots of graphical properties\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "\n",
    "from scipy import integrate \n",
    "from scipy.stats import norm\n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "\n",
    "# Load custom scripts in reusable_code folder\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "\n",
    "import google_api_functions as gaf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "pd.set_option('display.max_rows', 5000)\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting data to determine LTV\n",
    "bq = bigquery.Client(project='itv-bde-analytics-prd',credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io import gbq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter of 500+ customers needed per segment \n",
    "\n",
    "query=\"\"\"\n",
    "\n",
    "with table1 as \n",
    "\n",
    "(select distinct a.*,concat(a.itvid, a.subscriptionid) as ID, c.Segment, b.signuptime from \n",
    "\n",
    "(select distinct  itvid, subscriptionid,  proratedDailyRevenue,date_value from `itv-bde-analytics-prd.britbox_model.LTV_Entlmt_Daily_Revenue`) as a \n",
    "\n",
    "inner join \n",
    "\n",
    "(select distinct britbox_ID, subscriptionid, subscription.firstStart as signuptime from  `itv-bde-analytics-prd.britbox_analytics.entitlements`\n",
    "where date(subscription.firstStart) >= '2020-04-01' and billingprovider in ('iTunes', 'Stripe')\n",
    "and date(subscription.firstStart) <= '2020-11-10'\n",
    ") as b\n",
    "\n",
    "on a.itvid = b.britbox_ID \n",
    "    and a.subscriptionid = b.subscriptionid \n",
    "\n",
    "and date(b.signuptime) <= date(a.date_value) and date(b.signuptime) <= (current_date() -1)\n",
    "\n",
    "left join \n",
    "\n",
    "(select * from (select distinct \n",
    "britbox_ID, \n",
    "platform_clean as Segment, \n",
    "count(distinct stream_ID) as Streams\n",
    ",row_number() over (partition by britbox_ID order by count(distinct stream_ID) desc) as row_num\n",
    "from `itv-bde-analytics-dev.britbox_analytics.Viewing_clean`\n",
    "where open_event_time >= '2020-04-01' and  open_event_time <= '2020-11-10' \n",
    "group by 1,2) where row_num = 1 and lower(Segment) like '%connected%' ) as c\n",
    "\n",
    "on a.itvid = c.britbox_ID\n",
    "\n",
    ")\n",
    "\n",
    ",table1a as \n",
    "\n",
    "(select * , row_number() over (partition by id , Segment , signuptime order by date_value ) as Day from table1)\n",
    "\n",
    " , table2 as\n",
    " (\n",
    " select distinct  * , proratedDailyRevenue/Customers as avg_dailyrev,\n",
    " max(Customers) over (partition by Segment) as Total_Customers,\n",
    " max(ifnull(proratedDailyRevenue/Customers, 0.00000000001)) over (partition by Segment) as max_rev \n",
    " from (select distinct \n",
    " Day,\n",
    " Segment,\n",
    " count(distinct id) as Customers,\n",
    " sum(ifnull(proratedDailyRevenue,0.00000000001))/100 as proratedDailyRevenue\n",
    "from  table1a\n",
    "\n",
    "group by 1,2\n",
    "order by 1,2)\n",
    "\n",
    "order by 1,2,3)\n",
    "\n",
    " select * from table2 where Total_Customers > 500\n",
    "\n",
    "    \"\"\"\n",
    "df = bq.query(query ).to_dataframe()\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the table so data is in correct structure for making graph\n",
    "df_2_pivot = pd.pivot_table(df,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "df_2_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.to_excel('Actual device type graph data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting actual data\n",
    "# This enables the data to be plotted in seperate lines split by 'segment' by allocating the ydata per row \n",
    "# hence why data needed to be pivoted so there's one row per segment\n",
    "\n",
    "traces = [go.Scatter (\n",
    "        x = df_2_pivot.columns,\n",
    "        y = df_2_pivot.loc[rowname],\n",
    "        mode = 'lines',\n",
    "        name = rowname\n",
    ")for rowname in df_2_pivot.index]\n",
    "\n",
    "# Plot the data\n",
    "graph = go.Figure(data = traces)\n",
    "#graph.update_xaxes(type=\"category\",)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Plot\n",
    "x = sorted(df.Day)\n",
    "y = df.Customers\n",
    "\n",
    "def plot_df(df, x, y, title=\"Daily Revenue\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "plot_df(df, x, y, title='Daily Revenue')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Curve Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the fitted curves per segment and plot this \n",
    "\n",
    "# This is used for the curve-fitting procedure later, which requires the function as an input\n",
    "def func_expdecay(xdata, a, b ,k):\n",
    "    return a * np.exp(-b * xdata) + k\n",
    "\n",
    "#def func_exp2(xdata, a, b, c, d):\n",
    "#    return a*np.exp(b*xdata + c) + d \n",
    "\n",
    "def func_log(xdata,a,b):\n",
    "    return a * -np.log(b*xdata) \n",
    "\n",
    "#def func_linear(xdata,a,b):\n",
    "#    return a * xdata +b  #mx +c\n",
    "\n",
    "def func_normal(xdata,a,mu,std):\n",
    "    return a*(np.exp(-((xdata-mu)**2)/(2*std**2)))\n",
    "\n",
    "def func_weib(xdata,n,a):\n",
    "    return (a / n) * (xdata / n)**(a-1) * np.exp(-(xdata/n)**a)\n",
    "\n",
    "def func_lognormal(xdata, a, mu, std):\n",
    "    return a*((1.0/(xdata*std*np.sqrt(2.0*np.pi)))*np.exp(-1.0*(((np.log(xdata)-mu)**2.0)/(2.0*(std**2.0)))))\n",
    "\n",
    "def func_power(x, a, b):\n",
    "    return a*(x**b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "function = [func_expdecay, func_log, func_normal,func_weib, func_lognormal, func_power]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_test(ydata,y2,test='CHI2'):\n",
    "    \"\"\"returns fit scores for chi2 and rmse \n",
    "    chisquare requires large freq ideally greater than 5 \n",
    "    (ref:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html)\n",
    "    \"\"\"\n",
    "    if (test=='CHI2') and (min(ydata)>5): \n",
    "        return stats.chisquare(f_obs=ydata, f_exp=y2)\n",
    "    elif test == 'RMSE':\n",
    "        return np.sqrt(np.mean((ydata - y2)**2))\n",
    "    else:\n",
    "        print('check conditions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test_type = \"RMSE\"\n",
    "list_1 = []\n",
    "list_2 = []\n",
    "list_3 = []\n",
    "LTV_dataset3 = pd.DataFrame()\n",
    "best_curves = {}\n",
    "best_params = {}\n",
    "\n",
    "# Determine the best fitted curve to the real data and integrate to determine LTV\n",
    "def plot_best_values(data,best_params,best_curve):\n",
    "    func = best_curves[rowname]\n",
    "    #x2 = np.linspace(min(xdata_max), 1095, 1095) # 3 years of data\n",
    "    x2 = range(min(xdata_max),1095 + 1)\n",
    "    params = best_params[rowname]\n",
    "    y2 = func(x2,*params)\n",
    "    y2[y2 < 0] = 0\n",
    "    func_out = func(x2,*params)\n",
    "    func_out[func_out < 0] = 0\n",
    "    plt.plot(xdata_all,ydata_all,c='b')\n",
    "    plt.plot(x2,func_out,c='r')\n",
    "    plt.title(rowname + \": \" + func.__name__)\n",
    "    plt.fill_between(xdata_all, ydata_all, where = [(x >= 0)  and (x <= max(xdata_all)) for x in xdata_all], color = 'blue', alpha = 0.3)\n",
    "    plt.fill_between(x2, y2, where = [(x > max(xdata_all)) for x in x2 ], color = 'red', alpha = 0.3)\n",
    "    save_name = rowname + \": \" + func.__name__ + '.png'\n",
    "    #plt.savefig(save_name)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def mean_absolute_percentage_error(ydata_max, y2): \n",
    "    ydata_max, y_py2red = np.array(ydata_max), np.array(y2)\n",
    "    return np.mean(np.abs((ydata_max - y2) / ydata_max)) \n",
    "    \n",
    "for n,rowname in enumerate(df_2_pivot.index) : \n",
    "    \n",
    "    print('Segment: ' + str(rowname))\n",
    "    errors = []\n",
    "    param_list= []\n",
    "    \n",
    "    sub_df=df[['Day','Segment','avg_dailyrev','max_rev','Total_Customers']][df['Segment']==rowname]\n",
    "    \n",
    "    sub_df = sub_df.reset_index(drop = True)\n",
    "    # Find Index in which avg_dailyrev = max_rev\n",
    "    Starting_Curve_Value = sub_df[sub_df.avg_dailyrev==sub_df.max_rev].index.values\n",
    "    \n",
    "    Starting_Curve_Value2 = Starting_Curve_Value[0]\n",
    "    \n",
    "    # Subset this data so you start the curve fitting from the max value\n",
    "    sub_df2 = sub_df.iloc[Starting_Curve_Value2:]\n",
    "\n",
    "    sub_pivot=pd.pivot_table(sub_df2,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "    \n",
    "    df_pivot=pd.pivot_table(sub_df,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "    \n",
    "    Total_Customers = max(sub_df.Total_Customers)\n",
    "    \n",
    "    for func in function : \n",
    "\n",
    "        #print('Function: ' + func.__name__)\n",
    "        \n",
    "        xdata_all = np.array(df_pivot.columns)\n",
    "        ydata_all = np.array(df_pivot.loc[rowname])\n",
    "        \n",
    "        xdata_max = np.array(sub_pivot.columns)\n",
    "        ydata_max = np.array(sub_pivot.loc[rowname])\n",
    "        \n",
    "        #print(sum(np.isnan(ydata_all)))\n",
    "    \n",
    "        if func.__name__ in [  'func_expdecay']:\n",
    "            p0 = (0.2,0.1, 1.5) # inital guess\n",
    "    \n",
    "        if func.__name__ in ['func_normal']:\n",
    "            mu, std = norm.fit(xdata_max)\n",
    "            p0 = (85,mu,std)\n",
    "            \n",
    "        if func.__name__ in ['func_lognormal']:\n",
    "            mu, std = norm.fit(xdata_max)\n",
    "            p0 = (10,mu,std)\n",
    "    \n",
    "        if func.__name__ in [ 'func_linear', 'func_log']:\n",
    "            p0 = (2,3)\n",
    "        \n",
    "        if func.__name__ in ['func_weib', 'func_power']:\n",
    "            p0 = (1,1)\n",
    "    \n",
    "        params, params_covariance = optimize.curve_fit(func, xdata_max, ydata_max,p0, maxfev=100000,method='dogbox') \n",
    "        \n",
    "        #print('Parameters: ' + str(params))\n",
    "        list_1.append(params)\n",
    "        list_2.append(params_covariance)\n",
    "        \n",
    "        range1 = len(ydata_max) + (len(ydata_all)-len(ydata_max))\n",
    "        \n",
    "        x2 = np.linspace(min(xdata_max), range1, len(ydata_max))  \n",
    "        y2 = func(x2,*params) # Apply the fitted curve to the dummy X values\n",
    "    \n",
    "        # Account for when y values trend into negatives as we wouldn't want a negative revenue\n",
    "        y2[y2 < 0] = 0\n",
    "    \n",
    "        fit_error = fit_test(ydata_max,y2,test=test_type)\n",
    "        #print(test_type,fit_error)\n",
    "        \n",
    "        errors.append(fit_error)\n",
    "        param_list.append(params)\n",
    "        \n",
    "        #mean_absolute_percentage_error = mean_absolute_percentage_error(ydata_max, y2)\n",
    "        \n",
    "        #plt.plot(x2, y2, color=np.random.rand(3,), label=func.__name__) # generates a random different colour per segment - check if there's a way to fix this? \n",
    "        #plt.xlabel(\"Months Subscribed\")\n",
    "        #plt.ylabel(\"Daily Revenue\")\n",
    "        #plt.title(\"Segment Decay Curve\")\n",
    "        \n",
    "    best_fit_idx = np.argmin(errors)\n",
    "    best_curves[rowname] = function[best_fit_idx]\n",
    "    best_params[rowname] = param_list[best_fit_idx]\n",
    "    \n",
    "    percentage_error = \"{:.2%}\".format(mean_absolute_percentage_error(ydata_max, y2))\n",
    "\n",
    "    #plt.plot(xdata_all, ydata_all, 'bo', label='Real Data', markersize=0.5) # This is the real data but omitted for now as it makes the graph look messy\n",
    "    #plt.legend(loc='best')\n",
    "    #plt.show()\n",
    "\n",
    "    # Making additional variables for the purpose of integrating only AFTER there is no more actual data\n",
    "    #x3 = np.linspace(max(xdata_all), 1095, 1095)\n",
    "    x3 = range(max(xdata_all),1095 + 1)\n",
    "    y3 = func(x3,*params)\n",
    "    y3[y3 < 0] = 0 \n",
    "        \n",
    "    # Integrate actual data\n",
    "    val_actual_curve = round(scipy.integrate.trapz(ydata_all,xdata_all),2)\n",
    "      \n",
    "    # Integrate forecasted data between end of actual and 'end'\n",
    "    val_fitted_curve = round(scipy.integrate.trapz(y3, x3),2)\n",
    "       \n",
    "    LTV = round((val_actual_curve + val_fitted_curve),2)\n",
    "        \n",
    "    print(str(rowname) + ' Integrated Value for Actual Data : ' + str(val_actual_curve))\n",
    "    print(str(rowname) + ' Integrated Value for Forecast Data : ' + str(val_fitted_curve))\n",
    "    print(str(rowname) + ' LTV Value : ' + str(LTV))\n",
    "                \n",
    "    # Export forecasted data sets to csv\n",
    "    # All Data\n",
    "    export_dataset = pd.DataFrame({'Avg_Daily_Rev':ydata_all} , index=xdata_all) \n",
    "    # Curve Data \n",
    "    export_dataset2 = pd.DataFrame({'Avg_Daily_Rev':y2} , index=x2)\n",
    "    # Integrate Data\n",
    "    export_dataset3 = pd.DataFrame({'Avg_Daily_Rev':y3} , index=x3)\n",
    "    export_dataset4 = pd.concat([export_dataset, export_dataset2], axis = 1)\n",
    "    export_dataset5 = pd.merge(export_dataset4, export_dataset3, left_index=True, right_index = True, how = 'left')\n",
    "    #export_dataset5.to_excel(rowname + ' Graph Data.xlsx')\n",
    "        \n",
    "    plot_best_values(df,best_params,best_curves)\n",
    "    \n",
    "    LTV_dataset2 = pd.DataFrame({\"Segment\" : rowname, 'LTV': [LTV], 'Total Customers': [Total_Customers], 'Mean absolute % error' : [percentage_error]}) \n",
    "    LTV_dataset3 = LTV_dataset3.append(LTV_dataset2, ignore_index=True)\n",
    "\n",
    "print(LTV_dataset3)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
