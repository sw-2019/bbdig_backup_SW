{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer life time value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install this package for use of graphical properties\n",
    "#!pip install plotly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from   scipy import optimize\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import norm\n",
    "import sys\n",
    "from datetime import date\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Install/import plotly packages- this package has lots of graphical properties\n",
    "import plotly.graph_objects as go\n",
    "import plotly.offline as pyo\n",
    "from pandas.io import gbq\n",
    "from scipy import integrate \n",
    "\n",
    "from matplotlib.patches import Polygon\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# Load custom scripts in reusable_code folder\n",
    "sys.path.append(r'/home/jupyter/reusable_code')\n",
    "\n",
    "import google_api_functions as gaf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds=gaf.Authenticate_Google(r'/home/jupyter/reusable_code/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Define Functions for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the input dataset follows the required format for the LTV model\n",
    "\n",
    "# Required Structure : \n",
    "\n",
    "# Britbox_ID\n",
    "# Subscription ID\n",
    "# signuptime - date format\n",
    "# Segment\n",
    "\n",
    "def Check_input_table_structure(dataset): \n",
    "    Error_flag=False\n",
    "    # Check column header names - structure should be britbox_id, subscriptionid, signuptime, segment, order is not important\n",
    "   \n",
    "    mydata = [x.lower() for x in dataset.columns]\n",
    "    required_columns = ['britbox_id', 'subscriptionid', 'signuptime', 'segment']\n",
    "    \n",
    "    # Columns in 'imported dataset' that don't match the LTV framework\n",
    "    if mydata == required_columns:\n",
    "        pass\n",
    "    elif mydata != required_columns :\n",
    "        missing_required_variables = [x for x in mydata + required_columns if x not in mydata]\n",
    "        additional_variables = [x for x in mydata + required_columns if x not in required_columns]\n",
    "        if len(missing_required_variables) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            Error_flag=True\n",
    "            print('Missing fields required ' + str(missing_required_variables))\n",
    "        if len(additional_variables) == 0:\n",
    "            pass\n",
    "        else:\n",
    "            Error_flag=True\n",
    "            print('Additional fields included, please delete ' + str(additional_variables))\n",
    "        \n",
    "    # Check signupdate format - must be in YYY-MM-DD \n",
    "    if len(re.findall(r\"(([12]\\d{3})-(0[1-9]|1[012])-(0\\d|1\\d|2\\d|3[01]))\", str(dataset.signuptime)))>0:\n",
    "        pass\n",
    "    else:\n",
    "        Error_flag=True\n",
    "        print(\"Error- Sign Up Date in wrong format. Please use YYYY-MM-DD\")\n",
    "        \n",
    "    # Check date validity - should not be in the future\n",
    "    if max(dataset.signuptime) < date.today() :\n",
    "        pass\n",
    "    elif max(dataset.signuptime)>= date.today():\n",
    "        Error_flag=True\n",
    "        print(\"Error - Sign Up Dates in the future, check entitlements table or joins\")\n",
    "        \n",
    "    # Check signupdate format, must be after launch date\n",
    "    if min(dataset.signuptime) < datetime.date(2019, 11, 7) :\n",
    "        Error_flag=True\n",
    "        print(\"Error- Minimum Sign Up Date before Launch Date, check data\")\n",
    "    \n",
    "    if Error_flag==False:\n",
    "        print('Table Structure all ok')\n",
    "    \n",
    "# The code should then take this table and join it to the revenues data and generate the resultant output in the format needed for the rest of the LTV code\n",
    "\n",
    "def plot_df(df, x, y, title=\"Daily Revenue\", xlabel='Date', ylabel='Value', dpi=100):\n",
    "    plt.figure(figsize=(16,5), dpi=dpi)\n",
    "    plt.plot(x, y, color='tab:red')\n",
    "    plt.gca().set(title=title, xlabel=xlabel, ylabel=ylabel)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Determine the fitted curves per segment and plot this \n",
    "\n",
    "# This is used for the curve-fitting procedure later, which requires the function as an input\n",
    "def func_expdecay(xdata, a, b ,k):\n",
    "    return a * np.exp(-b * xdata) + k\n",
    "\n",
    "def func_log(xdata,a,b):\n",
    "    return a * -np.log(b*xdata) \n",
    "\n",
    "def func_linear(xdata,a,b):\n",
    "    return a * xdata +b  #mx +c\n",
    "\n",
    "def func_normal(xdata,a,mu,std):\n",
    "    return a*(np.exp(-((xdata-mu)**2)/(2*std**2)))\n",
    "\n",
    "def func_weib(xdata,n,a):\n",
    "    return (a / n) * (xdata / n)**(a-1) * np.exp(-(xdata/n)**a)\n",
    "\n",
    "def func_lognormal(xdata, a, mu, std):\n",
    "    return a*((1.0/(xdata*std*np.sqrt(2.0*np.pi)))*np.exp(-1.0*(((np.log(xdata)-mu)**2.0)/(2.0*(std**2.0)))))\n",
    "\n",
    "def func_power(x, a, b):\n",
    "    return a*(x**b)\n",
    "\n",
    "def fit_test(ydata,y2,test='CHI2'):\n",
    "    \"\"\"returns fit scores for chi2 and rmse \n",
    "    chisquare requires large freq ideally greater than 5 \n",
    "    (ref:https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.chisquare.html)\n",
    "    \"\"\"\n",
    "    if (test=='CHI2') and (min(ydata)>5): \n",
    "        return stats.chisquare(f_obs=ydata, f_exp=y2)\n",
    "    elif test == 'RMSE':\n",
    "        return np.sqrt(np.mean((ydata - y2)**2))\n",
    "    else:\n",
    "        print('check conditions')\n",
    "\n",
    "def mean_absolute_percentage_error(actual, estimated): \n",
    "    actual, estimated = np.array(actual), np.array(estimated)\n",
    "    return np.mean(np.abs((actual - estimated) / actual)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def Run_LTV(signup_start,signup_end):\n",
    "\n",
    "        # Extracting data to determine LTV\n",
    "    bq = bigquery.Client(project='itv-bde-analytics-prd',credentials=creds)\n",
    "\n",
    "    Segment_Query = \"\"\" (select distinct britbox_ID, subscriptionid, subscription.firstStart as signuptime , 'All' as Segment \n",
    "    from  `itv-bde-analytics-prd.britbox_analytics.entitlements`\n",
    "    where date(subscription.firstStart) >= '{}' and date(subscription.firstStart) <= '{}'\n",
    "    and billingprovider in ('iTunes', 'Stripe') \n",
    "    )  \"\"\".format(signup_start,signup_end)\n",
    "\n",
    "    base_query=\"\"\"\n",
    "    select distinct itvid, subscriptionid,  proratedDailyRevenue,date_value  \n",
    "    from `itv-bde-analytics-prd.britbox_model.LTV_Entlmt_Daily_Revenue`\n",
    "        \"\"\"\n",
    "\n",
    "    Merged_query = \"\"\"\n",
    "    WITH\n",
    "      table1 AS (\n",
    "      SELECT\n",
    "        a.*,\n",
    "        CONCAT(a.itvid, a.subscriptionid) AS ID,\n",
    "        b.Segment,\n",
    "        b.signuptime\n",
    "      FROM  ( \"\"\" +  base_query +  \"\"\" ) AS a INNER JOIN (  \"\"\"  +  Segment_Query +  \"\"\") AS b\n",
    "      ON\n",
    "        a.itvid = b.britbox_ID\n",
    "        AND a.subscriptionid = b.subscriptionid\n",
    "        AND DATE(b.signuptime) <= DATE(a.date_value)\n",
    "        AND DATE(b.signuptime) <= (CURRENT_DATE() -1)),\n",
    "      table1a AS (\n",
    "      SELECT\n",
    "        DISTINCT id,\n",
    "        proratedDailyRevenue,\n",
    "        segment,\n",
    "        ROW_NUMBER() OVER (PARTITION BY id, Segment, signuptime ORDER BY date_value) AS Day\n",
    "      FROM\n",
    "        table1),\n",
    "\n",
    "      table2 AS (\n",
    "      SELECT\n",
    "        DISTINCT *,\n",
    "        proratedDailyRevenue/Customers AS avg_dailyrev,\n",
    "        MAX(Customers) OVER (PARTITION BY Segment) AS Total_Customers,\n",
    "        MAX(ifnull(proratedDailyRevenue/Customers, 0.00000000001)) OVER (PARTITION BY Segment) AS max_rev\n",
    "      FROM (\n",
    "        SELECT\n",
    "          DISTINCT Day,\n",
    "          Segment,\n",
    "          COUNT(DISTINCT id) AS Customers,\n",
    "          SUM(ifnull(proratedDailyRevenue,0.00000000001))/100 AS proratedDailyRevenue\n",
    "        FROM\n",
    "          table1a\n",
    "        GROUP BY\n",
    "          1,\n",
    "          2\n",
    "        ORDER BY\n",
    "          1,\n",
    "          2)\n",
    "      ORDER BY\n",
    "        1,\n",
    "        2,\n",
    "        3)\n",
    "\n",
    "    SELECT\n",
    "      *\n",
    "    FROM\n",
    "      table2\n",
    "    WHERE\n",
    "      Total_Customers > 500\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    Final_Table = bq.query(Merged_query).to_dataframe()\n",
    "\n",
    "    # Pivot the table so data is in correct structure for making graph\n",
    "    df_2_pivot = pd.pivot_table(Final_Table,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "\n",
    "    #################### INITIALISATION  ####################\n",
    "    # Specify the test used to determine Goodness of Fit of the fitted curves\n",
    "    test_type = \"RMSE\"\n",
    "    integrate_to_days=1825 # How many days to integrate to e.g. 365 is one year\n",
    "    function = [func_expdecay, func_normal,func_weib, func_lognormal,func_power] # Choose which functions to fit\n",
    "\n",
    "    # Initialise empty lists where the parameters can be stored for later evaluation if needed\n",
    "    list_parameters = []\n",
    "    list_parameter_covariances = []\n",
    "\n",
    "    # Initialise an empty DataFrame to hold the final output \n",
    "    LTV_dataset3 = pd.DataFrame()\n",
    "\n",
    "    # Initialise empty dictionaries to hold the best functions (curves) and parameters\n",
    "    selected_functions = {}\n",
    "    LTV_selections=[]\n",
    "\n",
    "    #### ALL SEGMENTS AND DAYS\n",
    "    # df is the original, full table holding all segments and days both as records\n",
    "    # df_2_pivot still holds all data, but now days are in columns, and there is just one row per segment\n",
    "\n",
    "    #### ONE SEGMENT AND ALL DAYS >> USED FOR PLOTTING AND EMPIRICAL LTV CALCS\n",
    "    ## sub_df is the original table but for just one segment (recreated in each loop)\n",
    "    #### df_pivot is a pivot of sub_df that has one row and all days held in columns\n",
    "\n",
    "    #### ONE SEGMENT AND ONLY THE DAYS AFTER THE MAX >> USED FOR CURVE FITTING\n",
    "    ### sub_df2 is the subset of sub_df that only looks at the data from the Max point onwards\n",
    "    #### sub_pivot is a pivot of sub_df2 that again transposes so that days are held in columns. This is used for curve fitting\n",
    "\n",
    "\n",
    "    #################### LOOP THROUGH SEGMENTS AND FIT CURVES  ####################\n",
    "\n",
    "    # Loop through each segment name (table is structured as one row per segment, one column per day)\n",
    "    for n,rowname in enumerate(df_2_pivot.index) : \n",
    "\n",
    "        print('''\\n ---------------------------------------------------------------------- \\n \\n Segment: ''' + str(rowname))\n",
    "\n",
    "        #Initialise empty lists for later use\n",
    "        errors = []\n",
    "        param_list= []\n",
    "\n",
    "        #################### Create a Dataframe that finds the maximum daily revenue and only holds data from there onwards\n",
    "        # Assumption is that there will pretty much always be a decay curve, barring any major changes in price\n",
    "        # This dataframe created will be the one used to fit a function, as it is much easier to fit a decay curve than something which also account for the trial period\n",
    "\n",
    "        # Subset columns needed and only this segment\n",
    "        sub_df=Final_Table[['Day','Segment','avg_dailyrev','max_rev','Total_Customers']][Final_Table['Segment']==rowname]\n",
    "        sub_df = sub_df.reset_index(drop = True)\n",
    "\n",
    "        # Find Index in which avg_dailyrev = max_rev\n",
    "        Starting_Curve_Value = sub_df[sub_df.avg_dailyrev==sub_df.max_rev].index.values\n",
    "        Starting_Curve_Value2 = Starting_Curve_Value[0] # There may be multiple days with the same (max) revenue, so start from the first\n",
    "\n",
    "        # Subset this data so you start the curve fitting from the max value\n",
    "        sub_df2 = sub_df.iloc[Starting_Curve_Value2:]\n",
    "\n",
    "\n",
    "        #################### Transpose df and sub_df2 to get days into columns for the selected segment    \n",
    "        sub_pivot=pd.pivot_table(sub_df2,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "        df_pivot=pd.pivot_table(sub_df,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "\n",
    "        # Determines number of customers per segment\n",
    "        Total_Customers = max(sub_df.Total_Customers)\n",
    "\n",
    "        #################### Loop through the range of functions to be fitted to the LTV data     \n",
    "        func_dict={}   \n",
    "        for func in function : \n",
    "\n",
    "            #print('Function: ' + func.__name__)\n",
    "\n",
    "            # Turn the above Data Frames into series/arrays: all= _all data for a segment _max = only data since the max\n",
    "            xdata_all = np.array(df_pivot.columns)\n",
    "            ydata_all = np.array(df_pivot.loc[rowname])\n",
    "\n",
    "            xdata_max = np.array(sub_pivot.columns)\n",
    "            ydata_max = np.array(sub_pivot.loc[rowname])\n",
    "\n",
    "            #################### Define p0 for each function (effectively parameter seeds) >> could be moved into the functions themselves as defaults\n",
    "\n",
    "            if func.__name__ in [  'func_expdecay']:\n",
    "                p0 = (0.2,0.1, 1.5) # inital guess\n",
    "\n",
    "            if func.__name__ in ['func_normal']:\n",
    "                mu, std = norm.fit(xdata_max)\n",
    "                p0 = (85,mu,std)\n",
    "\n",
    "            if func.__name__ in ['func_lognormal']:\n",
    "                mu, std = norm.fit(xdata_max)\n",
    "                p0 = (10,mu,std)\n",
    "\n",
    "            if func.__name__ in [ 'func_linear', 'func_log']:\n",
    "                p0 = (2,3)\n",
    "\n",
    "            if func.__name__ in ['func_weib', 'func_power']:\n",
    "                p0 = (1,1)\n",
    "\n",
    "            if func.__name__ in ['func_exp2']:\n",
    "                p0 = (0.2,0.1,1.5,2)\n",
    "\n",
    "            #################### Attempt to fit the function, return the parameters & covariance of the best fit (per function)\n",
    "            try:\n",
    "                params, params_covariance = optimize.curve_fit(func, xdata_max, ydata_max,p0, maxfev=100000,method='dogbox') \n",
    "\n",
    "                # Generate an array of x values (days) from the peak to the end of the observed dataset.\n",
    "                # We do this because the number of days for any given segment may not = the total number of days across all segments \n",
    "                # E.g. some segments we may only have 100 days for, some we have 200 for, or the peak is at a different place.\n",
    "                x2 = np.linspace(min(xdata_max),max(xdata_max),len(xdata_max))\n",
    "                y2 = func(x2,*params) # Apply the fitted curve to the dummy X values\n",
    "\n",
    "                # Account for when y values trend into negatives as we wouldn't want a negative revenue-- SHOULD WE DO THIS HERE OR LATER, SURELY ONLY NEED FOR INTEGRATION?\n",
    "                y2[y2 < 0] = 0\n",
    "\n",
    "                fit_error = fit_test(ydata_max,y2,test=test_type) # Fits the generated curve against the observed using intended test specified before loop\n",
    "                abserr=mean_absolute_percentage_error(ydata_max,y2) # Calculates mean absolute error\n",
    "\n",
    "                print('Function {} on segment {} has an average error of {:.2f}% and a value of {:.2f} for {}'.\\\n",
    "                      format(func,rowname,abserr*100,fit_error,test_type))\n",
    "\n",
    "                func_dict[func]={'error':fit_error,'params':params,'mape':abserr}\n",
    "                 # Plot each function\n",
    "                plt.plot(x2, y2, color=np.random.rand(3,), label=func.__name__) # generates a random different colour per segment - check if there's a way to fix this? \n",
    "\n",
    "            except:\n",
    "                pass\n",
    "                print(\"Failed to fit function {} to segment {}\".format(func,rowname))\n",
    "\n",
    "\n",
    "\n",
    "        #################### Select best fitting function according to error, store the function name e.g. \"normal\" and the parameters\n",
    "        selected_func,selected_params = [(key,func_dict[key]['params']) for key in func_dict if func_dict[key]['error']==min([d['error'] for d in func_dict.values()])][0]\n",
    "        selected_functions[rowname]= {'function':selected_func,'params':selected_params} # Add to dictionary storing choices for all segments\n",
    "\n",
    "\n",
    "        #################### Plot actuals on same graph as above\n",
    "        plt.plot(xdata_all, ydata_all, 'bo', label='Real Data', markersize=0.5) # This is the real data but omitted for now as it makes the graph look messy\n",
    "\n",
    "        # Format the plot\n",
    "        plt.xlabel(\"Months Subscribed\")\n",
    "        plt.ylabel(\"Daily Revenue\")\n",
    "        plt.title(\"Segment Decay Curve\")\n",
    "        plt.legend(loc='best')\n",
    "        plt.show() # Display the plot\n",
    "\n",
    "\n",
    "        #################### Determine the best fitted curve to the real data and integrate to determine LTV\n",
    "\n",
    "        observed_days=max(xdata_all) # Calculate the number of days of actual data we have\n",
    "        predicted_days=integrate_to_days-observed_days # Calculate the number of days we're predicting\n",
    "\n",
    "\n",
    "        ###### Select the necessary data ranges\n",
    "        empirical_x, empirical_y=xdata_all,ydata_all # The actual data observed. This will be both used for visual and for integration\n",
    "\n",
    "        forecast_x=range(observed_days+1,integrate_to_days + 1)\n",
    "        forecast_y=selected_func(forecast_x,*selected_params) # The forecast into the future. This should be sequentially after the observed data ends and is used for the integration\n",
    "\n",
    "        curve_x=range(min(xdata_max),integrate_to_days + 1)\n",
    "        curve_y=selected_func(curve_x,*selected_params) # The curve fitted to both the empirical data after the max, and the forecast moving forwards. Used for the visual\n",
    "\n",
    "\n",
    "        ###### Set -ve forecasts to 0\n",
    "        forecast_y[forecast_y<0]=0\n",
    "        curve_y[curve_y<0]=0\n",
    "\n",
    "        ###### Plot and Integrate\n",
    "        # Plot & integrate empirical data\n",
    "        plt.plot(empirical_x,empirical_y,c='b') # Plot line in blue\n",
    "        plt.fill_between(empirical_x,empirical_y, where = [(x >= 0)  and (x <= max(empirical_x)) for x in empirical_x], color = 'blue', alpha = 0.3) # Fill in under the graph\n",
    "        val_actual_curve = scipy.integrate.trapz(empirical_y,empirical_x) # Integrate under the curve\n",
    "\n",
    "        # Plot & integrate the fitted curve\n",
    "        plt.plot(curve_x,curve_y,c='r') # Plot the line in red\n",
    "        plt.fill_between(forecast_x,forecast_y, color = 'red', alpha = 0.3) # Fill in under the forecasted area only\n",
    "        val_fitted_curve = scipy.integrate.trapz(forecast_y, forecast_x)\n",
    "\n",
    "        # Generate clean name, add as title and save figure\n",
    "        title=rowname + \": \" +str(selected_func.__name__).replace('func_','')\n",
    "        plt.title(title)\n",
    "        save_name=title+ '.png'\n",
    "        #plt.savefig(save_name)\n",
    "        plt.show()\n",
    "\n",
    "        LTV = (val_actual_curve + val_fitted_curve)\n",
    "        print('LTV Value of Cohort: {} is £{:.2f}'.format(str(rowname), LTV))\n",
    "        print('The observed component (blue) totals £{:.2f} across the first {} days'.format(val_actual_curve,observed_days))\n",
    "        print('The forecasted component (red) totals £{:.2f} across the next {} days'.format(val_fitted_curve,predicted_days))\n",
    "\n",
    "        LTV_selections.append({'Segment':rowname,\\\n",
    "                               'Customers' : Total_Customers,\\\n",
    "                               'LTV':round(LTV,2),'LTV_days':integrate_to_days,\\\n",
    "                               'LTV_Observed':round(val_actual_curve,2),'LTV_observed_days':observed_days,\\\n",
    "                               'LTV_Predicted':round(val_fitted_curve,2),'LTV_predicted_days':predicted_days,\\\n",
    "                               'Fitted_Function':str(selected_func.__name__).replace('func_',''),'Fitted_Params':str(selected_params),\\\n",
    "                              })\n",
    "\n",
    "        # Export forecasted data sets to csv\n",
    "        # All Data\n",
    "        export_dataset = pd.DataFrame({'Avg_Daily_Rev':empirical_y} , index=empirical_x) \n",
    "        # Curve Data \n",
    "        export_dataset2 = pd.DataFrame({'Avg_Daily_Rev':curve_y} , index=curve_x)\n",
    "        # Integrate Data\n",
    "        export_dataset3 = pd.DataFrame({'Avg_Daily_Rev':forecast_y} , index=forecast_x)\n",
    "\n",
    "        export_dataset4 = pd.concat([export_dataset, export_dataset2], axis = 1)\n",
    "\n",
    "        export_dataset5 = pd.merge(export_dataset4, export_dataset3, left_index=True, right_index = True, how = 'left')\n",
    "\n",
    "        #export_dataset5.to_excel(rowname + ' Graph Data.xlsx')\n",
    "\n",
    "    LTV_final_df=pd.DataFrame(LTV_selections)\n",
    "    LTV_final_df['signup_window_Start']=signup_start\n",
    "    LTV_final_df['signup_window_End']=signup_end\n",
    "    return LTV_final_df\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run_LTV(start_date,end_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n,i in enumerate(range(2,13)):\n",
    "    print(i,n)\n",
    "    \n",
    "    #start_date=datetime.now().date() - timedelta(weeks=i)\n",
    "    #end_date=datetime.now().date() - timedelta(weeks=(i-1))\n",
    "    start_date=datetime.now().date() - relativedelta(months=i)\n",
    "    end_date=datetime.now().date() - relativedelta(months=(i-1))\n",
    "    \n",
    "    relativedelta\n",
    "    results=Run_LTV(start_date,end_date)\n",
    "\n",
    "    if n==0:\n",
    "        final_df=results.copy()\n",
    "    else:\n",
    "        final_df=pd.concat([final_df,results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Extracting data to determine LTV\n",
    "bq = bigquery.Client(project='itv-bde-analytics-prd',credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "Segment_Query = \"\"\" (select distinct britbox_ID, subscriptionid, subscription.firstStart as signuptime , 'All' as Segment \n",
    "from  `itv-bde-analytics-prd.britbox_analytics.entitlements`\n",
    "where date(subscription.firstStart) >= date_sub(current_date(),INTERVAL 6 MONTH) and date(subscription.firstStart) <= date_sub(current_date(),INTERVAL 10 WEEK)\n",
    "and billingprovider in ('iTunes', 'Stripe') \n",
    ")  \"\"\"\n",
    "\n",
    "Segment_table = bq.query(Segment_Query).to_dataframe()\n",
    "\n",
    "Check_input_table_structure(Segment_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Base table currently set to 2020+ in the view within big query, earlier data can be looked at but through analysis \n",
    "# earlier cohorts skew the behaviour and product bad fits for the LTV model\n",
    "# Filter of 500+ customers needed per segment \n",
    "\n",
    "base_query=\"\"\"\n",
    "select distinct itvid, subscriptionid,  proratedDailyRevenue,date_value  \n",
    "from `itv-bde-analytics-prd.britbox_model.LTV_Entlmt_Daily_Revenue`\n",
    "    \"\"\"\n",
    "\n",
    "Merged_query = \"\"\"\n",
    "WITH\n",
    "  table1 AS (\n",
    "  SELECT\n",
    "    a.*,\n",
    "    CONCAT(a.itvid, a.subscriptionid) AS ID,\n",
    "    b.Segment,\n",
    "    b.signuptime\n",
    "  FROM  ( \"\"\" +  base_query +  \"\"\" ) AS a INNER JOIN (  \"\"\"  +  Segment_Query +  \"\"\") AS b\n",
    "  ON\n",
    "    a.itvid = b.britbox_ID\n",
    "    AND a.subscriptionid = b.subscriptionid\n",
    "    AND DATE(b.signuptime) <= DATE(a.date_value)\n",
    "    AND DATE(b.signuptime) <= (CURRENT_DATE() -1)),\n",
    "  table1a AS (\n",
    "  SELECT\n",
    "    DISTINCT id,\n",
    "    proratedDailyRevenue,\n",
    "    segment,\n",
    "    ROW_NUMBER() OVER (PARTITION BY id, Segment, signuptime ORDER BY date_value) AS Day\n",
    "  FROM\n",
    "    table1),\n",
    "  \n",
    "  table2 AS (\n",
    "  SELECT\n",
    "    DISTINCT *,\n",
    "    proratedDailyRevenue/Customers AS avg_dailyrev,\n",
    "    MAX(Customers) OVER (PARTITION BY Segment) AS Total_Customers,\n",
    "    MAX(ifnull(proratedDailyRevenue/Customers, 0.00000000001)) OVER (PARTITION BY Segment) AS max_rev\n",
    "  FROM (\n",
    "    SELECT\n",
    "      DISTINCT Day,\n",
    "      Segment,\n",
    "      COUNT(DISTINCT id) AS Customers,\n",
    "      SUM(ifnull(proratedDailyRevenue,0.00000000001))/100 AS proratedDailyRevenue\n",
    "    FROM\n",
    "      table1a\n",
    "    GROUP BY\n",
    "      1,\n",
    "      2\n",
    "    ORDER BY\n",
    "      1,\n",
    "      2)\n",
    "  ORDER BY\n",
    "    1,\n",
    "    2,\n",
    "    3)\n",
    "\n",
    "SELECT\n",
    "  *\n",
    "FROM\n",
    "  table2\n",
    "WHERE\n",
    "  Total_Customers > 500\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "Final_Table = bq.query(Merged_query).to_dataframe()\n",
    "\n",
    "Final_Table.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Pivot the table so data is in correct structure for making graph\n",
    "df_2_pivot = pd.pivot_table(Final_Table,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "df_2_pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Plot data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Plotting actual data\n",
    "# This enables the data to be plotted in seperate lines split by 'segment' by allocating the ydata per row \n",
    "# hence why data needed to be pivoted so there's one row per segment\n",
    "\n",
    "traces = [go.Scatter (\n",
    "        x = df_2_pivot.columns,\n",
    "        y = df_2_pivot.loc[rowname],\n",
    "        mode = 'lines',\n",
    "        name = rowname\n",
    ")for rowname in df_2_pivot.index]\n",
    "\n",
    "# Plot the data\n",
    "graph = go.Figure(data = traces)\n",
    "#graph.update_xaxes(type=\"category\",)\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Draw Plot for Daily Revenue, sense check for any data anomolies\n",
    "x = sorted(Final_Table.Day)\n",
    "y = Final_Table.avg_dailyrev\n",
    "plot_df(Final_Table, x, y, title='Daily Revenue')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Draw Plot for Daily Customers, expecting a constant decreasing volume\n",
    "x = sorted(Final_Table.Day)\n",
    "y = Final_Table.Customers\n",
    "plot_df(Final_Table, x, y, title='Daily Customers')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Curve Fitting and Determining LTV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#################### INITIALISATION  ####################\n",
    "# Specify the test used to determine Goodness of Fit of the fitted curves\n",
    "test_type = \"RMSE\"\n",
    "integrate_to_days=1825 # How many days to integrate to e.g. 365 is one year\n",
    "function = [func_expdecay, func_normal,func_weib, func_lognormal,func_power] # Choose which functions to fit\n",
    "\n",
    "# Initialise empty lists where the parameters can be stored for later evaluation if needed\n",
    "list_parameters = []\n",
    "list_parameter_covariances = []\n",
    "\n",
    "# Initialise an empty DataFrame to hold the final output \n",
    "LTV_dataset3 = pd.DataFrame()\n",
    "\n",
    "# Initialise empty dictionaries to hold the best functions (curves) and parameters\n",
    "selected_functions = {}\n",
    "LTV_selections=[]\n",
    "\n",
    "#### ALL SEGMENTS AND DAYS\n",
    "# df is the original, full table holding all segments and days both as records\n",
    "# df_2_pivot still holds all data, but now days are in columns, and there is just one row per segment\n",
    "\n",
    "#### ONE SEGMENT AND ALL DAYS >> USED FOR PLOTTING AND EMPIRICAL LTV CALCS\n",
    "## sub_df is the original table but for just one segment (recreated in each loop)\n",
    "#### df_pivot is a pivot of sub_df that has one row and all days held in columns\n",
    "\n",
    "#### ONE SEGMENT AND ONLY THE DAYS AFTER THE MAX >> USED FOR CURVE FITTING\n",
    "### sub_df2 is the subset of sub_df that only looks at the data from the Max point onwards\n",
    "#### sub_pivot is a pivot of sub_df2 that again transposes so that days are held in columns. This is used for curve fitting\n",
    "\n",
    "\n",
    "#################### LOOP THROUGH SEGMENTS AND FIT CURVES  ####################\n",
    "\n",
    "# Loop through each segment name (table is structured as one row per segment, one column per day)\n",
    "for n,rowname in enumerate(df_2_pivot.index) : \n",
    "    \n",
    "    print('''\\n ---------------------------------------------------------------------- \\n \\n Segment: ''' + str(rowname))\n",
    "    \n",
    "    #Initialise empty lists for later use\n",
    "    errors = []\n",
    "    param_list= []\n",
    "    \n",
    "    #################### Create a Dataframe that finds the maximum daily revenue and only holds data from there onwards\n",
    "    # Assumption is that there will pretty much always be a decay curve, barring any major changes in price\n",
    "    # This dataframe created will be the one used to fit a function, as it is much easier to fit a decay curve than something which also account for the trial period\n",
    "    \n",
    "    # Subset columns needed and only this segment\n",
    "    sub_df=Final_Table[['Day','Segment','avg_dailyrev','max_rev','Total_Customers']][Final_Table['Segment']==rowname]\n",
    "    sub_df = sub_df.reset_index(drop = True)\n",
    "    \n",
    "    # Find Index in which avg_dailyrev = max_rev\n",
    "    Starting_Curve_Value = sub_df[sub_df.avg_dailyrev==sub_df.max_rev].index.values\n",
    "    Starting_Curve_Value2 = Starting_Curve_Value[0] # There may be multiple days with the same (max) revenue, so start from the first\n",
    "    \n",
    "    # Subset this data so you start the curve fitting from the max value\n",
    "    sub_df2 = sub_df.iloc[Starting_Curve_Value2:]\n",
    "\n",
    "    \n",
    "    #################### Transpose df and sub_df2 to get days into columns for the selected segment    \n",
    "    sub_pivot=pd.pivot_table(sub_df2,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "    df_pivot=pd.pivot_table(sub_df,  values = 'avg_dailyrev' , index = ['Segment'], columns = ['Day'])\n",
    "    \n",
    "    # Determines number of customers per segment\n",
    "    Total_Customers = max(sub_df.Total_Customers)\n",
    "    \n",
    "    #################### Loop through the range of functions to be fitted to the LTV data     \n",
    "    func_dict={}   \n",
    "    for func in function : \n",
    "\n",
    "        #print('Function: ' + func.__name__)\n",
    "        \n",
    "        # Turn the above Data Frames into series/arrays: all= _all data for a segment _max = only data since the max\n",
    "        xdata_all = np.array(df_pivot.columns)\n",
    "        ydata_all = np.array(df_pivot.loc[rowname])\n",
    "        \n",
    "        xdata_max = np.array(sub_pivot.columns)\n",
    "        ydata_max = np.array(sub_pivot.loc[rowname])\n",
    "        \n",
    "        #################### Define p0 for each function (effectively parameter seeds) >> could be moved into the functions themselves as defaults\n",
    "     \n",
    "        if func.__name__ in [  'func_expdecay']:\n",
    "            p0 = (0.2,0.1, 1.5) # inital guess\n",
    "    \n",
    "        if func.__name__ in ['func_normal']:\n",
    "            mu, std = norm.fit(xdata_max)\n",
    "            p0 = (85,mu,std)\n",
    "            \n",
    "        if func.__name__ in ['func_lognormal']:\n",
    "            mu, std = norm.fit(xdata_max)\n",
    "            p0 = (10,mu,std)\n",
    "    \n",
    "        if func.__name__ in [ 'func_linear', 'func_log']:\n",
    "            p0 = (2,3)\n",
    "        \n",
    "        if func.__name__ in ['func_weib', 'func_power']:\n",
    "            p0 = (1,1)\n",
    "        \n",
    "        if func.__name__ in ['func_exp2']:\n",
    "            p0 = (0.2,0.1,1.5,2)\n",
    "    \n",
    "        #################### Attempt to fit the function, return the parameters & covariance of the best fit (per function)\n",
    "        try:\n",
    "            params, params_covariance = optimize.curve_fit(func, xdata_max, ydata_max,p0, maxfev=100000,method='dogbox') \n",
    "            \n",
    "            # Generate an array of x values (days) from the peak to the end of the observed dataset.\n",
    "            # We do this because the number of days for any given segment may not = the total number of days across all segments \n",
    "            # E.g. some segments we may only have 100 days for, some we have 200 for, or the peak is at a different place.\n",
    "            x2 = np.linspace(min(xdata_max),max(xdata_max),len(xdata_max))\n",
    "            y2 = func(x2,*params) # Apply the fitted curve to the dummy X values\n",
    "\n",
    "            # Account for when y values trend into negatives as we wouldn't want a negative revenue-- SHOULD WE DO THIS HERE OR LATER, SURELY ONLY NEED FOR INTEGRATION?\n",
    "            y2[y2 < 0] = 0\n",
    "\n",
    "            fit_error = fit_test(ydata_max,y2,test=test_type) # Fits the generated curve against the observed using intended test specified before loop\n",
    "            abserr=mean_absolute_percentage_error(ydata_max,y2) # Calculates mean absolute error\n",
    "            \n",
    "            print('Function {} on segment {} has an average error of {:.2f}% and a value of {:.2f} for {}'.\\\n",
    "                  format(func,rowname,abserr*100,fit_error,test_type))\n",
    "\n",
    "            func_dict[func]={'error':fit_error,'params':params,'mape':abserr}\n",
    "             # Plot each function\n",
    "            plt.plot(x2, y2, color=np.random.rand(3,), label=func.__name__) # generates a random different colour per segment - check if there's a way to fix this? \n",
    "        \n",
    "        except:\n",
    "            pass\n",
    "            print(\"Failed to fit function {} to segment {}\".format(func,rowname))\n",
    "            \n",
    "   \n",
    "       \n",
    "    #################### Select best fitting function according to error, store the function name e.g. \"normal\" and the parameters\n",
    "    selected_func,selected_params = [(key,func_dict[key]['params']) for key in func_dict if func_dict[key]['error']==min([d['error'] for d in func_dict.values()])][0]\n",
    "    selected_functions[rowname]= {'function':selected_func,'params':selected_params} # Add to dictionary storing choices for all segments\n",
    "\n",
    "    \n",
    "    #################### Plot actuals on same graph as above\n",
    "    plt.plot(xdata_all, ydata_all, 'bo', label='Real Data', markersize=0.5) # This is the real data but omitted for now as it makes the graph look messy\n",
    "    \n",
    "    # Format the plot\n",
    "    plt.xlabel(\"Months Subscribed\")\n",
    "    plt.ylabel(\"Daily Revenue\")\n",
    "    plt.title(\"Segment Decay Curve\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show() # Display the plot\n",
    "    \n",
    "    \n",
    "    #################### Determine the best fitted curve to the real data and integrate to determine LTV\n",
    "    \n",
    "    observed_days=max(xdata_all) # Calculate the number of days of actual data we have\n",
    "    predicted_days=integrate_to_days-observed_days # Calculate the number of days we're predicting\n",
    "\n",
    "    \n",
    "    ###### Select the necessary data ranges\n",
    "    empirical_x, empirical_y=xdata_all,ydata_all # The actual data observed. This will be both used for visual and for integration\n",
    "    \n",
    "    forecast_x=range(observed_days+1,integrate_to_days + 1)\n",
    "    forecast_y=selected_func(forecast_x,*selected_params) # The forecast into the future. This should be sequentially after the observed data ends and is used for the integration\n",
    "    \n",
    "    curve_x=range(min(xdata_max),integrate_to_days + 1)\n",
    "    curve_y=selected_func(curve_x,*selected_params) # The curve fitted to both the empirical data after the max, and the forecast moving forwards. Used for the visual\n",
    "    \n",
    "\n",
    "    ###### Set -ve forecasts to 0\n",
    "    forecast_y[forecast_y<0]=0\n",
    "    curve_y[curve_y<0]=0\n",
    "    \n",
    "    ###### Plot and Integrate\n",
    "    # Plot & integrate empirical data\n",
    "    plt.plot(empirical_x,empirical_y,c='b') # Plot line in blue\n",
    "    plt.fill_between(empirical_x,empirical_y, where = [(x >= 0)  and (x <= max(empirical_x)) for x in empirical_x], color = 'blue', alpha = 0.3) # Fill in under the graph\n",
    "    val_actual_curve = scipy.integrate.trapz(empirical_y,empirical_x) # Integrate under the curve\n",
    "    \n",
    "    # Plot & integrate the fitted curve\n",
    "    plt.plot(curve_x,curve_y,c='r') # Plot the line in red\n",
    "    plt.fill_between(forecast_x,forecast_y, color = 'red', alpha = 0.3) # Fill in under the forecasted area only\n",
    "    val_fitted_curve = scipy.integrate.trapz(forecast_y, forecast_x)\n",
    "   \n",
    "    # Generate clean name, add as title and save figure\n",
    "    title=rowname + \": \" +str(selected_func.__name__).replace('func_','')\n",
    "    plt.title(title)\n",
    "    save_name=title+ '.png'\n",
    "    #plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    LTV = (val_actual_curve + val_fitted_curve)\n",
    "    print('LTV Value of Cohort: {} is £{:.2f}'.format(str(rowname), LTV))\n",
    "    print('The observed component (blue) totals £{:.2f} across the first {} days'.format(val_actual_curve,observed_days))\n",
    "    print('The forecasted component (red) totals £{:.2f} across the next {} days'.format(val_fitted_curve,predicted_days))\n",
    "    \n",
    "    LTV_selections.append({'Segment':rowname,\\\n",
    "                           'Customers' : Total_Customers,\\\n",
    "                           'LTV':round(LTV,2),'LTV_days':integrate_to_days,\\\n",
    "                           'LTV_Observed':round(val_actual_curve,2),'LTV_observed_days':observed_days,\\\n",
    "                           'LTV_Predicted':round(val_fitted_curve,2),'LTV_predicted_days':predicted_days,\\\n",
    "                           'Fitted_Function':str(selected_func.__name__).replace('func_',''),'Fitted_Params':str(selected_params),\\\n",
    "                          })\n",
    "\n",
    "    # Export forecasted data sets to csv\n",
    "    # All Data\n",
    "    export_dataset = pd.DataFrame({'Avg_Daily_Rev':empirical_y} , index=empirical_x) \n",
    "    # Curve Data \n",
    "    export_dataset2 = pd.DataFrame({'Avg_Daily_Rev':curve_y} , index=curve_x)\n",
    "    # Integrate Data\n",
    "    export_dataset3 = pd.DataFrame({'Avg_Daily_Rev':forecast_y} , index=forecast_x)\n",
    "\n",
    "    export_dataset4 = pd.concat([export_dataset, export_dataset2], axis = 1)\n",
    "\n",
    "    export_dataset5 = pd.merge(export_dataset4, export_dataset3, left_index=True, right_index = True, how = 'left')\n",
    "\n",
    "    #export_dataset5.to_excel(rowname + ' Graph Data.xlsx')\n",
    "          \n",
    "LTV_final_df=pd.DataFrame(LTV_selections)\n",
    "LTV_final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "r-cpu.3-6.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.3-6:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
